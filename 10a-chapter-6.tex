\chapter{Conclusion and Future Direction} \label{chap:chap-6}
\section{Summary of Results}

This dissertation explores compositional generalization through neurosymbolic models, analyzing and proposing architectures that combine neural network flexibility with symbolic computation's interpretability and invariance to distributional shifts.

First, the Role Learning Network (ROLE) demonstrates how sequence-to-sequence RNNs encode symbolic structure by implicitly converging on TPRs. ROLE recovers the latent structures, which allows both interpretation and causal manipulation, demonstrating that the discovered structures are not merely correlational but actually drive the model's behavior.

Next, I present the Differentiable Tree Machine (DTM) and its sparse successor sDTM, novel neurosymbolic architectures explicitly designed for compositional generalization via differentiable symbolic tree operations. DTM integrates a differentiable interpreter with a neural agent that sequences symbolic transformations and utilizes external memory to manage intermediate states. sDTM exchanges symbolic trees represented with TPRs for Sparse Coordinate Trees, a new vector symbolic encoding method. Empirical evaluations demonstrate that sDTM achieves superior out-of-distribution generalization compared to traditional neural and neurosymbolic approaches across multiple tasks and distributional shifts.
 
Finally, I investigated improvements for Transformer architectures for modeling Regular Languages. Recurrent Transformers reveal a a crucial trade-off between parallelism and length generalization—specifically, incorporating token- or chunk- layer recurrence, as exemplified by Staircase Attention with small chunk sizes, was identified as key to achieving strong length generalization on Regular Languages, mirroring the capabilities of RNNs but at the cost of reduced training parallelism compared to standard Transformers. Generalization performance was observed to degrade rapidly as chunk size increased.

TODO: Pooling by multiheaded attention helps both DTM and Superposition Stacks. This may be a generally helpful tool for non-fixed dimensional vector-symbolic techniques.

\section{Limitations and Future Work}

As with much machine learning research of the past decade, expemlified by research focused on inductive biases, this dissertation tastes of the Bitter Lesson \citep{Sutton2019BitterLesson}. Each of the inductive biases that I investigated in my research, whether the differentiable interpreter, Sparse Coordinate Trees, adding recurrence to Transformers, or stack attention, led to improvements on some small domain of experiments while sacrificing scalability and applicability to general purpose datasets. However, the field is progressing at such a remarkable pace with increasingly large-scale experiments that specialized inductive biases are often superseded by general-purpose architectures trained on massive datasets. Despite this limitation, I retain some hope that future neural architectures will take insight from human cognition and the importance of compositional generalization. Whether this future architecture research is conducted by humans or LLMs\ldots then may this dissertation serve as useful prompt.

\subsection{Interpreting implicit recursive structure}
The symbolic decomposition of RNN states that DISCOVER produces lack recursive structure. Instead, DISCOVER produces a flat key-value dictionary, where the keys are roles and the values are fillers. In these experiments, DISCOVER is able to overcome a lack of recursive structure by having an abundance of roles—for example, a binary tree of depth three can use seven independent roles for all possible nodes instead of two roles recursively applied. The lack of recursion may be partially responsible for the complexity of the learned role scheme on SCAN (see Appendix \ref{sec:rldn-Alg}). A top level distinction in the interpretation of the learned role scheme is the presence of conjunctions such as ``and'' and ``after''; a recursive role scheme would allow roles to be shared between subsequences, with a higher order role to specify whether the subsequence comes first or second. Since the publication of \citet{soulos2019discovering}, a variety of works have investigated the presence and interpretation of structural elements in Transformer language models \citep{hewitt2019structural,murty2022characterizing,murty2023grokking}. Integrating these techniques for with a version of ROLE that allows for recursive roles would shed light on whether neural networks implicitly converge to recursively structured TPRs.

\subsection{Large Recurrent Language Models?}
Chapter \ref{chap:chap-4} showed that Recurrent Transformers with small chunk sizes have much better state tracking abilities out-of-distribution than standard Transformers. This suggests that the chunk-layer recurrence helps the Transformer build a robust model of the underlying system, provided that the chunk size is small enough. However, the slowdown incurred from disrupting the Transformer's parallelizability over the sequence length makes pretraining a recurrent transformer impractical.

One idea is to keep the parallelizable pretraining, but add recurrence during a post-training phase. In particular, many forms of post-training now rely on reinforcement learning (RL) where a model autoregressively produces outputs, and the entire outputs are judged according to a reward model. In the RL case, adding recurrence would not have any effect on the speed of training, since RL cannot use teacher forcing and must generate each token one after another.

Unfortunately, my experiments to add recurrence during post-training did not yield any measurable benefits. I explored whether supervised fine-tuning of Llama 3 \citep{grattafiori2024llama3herdmodels} on the Boxes dataset \citep{kim-schuster-2023-entity} would improve generalization. The samples in Boxes include descriptions of an initial world state, state changes akin to a finite state machine, and predictions of the final state. For example, a prompt might contain ``Box A contains the apple, box B is empty, and box C contains the carrot. Move the carrot from box C to box B. Box c <MASK>'' and the model needs to predict the contents of box c. The state changes of objects in boxes can be modeled by a finite state machine, but I found that fine-tuning Llama 3 with staircase attention \citep{ju_staircase_2022} did not improve generalization. 

%dynamic chunk size with sentence as a thought
%- Cold start RL?
%- SFT into a recurrent transformer and then RL?

\subsection{Superposition Shift-Reduce Model}
One of the major limitations of the original Differentiable Tree Machine (DTM) introduced in Chapter \ref{chap:chap-3} was the requirement that the dataset have tree-to-tree samples, which dramatically limited the applicability of the model. I expanded the applicability to sequence-to-sequence tasks in Chapter \ref{chap:chap-4}, but a major area of modeling is still absent: language modeling. DTM is a bidirectional non-autoregressive model, meaning that it reads in an entire input and produces an entire output in a single pass. This contrasts with popular architectures like RNNs and Transformers, where the decoding phase is autoregressive.

This introduces the question of how to expand DTM so that it can be used on next-token prediction tasks. Shift-reduce parsing is a method to parse text in a left-to-right manner, and this technique has been combined with neural networks which replace the shift operation with next token prediction to create shift-reduce language models \citep{dyer_recurrent_2016, choe_parsing_2016, qian_structural_2021}. These models are frequently supervised with annotated parse trees from either the Penn Treebank \citep{marcus-etal-1993-building} or BLLIP \citep{charniak2000bllip}. Unsupervised variants also exist, where the model infers the correct syntactic parse without version, but these are difficult to train (TODO cite stuff). A similar unsupervised direction are models with a softer inductive bias towards internally modeling syntax (TODO cite other stuff).

By reframing shift and reduce, I can show that the differentiable sturctural operations from trees and stacks can make shift and reduce differentiable. Shift is equivalent to \textit{push}, and if we assume a binary tree, reduce is equivalent to \textit{pop}, \textit{pop}, \textit{cons}, \textit{push}. The differentiable stack of Chapter \ref{chap:chap-5} and the differentiable trees of Chapters \ref{chap:chap-3} and \ref{chap:chap-4} support an unsupervised shift-reduce language model, where the output space is simply the next token, and an internal stack of subtrees keeps track of the parse in a differentiable manner.

\section{Finale}
In conclusion, this dissertation has explored several complementary approaches to compositional generalization in neural networks—from discovering latent symbolic structures in trained RNNs to explicitly designing neurosymbolic architectures with differentiable tree operations, and from enhancing Transformers with recurrence to augmenting them with stack-like structures. Across these diverse approaches, a consistent finding emerges: models that effectively blend neural flexibility with symbolic reasoning achieve superior compositional generalization beyond the training distribution (I'm meh on this sentence). The true potential may lie not in choosing between pure neural architectures and symbolic systems, but in identifying the critical intersection points where symbolic operations can enhance neural processing without sacrificing scalability. As machine learning continues to mature, I believe the field will ultimately converge toward architectures that incorporate the best of both paradigms—the flexibility and learning capabilities of neural networks alongside the systematic compositionality of symbolic approaches—paving the way for artificial intelligence systems with more robust, human-like generalization.