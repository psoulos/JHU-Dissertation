\chapter{Appendix for Compositional Generalization Across Distributional Shifts with Sparse Tree Operations} \label{chap:appendix-c}

\section{Appendix}

\subsection{\fullrepname as Tensor Product Representations} \label{sec:sdtm-appendix-sparse-tprs}
This section shows that \fullrepname is the same as a TPR with the constraint that the role basis is the standard basis. TPRs define structural positions as role vectors $r_i \in \mathbb{R}^{d_r}$, and the content that fills these positions is defined by filler vectors $f_i \in \mathbb{R}^{d_f}$. For a particular role and filler pair, the filler $f_i$ is \textit{bound} to the role $r_i$ using the tensor/outer product: $f_i \otimes r_i \in \mathbb{R}^{d_f \times d_r}$. The representation of an entire structure is the sum over all $N$ individual filler-role pairs: $T = \sum_{i=1}^N f_i \otimes r_i  \in \mathbb{R}^{d_f \times d_r}$. As shown in the previous two equations, the dimensionality of a single filler-role pair is equal to the dimensionality of an entire structure: both have dimensionality $\mathbb{R}^{d_f \times d_r}$. This means that a tree with only a filled root node takes up the same memory as a dense tree with every node filled. An important requirement for TPRs is that the role vectors must be linearly independent; this ensures that a filler can be \textit{unbound} from a role without introducing noise using the inner product: $f_j = Tr^+_j$, where $\{r^+_i\}_i$ is the basis dual to $\{r_i\}_i$. Previous work typically used randomly initialized and frozen orthonormal vectors to define the role basis. By defining our role vectors in a sparse manner as opposed to random initialization, we can greatly reduce the memory used by TPRs.

Classic symbolic data structures grow in memory linearly with the number of filled positions.  It is possible to replicate this behavior with TPRs by defining the role vectors to be the standard one-hot basis, which is orthonormal by definition. The $i$-th element of role vector $r_i$ is 1, and the other elements are 0. When a filler and role vector are both dense, the resulting bound vector is also dense. When the role vector is one-hot, the resulting bound vector is 0 everywhere except for column $i$ which corresponds to the value 1 in $r_i$. By using a sparse tensor representation that only keeps track of dimensions that are not equal to 0, we can reduce the memory usage of TPRs to linear growth that scales with the number of filled positions, like a classical symbolic data structure. This however forgoes a motivating desideratum for the design of TPRs, that roles (and not just fillers) have similarity relations that support generalization across structural positions.

We can additionally improve the efficiency by refraining from performing the outer product. Since we are not performing a \textbf{tensor} product, this technique is only implicitly a \textbf{Tensor} Product Representation. Instead, we can keep the filler and role vectors in two aligned lists. A filler is bound to a role by sharing an index in our aligned lists. This is equivalent to the \textit{binding} and \textit{unbinding} from classical dense TPRs without having to perform multiplication.

Since we are not performing an outer product, instead of storing sparse role vectors, we can simply store a role integer, where the integer corresponds to the one-hot dimension. We derive a tree addressing scheme based on Gorn addresses \citep{gorn+1967+77+115}. In our scheme, addresses are read from right to left, giving the path from the root where a left-branch is indicated by a $0$ and a right-branch is indicated by a $1$. We need a way to distinguish between leading $0$s and left-branches (e.g., $010$ vs.\ $10$), so we start our addressing scheme at $1$ instead of $0$. This indicates that all $0$s to the left of the left-most $1$ are unfilled and not left-branches; the left-most 1 and all preceding 0s are ignored when decoding the path-from-root. Figure \ref{fig:sparse-tpr} shows an example encoding of a tree in the sparse implicit approach.

We can compare the memory requirements of the Sparse Coordinate Tree encoding used in the \sdtm to the memory requirements of the full TPRs used in the original \dtm of \citet{Soulos_2023_DifferentiableTreeOperations}. A TPR uses the same amount of memory regardless of the number of filled nodes. As with all sparse tensor formats, the memory savings arise when there are many zeros. In a dense tree where every node is occupied, the classical dense TPR approach is actually more efficient: the \abvrepname's value list has the same total dimension as the classical TPR, but, in addition, the \abvrepname encoding includes the list of filled-node addresses.

\subsection{Agent Figure}
See Figure \ref{fig:agent}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/sdtm/Agent.pdf}
    \caption{Adapted from \citet{Soulos_2023_DifferentiableTreeOperations}. One step of \dtm is expanded to show how the agent produces the input to the interpreter. The interpreter then writes the output to memory and encodes the output for the agent. Parts of the architecture with learnable parameters are indicated in yellow. The agent uses three linear transformations on top of a standard Transformer encoder layer to parameterize the inputs to the interpreter. The superscript indicates the layer number and refers to parameters and activations that are exclusive to this layer.}
    \label{fig:agent}
\end{figure}

\subsection{Lexical Regularization Ablation} \label{sec:sdtm-lex-reg-ablation}
To see the importance of adding noise to our input embeddings as defined in Section \ref{sec:sdtm-lex-reg}, we show the performance of \sdtm with and without this regularization in Table \ref{tab:lex-reg-ablation}. 
%The results show that without regularization, 

\begin{table}
  \caption{Comparing \sdtm's accuracy on SCAN 1-shot lexical OOD generalization with and without lexical regularization. We use LAUD to embed the output sequence in a tree.}
  \label{tab:lex-reg-ablation}
  \centering
  \begin{tabular}{ll}
    \toprule
    With noise     & .87 \\
    Without noise & 0.0 \\
    \bottomrule
\end{tabular}
\end{table}




\subsection{Dataset Preprocessing} \label{sec:sdtm-data-preprocessing}
We preprocessed GeoQuery according to the steps from \citet{shaw-etal-2021-compositional}. FOR2LAM and GeoQuery both contain non-binary trees, which we convert to binary form using Chomsky normal form. When a new node is inserted to make a branch binary, we use the token <NT>. For output sequences with length one embedded according to left-aligned uniform-depth, we make the single token the left child of a new <NT> root node.

\subsection{0-shot Lexical Test Generation}
For both FOR2LAM and SCAN, we introduce 0-shot lexical tests. For FOR2LAM, we do this by replacing every occurrence of `x' in the test set with a new token `z'. For the SCAN 0-shot set, we start with the 1-shot lexical test set and remove the sample containing the 1-shot word `jump'. We alter the output vocabulary to use the same tokens as the input vocabulary, since it is impossible for a word level model to translate between an input and output word without any exposure to that word.

\subsection{DTM Training Details} \label{sec:sdtm-dtm-training}
When applicable, we adopt the hyperparameters from \citet{Soulos_2023_DifferentiableTreeOperations}. Below we list the newly introduced hyperparameters and changes we made to existing parameters.

\citet{Soulos_2023_DifferentiableTreeOperations} set the dimensionality of the embeddings to be equal to the size of vocabulary. This works for the datasets with small vocabulary examined in the original paper. We keep this setting for Active$\leftrightarrow$Logical, but set the embedding dimension to 64 for FOR2LAM, and 128 for GeoQuery and SCAN. We also changed the loss function from mean-squared error to cross entropy.

For each new task, we need to decide how many layers to use for \sdtm. We followed the heuristic of doubling the max tree depth for the models with sequence input and quadrupling the number of layers for tree input. This leads to 56 layers for FOR2LAM, 22 layers for GeoQuery, and 14 layers for SCAN.

Pooling by multi-headed attention \ref{sec:sdtm-mha} introduces new hyperparameters such as number of pooling heads and pooling key dimensionality, and we set the value of these to be the same as the Transformer hyperparameters for the agent. Tree pruning \ref{sec:sdtm-pruning} introduces a new hyperparameter $k$ for the maximum number of nodes to keep. In general, a larger $k$ is better but uses more memory. For Active$\leftrightarrow$Logical we set $k=1024$, for FOR2LAM $k=1024$, for GeoQuery $k=2048$, and for SCAN $k=256$. With the memory savings from \abvrepname, pooling by multi-headed attention, and pruning, we increase the batch size from 16 to 64. We also increased the agent's model dimension to 256 with 8 heads of attention due to the memory savings except for Active$\leftrightarrow$Logical where we matched the original hyperparameters.

Random positional embeddings (RPE) also introduce a new hyperparameter for the max input integer, and we set this to be double the max input length. This leads to an RPE hyperparameter of 44 for GeoQuery and 18 for SCAN.

We noticed that randomly initializing and freezing our embedding vectors was essential for \sdtm to achieve 0-shot generalization on SCAN.

For the results, we reported the best run of 5 random seeds. Like \dtm, \sdtm suffers from high variance. Some runs get stuck in local optima and fail to achieve moderate performance on the training set, which leads to poor performance on the test sets. This is a known issue with models that use superposition data structures, and reporting the best run over a number of random seeds has been previously used \citep{yogatama2018memory, dusell2024stack}.

\subsection{\dtm Summary Statistics} \label{sec:sdtm-statistics}
Mean and standard deviation accuracies are shown in Tables \ref{tab:activelogical-stats}, \ref{tab:for2lam-geoquery-stats}, and \ref{tab:scan-stats}.

\begin{table}
  \caption{\textbf{Summary statistics for Active$\leftrightarrow$Logical.} Mean and standard deviation accuracies are shown.}
  \label{tab:activelogical-stats}
  \centering
  \begin{tabular}{llll}
    \toprule
    Model & \multicolumn{3}{c}{Split}         \\
    \cmidrule(r){2-4}
    & IID & \begin{tabular}[c]{@{}l@{}}0-Shot \\ Lexical\end{tabular} & Structural \\
    \cmidrule(r){1-4}
    Original \dtm  & $1.0\pm0.0$ & $1.0\pm0.0$ & $1.0\pm0.0$ \\ 
    \sdtm  & $1.0\pm0.0$ & $1.0\pm0.0$ & $1.0\pm0.0$    \\
    \sdtm (pruned k=1024) & $1.0\pm0.0$ & $1.0\pm0.0$ & $.86\pm.04$  \\
    \bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{\textbf{Summary statistics for FOR2LAM and GeoQuery.} Mean and standard deviation accuracies are shown.}
\centering
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{2}{c}{\textbf{FOR2LAM}} & \multicolumn{4}{c}{\textbf{GeoQuery}} \\
\cmidrule(r){2-3} \cmidrule(l){4-7}
Model & IID & 0-shot lexical & IID & Length & Template & TMCD \\
\midrule
\sdtm & 1.0$\pm$0.0 & .43$\pm$.11 & .68$\pm$.01 & .19$\pm$.01 & .16$\pm$.03 & .35$\pm$0.0 \\
\bottomrule
\end{tabular}
\label{tab:for2lam-geoquery-stats}
\end{table} 

\begin{table}
  \caption{\textbf{Summary statistics for SCAN.} Mean and standard deviation accuracies are shown.}
  \label{tab:scan-stats}
  \centering
  \begin{tabular}{lllllll}
    \toprule
    Model & \multicolumn{6}{c}{Split}         \\
    \cmidrule(r){2-7}
    & IID & \begin{tabular}[c]{@{}l@{}}1-Shot \\ Lexical\end{tabular} & \begin{tabular}[c]{@{}l@{}}0-Shot \\ Lexical\end{tabular} & Length & Template & MCD \\
    \midrule
    \sdtm (parse trees)     & .80$\pm$.39  & .51$\pm$.39 & .80$\pm$.34 & .34$\pm$.30 & .19$\pm$.38 & .02$\pm$.01  \\
    \sdtm (LAUD trees) & 1.0$\pm$0.0 & .70$\pm$.15 & .75$\pm$.22 & .04$\pm$.01 & .84$\pm$.12 & 0.0$\pm$0.0 \\
    \dtm (parse trees) & 0.0$\pm$0.0 & 0.0$\pm$0.0 & 0.0$\pm$0.0 & 0.0$\pm$0.0 & 0.0$\pm$0.0 & 0.0$\pm$0.0 \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Baseline Training Details} \label{sec:sdtm-baseline-training}
\textbf{NQG:} Active$\leftrightarrow$Logical rule induction used the following hyperparameters: sample size=training set size, terminal code length=8, allow repeated nts=True. The terminal code length setting was obtained via grid search over the values 1, 8, 32. For the actual training of the model we follow the hyperparameters utilised by \cite{Sun_2023_ReplicationStudyCompositional, shaw-etal-2021-compositional}. FOR2LAM used the same hyperparameters with the exception of sample size which had to be set to 1000 as additional increases became computationally intractable. Even under these settings rule induction took 42 hours on a machine with 64gb of ram. Writing the training set would take an additional week of processing time, which we considered computationally too expensive.% at time of submission. However, if required by the reviewers we can include these results for the camera ready version

\textbf{Transformer:} We followed the same hyperparameters obtained via grid search from \cite{Soulos_2023_DifferentiableTreeOperations}. Specifically these are: 30,000 steps of which 1000 were warmup and linear learning rate decay; batch size 256; one encoder layer and three decoder layer each with a hidden dimension of 1024 and two attention heads; the optimizer was Adam. 

\textbf{RU-Transformer:} We followed the hyperparameters reported by \cite{csordas-etal-2021-devil}. These are: 128 dimension hidden size with 256 feedforward; 8 attention heads; 3 layers; batch size 256; trained using Adam with learning rate $10^{-3}$. 

\subsection{Compute resources} \label{sec:sdtm-compute-resources}
All reported \sdtm runs could be processed on NVIDIA 16gb V100 GPUs. Depending on availability, we ran some seeds on 80gb H100 GPUs, but this is not necessary. The Transformer baselines were also run on NVIDIA 16gb V100 GPUs. NQG used NVIDIA 40gb A100 GPUs. The GPUs we used were hosted on an internal cluster. 

Designing our architecture involved many preliminary experiments that are not reported in the paper.

\subsection{Dataset Statistics and Samples} \label{sec:sdtm-data-stats-samples}
Example input and output pairs are shown for Active$\leftrightarrow$Logical in Figure \ref{fig:active-logical-sample}  FOR2LAM in Figure \ref{fig:for2lam-sample}, GeoQuery in Figure \ref{fig:geoquery-sample}, and SCAN in Figure \ref{fig:root-and-laud}. The memory usage of \dtm grows exponentially with tree depth, so we present the max depth of the datasets here:
\begin{itemize}
    \item Active$\leftrightarrow$Logical: max tree depth 10
    \item FOR2LAM: max tree depth 14
    \item GeoQuery: max tree depth 16
    \item SCAN: max tree depth 8
\end{itemize}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/sdtm/active_logical_example.pdf}
    \caption{An input and output pair from Active$\leftrightarrow$Logical. }
    \label{fig:active-logical-sample}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/sdtm/for2lam_example.pdf}
    \caption{An input and output pair from FOR2LAM. }
    \label{fig:for2lam-sample}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/sdtm/geoquery_example.pdf}
    \caption{An input and output pair from GeoQuery. }
    \label{fig:geoquery-sample}
\end{figure}

\subsection{Licenses} \label{sec:sdtm-licenses}
\textbf{Baselines:}
\begin{itemize}
    \item DTM: Permissive 2.0
    \item Transformer: BSD-3 (Pytorch implementation)
    \item RU-Transformer: MIT Licence 
    \item NQG: Apache 2.0
\end{itemize}

\textbf{Datasets:}
\begin{itemize}
    \item GeoQuery: GLP 2.0
    \item SCAN: BSD
    \item Active$\leftrightarrow$Logical: Permissive 2.0
    \item FOR2LAM: Not public (no licence obtained through email request to original authors)
\end{itemize}