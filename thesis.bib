%%%% this sample bibliography has been taken from Overleaf

@article{einstein,
  author =       "Albert Einstein",
  title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
                 [{On} the electrodynamics of moving bodies]",
  journal =      "Annalen der Physik",
  volume =       "322",
  number =       "10",
  pages =        "891--921",
  year =         "1905",
  DOI =          "http://dx.doi.org/10.1002/andp.19053221004",
  keywords =     "physics"
}

@inproceedings{soulos-etal-2020-discovering,
    title = "Discovering the Compositional Structure of Vector Representations with Role Learning Networks",
    author = "Soulos, Paul  and
      McCoy, R. Thomas  and
      Linzen, Tal  and
      Smolensky, Paul",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa{\l}a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.23/",
    doi = "10.18653/v1/2020.blackboxnlp-1.23",
    pages = "238--254",
    abstract = "How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model`s output is changed in the way predicted by our analysis."
}

@InProceedings{pmlr-v202-soulos23a,
  title = 	 {Differentiable Tree Operations Promote Compositional Generalization},
  author =       {Soulos, Paul and Hu, Edward J and Mccurdy, Kate and Chen, Yunmo and Fernandez, Roland and Smolensky, Paul and Gao, Jianfeng},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32499--32520},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/soulos23a/soulos23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/soulos23a.html},
  abstract = 	 {In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine (DTM) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, DTM achieves 100\% while existing baselines such as Transformer, Tree Transformer, LSTM, and Tree2Tree LSTM achieve less than 30\%. DTM remains highly interpretable in addition to its perfect performance.}
}


@book{dirac,
  title={The Principles of Quantum Mechanics},
  author={Paul Adrien Maurice Dirac},
  isbn={9780198520115},
  series={International series of monographs on physics},
  year={1981},
  publisher={Clarendon Press},
  keywords = {physics}
}

@book{latexcompanion,
    author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
    title     = "The \LaTeX\ Companion",
    year      = "1993",
    publisher = "Addison-Wesley",
    address   = "Reading, Massachusetts",
    keywords  = "latex"
}
 
@online{knuthwebsite,
    author    = "Donald Knuth",
    title     = "Knuth: Computers and Typesetting",
    url       = "http://www-cs-faculty.stanford.edu/~uno/abcde.html",
    addendum = "(accessed: 01.09.2016)",
    keywords  = "latex,knuth"
}

@inbook{knuth-fa,
   author = "Donald E. Knuth",
   title = "Fundamental Algorithms",
   publisher = "Addison-Wesley",
   year = "1973",
   chapter = "1.2",
   keywords  = "knuth,programming"
}

@book{knuth-acp,
   author = "Donald E. Knuth",
   publisher = "Addison-Wesley",
   title = "The Art of Computer Programming",
   series = "Four volumes",
   year = "1968",
   note = "Seven volumes planned",
   keywords  = "knuth,programming"
}

@article{ctan,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351",
    keywords  = "latex"
}



@book{pearl2000causality,
  title={Causality},
  author={Judea Pearl},
  publisher={MIT Press},
  address={Cambridge, MA},
  year={2000}
}


@article{fodor1990connectionism,
  title={Connectionism and the problem of systematicity: Why {Smolensky's} solution doesn't work},
  author={Fodor, Jerry and McLaughlin, Brian P},
  journal={Cognition},
  volume={35},
  number={2},
  pages={183--204},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{giulianelli2018hood,
    title = "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information",
    author = "Giulianelli, Mario  and
      Harding, Jack  and
      Mohnert, Florian  and
      Hupkes, Dieuwke  and
      Zuidema, Willem",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5426",
    doi = "10.18653/v1/W18-5426",
    pages = "240--248",
}

@article{fodor1997connectionism,
  title={Connectionism and the problem of systematicity (continued): Why {Smolensky's} solution still doesn't work},
  author={Fodor, Jerry},
  journal={Cognition},
  volume={62},
  number={1},
  pages={109--119},
  year={1997},
  publisher={Elsevier}
}

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@article{smolensky1987constituent,
  title={The constituent structure of connectionist mental states: A reply to {Fodor and Pylyshyn}},
  author={Smolensky, Paul},
  journal={Southern Journal of Philosophy},
  volume={26},
  number={Supplement},
  pages={137--161},
  year={1987}
}

@incollection{smolensky1991connectionism,
  title={Connectionism, constituency, and the language of thought},
  author={Smolensky, Paul},
  booktitle={Meaning in Mind: Fodor and his Critics},
  editor={Barry Loewer and Georges Rey},
  pages={201--227},
  publisher={Basil Blackwell},
  address={Oxford},
  year={1991}
}

@inproceedings{mccoy,
title={{RNN}s implicitly implement tensor-product representations},
author={R. Thomas McCoy and Tal Linzen and Ewan Dunbar and Paul Smolensky},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJx0sjC5FX},
}

@inproceedings{
andreas2019measuring,
title={Measuring Compositionality in Representation Learning},
author={Jacob Andreas},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJz05o0qK7},
}

@inproceedings{abnar2019blackbox,
    title = "Blackbox Meets Blackbox: Representational Similarity {\&} Stability Analysis of Neural Language Models and Brains",
    author = "Abnar, Samira  and
      Beinborn, Lisa  and
      Choenni, Rochelle  and
      Zuidema, Willem",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4820",
    doi = "10.18653/v1/W19-4820",
    pages = "191--203",
}

@inproceedings{chrupala2019correlating,
    title = "Correlating Neural and Symbolic Representations of Language",
    author = "Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1283",
    doi = "10.18653/v1/P19-1283",
    pages = "2952--2962",
}

@article{hupkes2018visualisation,
  title={Visualisation and `diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
  author={Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={907--926},
  year={2018}
}


@article{hupkes2020compositionality,
  title={Compositionality Decomposed: How do Neural Networks Generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@inproceedings{lake2018generalization,
  title={Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author={Brenden M. Lake and Marco Baroni},
  booktitle={International Conference on Machine Learning},
  url={https://arxiv.org/pdf/1711.00350.pdf},
  year={2018}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3104--3112},
  year={2014},
  url={https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}


@article{syntaxattention,
  title={Compositional generalization in a deep seq2seq model by separating syntax and semantics},
  author={Russin, Jake and Jo, Jason and O'Reilly, Randall C and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1904.09708},
  year={2019},
  url={https://arxiv.org/abs/1904.09708}
}

@inproceedings{mikolov2016roadmap,
  title={A roadmap towards machine intelligence},
  author={Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
  booktitle={International Conference on Intelligent Text Processing and Computational Linguistics},
  pages={29--61},
  year={2016},
  organization={Springer}
}

@book{Smolensky:2006:HMN:1205244,
 author = {Smolensky, Paul and Legendre, G{\'e}raldine},
 title = {The Harmonic Mind: From Neural Computation to Optimality-Theoretic GrammarVolume I: Cognitive Architecture (Bradford Books)},
 year = {2006},
 isbn = {0262195267},
 publisher = {The MIT Press},
}


@article{structure,
author = {McClelland, James and Botvinick, Matthew and Noelle, David and Plaut, David and Rogers, Timothy and Seidenberg, Mark and Smith, Linda},
year = {2010},
month = {08},
pages = {348-56},
title = {Letting Structure Emerge: Connectionist and Dynamical Systems Approaches to Cognition},
volume = {14},
journal = {Trends in cognitive sciences},
doi = {10.1016/j.tics.2010.06.002}
}

@inproceedings{palangi,
  title={Question-Answering with Grammatically-Interpretable Representations},
  author={Hamid Palangi and Paul Smolensky and Xiaodong He and Li Deng},
  booktitle={Proceedings of the Association for the Advancement of Artificial Intelligence},
  url={https://arxiv.org/pdf/1705.08432.pdf},
  year={2017}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{googlenmt,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016},
  url={https://arxiv.org/abs/1609.08144}
}


@inproceedings{kiros2015skip,
  title={Skip-thought vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3294--3302},
  year={2015},
  url={https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf}
}

@InProceedings{conneau2017supervised,
  author = 	"Conneau, Alexis
		and Kiela, Douwe
		and Schwenk, Holger
		and Barrault, Lo{\"i}c
		and Bordes, Antoine",
  title = 	"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"670--680",
  location = 	"Copenhagen, Denmark",
  url = 	"http://aclweb.org/anthology/D17-1070"
}

@inproceedings{socher2013recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}


@inproceedings{bowman2015large,
	Address = {Lisbon, Portugal},
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	Booktitle = {{Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}},
	Date-Added = {2015-11-06 18:15:56 +0000},
	Date-Modified = {2015-11-06 18:16:03 +0000},
	Month = {September},
	Pages = {632--642},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Url = {http://aclweb.org/anthology/D15-1075},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QaVBhcGVycy9TZW1hbnRpY3MvQm93bWFuIGV0IGFsIDIwMTUgLSBBIGxhcmdlIGFubm90YXRlZCBjb3JwdXMgZm9yIGxlYXJuaW5nIG5hdHVyYWwgbGFuZ3VhZ2UgaW5mZXJlbmNlLnBkZtIXCxgZV05TLmRhdGFPEQKaAAAAAAKaAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADVVNe4SCsAAAAN1SkfQm93bWFuIGV0IGFsIDIwMTUgLSAjMTg0MEU2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABhA5tJiWEYAAAAAAAAAAAABAAQAAAkgAAAAAAAAAAAAAAAAAAAACVNlbWFudGljcwAAEAAIAADVVQ/4AAAAEQAIAADSYp6WAAAAAQAUAA3VKQANvDIADbqKAA2SfQAGadwAAgBZTWFjaW50b3NoIEhEOlVzZXJzOgB0bGluemVuMToARHJvcGJveDoAUGFwZXJzOgBTZW1hbnRpY3M6AEJvd21hbiBldCBhbCAyMDE1IC0gIzE4NDBFNi5wZGYAAA4AsgBYAEIAbwB3AG0AYQBuACAAZQB0ACAAYQBsACAAMgAwADEANQAgAC0AIABBACAAbABhAHIAZwBlACAAYQBuAG4AbwB0AGEAdABlAGQAIABjAG8AcgBwAHUAcwAgAGYAbwByACAAbABlAGEAcgBuAGkAbgBnACAAbgBhAHQAdQByAGEAbAAgAGwAYQBuAGcAdQBhAGcAZQAgAGkAbgBmAGUAcgBlAG4AYwBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgCAVXNlcnMvdGxpbnplbjEvRHJvcGJveC9QYXBlcnMvU2VtYW50aWNzL0Jvd21hbiBldCBhbCAyMDE1IC0gQSBsYXJnZSBhbm5vdGF0ZWQgY29ycHVzIGZvciBsZWFybmluZyBuYXR1cmFsIGxhbmd1YWdlIGluZmVyZW5jZS5wZGYAEwABLwAAFQACAA///wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgD6AP8BBwOlA6cDrAO3A8ADzgPSA9kD4gPnA/QD9wQJBAwEEQAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAQT}}
	
@inproceedings{mccoy2019right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, R. Thomas  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
}

@inproceedings{jawahar-etal-2019-bert,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1356",
    doi = "10.18653/v1/P19-1356",
    pages = "3651--3657",
    abstract = "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",
}


@article{linzen2016assessing,
  title={Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the ACL},
  year={2016},
  url={https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00115}
}

@inproceedings{poliak2018collecting,
    title = "Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation",
    author = "Poliak, Adam  and
      Haldar, Aparajita  and
      Rudinger, Rachel  and
      Hu, J. Edward  and
      Pavlick, Ellie  and
      White, Aaron Steven  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1007",
    doi = "10.18653/v1/D18-1007",
    pages = "67--81",
}


@article{dasgupta2019analyzing,
  title={Analyzing machine-learned representations: A natural language case study},
  author={Dasgupta, Ishita and Guo, Demi and Gershman, Samuel J and Goodman, Noah D},
  journal={arXiv preprint arXiv:1909.05885},
  year={2019},
  url={https://arxiv.org/abs/1909.05885}
}

@inproceedings{kingma2015adam,
	Author = {Diederik Kingma and Jimmy Ba},
	Booktitle = {International Conference for Learning Representations},
	Title = {Adam: A Method for Stochastic Optimization},
	url={https://arxiv.org/pdf/1412.6980.pdf},
	Year = {2015}}


@inproceedings{wang2018glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
}


@InProceedings{manning-EtAl:2014:P14-5,
  author    = {Manning, Christopher D. and  Surdeanu, Mihai  and  Bauer, John  and  Finkel, Jenny  and  Bethard, Steven J. and  McClosky, David},
  title     = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
  booktitle = {Association for Computational Linguistics (ACL) System Demonstrations},
  year      = {2014},
  pages     = {55--60},
  url       = {http://www.aclweb.org/anthology/P/P14/P14-5010}
}

@incollection{mu2020compositional,
  title={Compositional Explanations of Neurons},
  author={Mu, Jesse and Andreas, Jacob},
  booktitle={Advances in Neural Information Processing Systems 33},
  year={2020},
  url={https://arxiv.org/pdf/2006.14032.pdf}
}

@inproceedings{
tenney2018what,
title={What do you learn from context? Probing for sentence structure in contextualized word representations},
author={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R. Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJzSgnRcKX},
}


@inproceedings{peters2018dissecting,
    title = "Dissecting Contextual Word Embeddings: Architecture and Representation",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Zettlemoyer, Luke  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1179",
    doi = "10.18653/v1/D18-1179",
    pages = "1499--1509",
}



@inproceedings{blevins2018hierarchical,
    title = "Deep {RNN}s Encode Soft Hierarchical Syntax",
    author = "Blevins, Terra  and
      Levy, Omer  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2003",
    doi = "10.18653/v1/P18-2003",
    pages = "14--19",
}


@inproceedings{belinkov2017evaluating,
    title = "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    author = "Belinkov, Yonatan  and
      M{\`a}rquez, Llu{\'\i}s  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Glass, James",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/I17-1001",
    pages = "1--10",
}


@inproceedings{conneau2018senteval,
    title = "{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations",
    author = "Conneau, Alexis  and
      Kiela, Douwe",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1269",
}

@inproceedings{conneau2018cram,
    title = "What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties",
    author = {Conneau, Alexis  and
      Kruszewski, German  and
      Lample, Guillaume  and
      Barrault, Lo{\"\i}c  and
      Baroni, Marco},
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1198",
    doi = "10.18653/v1/P18-1198",
    pages = "2126--2136",
}

@inproceedings{adi2016fine,
  title={Fine-grained analysis of sentence embeddings using auxiliary prediction tasks},
  author={Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
  booktitle = {International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/pdf?id=BJh6Ztuxl}
}

@article{wickelgren1969context,
  title={Context-sensitive coding, associative memory, and serial order in (speech) behavior.},
  author={Wickelgren, Wayne A.},
  journal={Psychological Review},
  volume={76},
  number={1},
  pages={1--15},
  year={1969},
  publisher={American Psychological Association}
}

@article{vanmassenhove2017investigating,
  title={Investigating `aspect' in {NMT} and {SMT}: Translating the {English} simple past and present perfect},
  author={Vanmassenhove, Eva and Du, Jinhua and Way, Andy},
  journal={Computational Linguistics in the Netherlands Journal},
  volume={7},
  pages={109--128},
  year={2017}
}

@inproceedings{bowman2016fast,
	Author = {Bowman, Samuel R. and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D. and Potts, Christopher},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Pages = {1466--1477},
	Publisher = {Association for Computational Linguistics},
	Title = {A Fast Unified Model for Parsing and Sentence Understanding},
	Url = {http://aclweb.org/anthology/P16-1139},
	Year = {2016}}

@inproceedings{vMeasure,
  added-at = {2011-09-18T23:01:04.000+0200},
  author = {Rosenberg, Andrew and Hirschberg, Julia},
  biburl = {https://www.bibsonomy.org/bibtex/238531ca6d31f767da8aef74bd6dadcf7/jil},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP-CoNLL)},
  description = {ESOM},
  interhash = {07bb9cbf44b156e34d57c40b9d2fd314},
  intrahash = {38531ca6d31f767da8aef74bd6dadcf7},
  keywords = {cluster clustering measure v-measure validation},
  pages = {410--420},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {{V}-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure},
  year = 2007
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019},
  url={https://www.aclweb.org/anthology/N19-1419/}
}

@article{Hochreiter:1997:LSM:1246443.1246450,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Computation},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@inproceedings{softattention,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  crossref  = {DBLP:conf/iclr/2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Smolensky:1990:TPV:102418.102425,
 author = {Smolensky, Paul},
 title = {Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
 journal = {Artif. Intell.},
 issue_date = {Nov. 1990},
 volume = {46},
 number = {1-2},
 month = nov,
 year = {1990},
 issn = {0004-3702},
 pages = {159--216},
 numpages = {58},
 url = {http://dx.doi.org/10.1016/0004-3702(90)90007-M},
 doi = {10.1016/0004-3702(90)90007-M},
 acmid = {102425},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
} 

@article{omlin1996extraction,
  title={Extraction of rules from discrete-time recurrent neural networks},
  author={Omlin, Christian W and Giles, C Lee},
  journal={Neural networks},
  volume={9},
  number={1},
  pages={41--52},
  year={1996},
  publisher={Elsevier}
}

@incollection{NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}


@inproceedings{weiss2018extracting,
  author    = {Gail Weiss and
               Yoav Goldberg and
               Eran Yahav},
  title     = {Extracting Automata from Recurrent Neural Networks Using Queries and
               Counterexamples},
  booktitle = {{International Conference on Machine Learning}},
  pages     = {5244--5253},
  year      = {2018},
  url={http://proceedings.mlr.press/v80/weiss18a.html}
}

@article{belinkov2019analysis,
  title={Analysis methods in neural language processing: A survey},
  author={Belinkov, Yonatan and Glass, James},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={49--72},
  year={2019},
  publisher={MIT Press},
  url={https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00254}
}

@inproceedings{mikolov2013linguistic,
    title = "Linguistic Regularities in Continuous Space Word Representations",
    author = "Mikolov, Tomas  and
      Yih, Wen-tau  and
      Zweig, Geoffrey",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1090",
    pages = "746--751",
}

@inproceedings{parikh2016decomposable,
    title = "A Decomposable Attention Model for Natural Language Inference",
    author = {Parikh, Ankur  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Das, Dipanjan  and
      Uszkoreit, Jakob},
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1244",
    doi = "10.18653/v1/D16-1244",
    pages = "2249--2255",
}

@inproceedings{klein2003accurate,
  title={Accurate unlexicalized parsing},
  author={Klein, Dan and Manning, Christopher D},
  booktitle={Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1},
  pages={423--430},
  year={2003},
  url={https://www.aclweb.org/anthology/P03-1054.pdf},
  organization={Association for Computational Linguistics}
}


@inproceedings{lakretz2019emergence,
    title = "The emergence of number and syntax units in {LSTM} language models",
    author = "Lakretz, Yair  and
      Kruszewski, German  and
      Desbordes, Theo  and
      Hupkes, Dieuwke  and
      Dehaene, Stanislas  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1002",
    doi = "10.18653/v1/N19-1002",
    pages = "11--20",
}


@inproceedings{
shen2018ordered,
title={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},
author={Yikang Shen and Shawn Tan and Alessandro Sordoni and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1l6qiR5F7},
}

@article{Voita2020InformationTheoreticPW,
  title={Information-Theoretic Probing with Minimum Description Length},
  author={Elena Voita and Ivan Titov},
  journal={arXiv preprint arXiv:2003.12298},
  year={2020},
  url={https://arxiv.org/abs/2003.12298}
}

@inproceedings{li-etal-2019-compositional,
    title = "Compositional Generalization for Primitive Substitutions",
    author = "Li, Yuanpeng  and
      Zhao, Liang  and
      Wang, Jianyu  and
      Hestness, Joel",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1438",
    doi = "10.18653/v1/D19-1438",
    pages = "4293--4302",
    abstract = "Compositional generalization is a basic mechanism in human language learning, but current neural networks lack such ability. In this paper, we conduct fundamental research for encoding compositionality in neural networks. Conventional methods use a single representation for the input sentence, making it hard to apply prior knowledge of compositionality. In contrast, our approach leverages such knowledge with two representations, one generating attention maps, and the other mapping attended input words to output symbols. We reduce the entropy in each representation to improve generalization. Our experiments demonstrate significant improvements over the conventional methods in five NLP tasks including instruction learning and machine translation. In the SCAN domain, it boosts accuracies from 14.0{\%} to 98.8{\%} in Jump task, and from 92.0{\%} to 99.7{\%} in TurnLeft task. It also beats human performance on a few-shot learning task. We hope the proposed approach can help ease future research towards human-level compositional language learning.",
}

@article{warstadt2019blimp,
  title={{BLiMP}: A Benchmark of Linguistic Minimal Pairs for English},
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  journal={Proceedings of the
Society for Computation in Linguistics.},
  year={2020},
  url={https://scholarworks.umass.edu/scil/vol3/iss1/43/}
}


@inproceedings{hu2020systematic,
    title = "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
    author = "Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url="https://www.aclweb.org/anthology/2020.acl-main.158.pdf",
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020},
  url={https://arxiv.org/abs/2005.14165}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017},
  url={https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@inproceedings{goodwin2020probing,
    title = "Probing Linguistic Systematicity",
    author = "Goodwin, Emily  and
      Sinha, Koustuv  and
      O{'}Donnell, Timothy J.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.177",
    doi = "10.18653/v1/2020.acl-main.177",
    pages = "1958--1969",
    abstract = "Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.",
}

@article{ravichander2020probing,
  title={Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?},
  author={Ravichander, Abhilasha and Belinkov, Yonatan and Hovy, Eduard},
  journal={arXiv preprint arXiv:2005.00719},
  year={2020}
}

@misc{najoung,
  doi = {10.48550/ARXIV.2212.10769},
  url = {https://arxiv.org/abs/2212.10769},
  author = {Kim, Najoung and Linzen, Tal and Smolensky, Paul},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@inproceedings{csordas2021devil,
  title={The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, Juergen},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={619--634},
  year={2021}
}


@article{Smolensky1990TensorPV,
  title={Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
  author={Paul Smolensky},
  journal={Artif. Intell.},
  year={1990},
  volume={46},
  pages={159-216}
}

@article{chen2018tree,
  title={Tree-to-tree neural networks for program translation},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{Graves2014NeuralTM,
  title={Neural Turing Machines},
  author={Alex Graves and Greg Wayne and Ivo Danihelka},
  journal={ArXiv},
  year={2014},
  volume={abs/1410.5401}
}

@article{Newell1980PhysicalSS,
  title={Physical Symbol Systems},
  author={Allen Newell},
  journal={Cogn. Sci.},
  year={1980},
  volume={4},
  pages={135-183}
}

@article{Weston2014MemoryN,
  title={Memory Networks},
  author={Jason Weston and Sumit Chopra and Antoine Bordes},
  journal={CoRR},
  year={2014},
  volume={abs/1410.3916}
}

@article{Graves2016HybridCU,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Alex Graves and Greg Wayne and Malcolm Reynolds and Tim Harley and Ivo Danihelka and Agnieszka Grabska-Barwinska and Sergio Gomez Colmenarejo and Edward Grefenstette and Tiago Ramalho and John P. Agapiou and Adri{\`a} Puigdom{\`e}nech Badia and Karl Moritz Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and Christopher Summerfield and Phil Blunsom and Koray Kavukcuoglu and Demis Hassabis},
  journal={Nature},
  year={2016},
  volume={538},
  pages={471-476}
}

@inproceedings{palangi2018question,
  title={Question-answering with grammatically-interpretable representations},
  author={Palangi, Hamid and Smolensky, Paul and He, Xiaodong and Deng, Li},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{jiang2021enriching,
  title={Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization},
  author={Jiang, Yichen and Celikyilmaz, Asli and Smolensky, Paul and Soulos, Paul and Rao, Sudha and Palangi, Hamid and Fernandez, Roland and Smith, Caitlin and Bansal, Mohit and Gao, Jianfeng},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4780--4793},
  year={2021}
}

@article{soulos2021structural,
  title={Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages},
  author={Soulos, Paul and Rao, Sudha and Smith, Caitlin and Rosen, Eric and Celikyilmaz, Asli and McCoy, R Thomas and Jiang, Yichen and Haley, Coleman and Fernandez, Roland and Palangi, Hamid and others},
  journal={Proceedings of Machine Translation Summit XVIII},
  year={2021}
}

@article{Reed2015NeuralP,
  title={Neural Programmer-Interpreters},
  author={Scott E. Reed and Nando de Freitas},
  journal={CoRR},
  year={2015},
  volume={abs/1511.06279}
}

@inproceedings{tai-etal-2015-improved,
    title = "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
    author = "Tai, Kai Sheng  and
      Socher, Richard  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1150",
    doi = "10.3115/v1/P15-1150",
    pages = "1556--1566",
}

@inproceedings{wang-etal-2019-tree,
    title = "Tree Transformer: Integrating Tree Structures into Self-Attention",
    author = "Wang, Yaushian  and
      Lee, Hung-Yi  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1098",
    doi = "10.18653/v1/D19-1098",
    pages = "1061--1070",
    abstract = "Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed {``}Constituent Attention{''} module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.",
}


@InProceedings{bosnjak17a,
  title = 	 {Programming with a Differentiable Forth Interpreter},
  author =       {Matko Bo{\v{s}}njak and Tim Rockt{\"a}schel and Jason Naradowsky and Sebastian Riedel},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {547--556},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/bosnjak17a/bosnjak17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/bosnjak17a.html},
  abstract = 	 {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.}
}

@inproceedings{cho-etal-2014-properties,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@inproceedings{sutskever,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}



@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}

@inproceedings{vaswani,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{shiv2019novel,
  title={Novel positional encodings to enable tree-based transformers},
  author={Shiv, Vighnesh and Quirk, Chris},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kleyko2022,
author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
title = {A Survey on Hyperdimensional Computing Aka Vector Symbolic Architectures, Part I: Models and Data Transformations},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3538531},
doi = {10.1145/3538531},
abstract = {This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and distributed vector representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent years, the necessity for a comprehensive survey of the field has become extremely important. Therefore, amongst other aspects of the field, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey [84] is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {130},
numpages = {40},
keywords = {tensor product representations, holographic reduced representations, machine learning, Artificial intelligence, modular composite representations, sparse binary distributed representations, sparse block codes, multiply-add-permute, geometric analogue of holographic reduced representations, data structures, hyperdimensional computing, binary spatter codes, distributed representations, matrix binding of additive terms, vector symbolic architectures}
}


@inproceedings{gayler2003vsa_jackendoff,
  title={Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience},
  author = {Ross W Gayler},
  booktitle = {Proceedings of the ICCS/ASCS Joint International Conference on Cognitive Science (ICCS/ASCS 2003)},
  pages={133--138},
  month = {07},
  year={2003},
  date={2003-07-17},
  publisher = {University of New South Wales},
  address = {Sydney, NSW, AU},
  editor = {Slezak, Peter},
  archivePrefix = {arXiv},
  arxivId = {cs/0412059v1},
  eprint = {0412059v1},
  primaryClass = {cs},
  url = {http://arxiv.org/abs/cs/0412059}
}

@article{kanerva2009hyperdimensional,
  title={Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors},
  author={Kanerva, Pentti},
  journal={Cognitive computation},
  volume={1},
  pages={139--159},
  year={2009},
  publisher={Springer}
}


@article{sartran2022transformer,
  title={Transformer Grammars: Augmenting Transformer language models with syntactic inductive biases at scale},
  author={Sartran, Laurent and Barrett, Samuel and Kuncoro, Adhiguna and Stanojevi{\'c}, Milo{\v{s}} and Blunsom, Phil and Dyer, Chris},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={1423--1439},
  year={2022},
  publisher={MIT Press}
}

@inproceedings{
maddison2017the,
title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1jE5L5gl}
}

@inproceedings{
jang2017categorical,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}
@inproceedings{dong2016language,
  title={Language to Logical Form with Neural Attention},
  author={Dong, Li and Lapata, Mirella},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={33--43},
  year={2016}
}

@InProceedings{chen,
  title = 	 {Mapping natural-language problems to formal-language solutions using structured neural representations},
  author =       {Chen, Kezhen and Huang, Qiuyuan and Palangi, Hamid and Smolensky, Paul and Forbus, Ken and Gao, Jianfeng},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1566--1575},
  year = 	 {2020},
  editor = 	 {III, Hal Daum and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20g/chen20g.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20g.html},
  abstract = 	 {Generating formal-language programs represented by relational tuples, such as Lisp programs or mathematical operations, to solve problems stated in natural language is a challenging task because it requires explicitly capturing discrete symbolic structural information implicit in the input. However, most general neural sequence models do not explicitly capture such structural information, limiting their performance on these tasks. In this paper, we propose a new encoder-decoder model based on a structured neural representation, Tensor Product Representations (TPRs), for mapping Natural-language problems to Formal-language solutions, called TP-N2F. The encoder of TP-N2F employs TPR binding to encode natural-language symbolic structure in vector space and the decoder uses TPR unbinding to generate, in symbolic space, a sequential program represented by relational tuples, each consisting of a relation (or operation) and a number of arguments. TP-N2F considerably outperforms LSTM-based seq2seq models on two benchmarks and creates new state-of-the-art results. Ablation studies show that improvements can be attributed to the use of structured TPRs explicitly in both the encoder and decoder. Analysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F.}
}

@article{imanol,
  author    = {Imanol Schlag and
               Paul Smolensky and
               Roland Fernandez and
               Nebojsa Jojic and
               J{\"{u}}rgen Schmidhuber and
               Jianfeng Gao},
  title     = {Enhancing the Transformer with Explicit Relational Encoding for Math
               Problem Solving},
  journal   = {CoRR},
  volume    = {abs/1910.06611},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.06611},
  eprinttype = {arXiv},
  eprint    = {1910.06611},
  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-06611.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{soulos2020discovering,
  title={Discovering the Compositional Structure of Vector Representations with Role Learning Networks},
  author={Soulos, Paul and McCoy, R Thomas and Linzen, Tal and Smolensky, Paul},
  booktitle={Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  pages={238--254},
  year={2020}
}

@inproceedings{
mccoy2018rnns,
title={{RNN}s implicitly implement tensor-product representations},
author={R. Thomas McCoy and Tal Linzen and Ewan Dunbar and Paul Smolensky},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJx0sjC5FX},
}



@article{nram,
title	= {Neural Random Access Machines},
author	= {Karol Kurach and Marcin Andrychowicz and Ilya Sutskever},
year	= {2016},
URL	= {http://arxiv.org/abs/1511.06392},
journal	= {ICLR}
}

@article{Andreas2015NeuralMN,
  title={Neural Module Networks},
  author={Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Dan Klein},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={39-48}
}

@inproceedings{Joulin2015InferringAP,
  title={Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
  author={Armand Joulin and Tomas Mikolov},
  booktitle={NIPS},
  year={2015}
}

@article{Grefenstette2015LearningTT,
  title={Learning to Transduce with Unbounded Memory},
  author={Edward Grefenstette and Karl Moritz Hermann and Mustafa Suleyman and Phil Blunsom},
  journal={ArXiv},
  year={2015},
  volume={abs/1506.02516}
}

@inproceedings{kim-linzen-2020-cogs,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
}

@book{plate,
author = {Plate, Tony A.},
title = {Holographic Reduced Representation: Distributed Representation for Cognitive Structures},
year = {2003},
isbn = {1575864290},
publisher = {CSLI Publications},
address = {USA}
}


@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{
eliasmith,
author = {Chris Eliasmith  and Terrence C. Stewart  and Xuan Choo  and Trevor Bekolay  and Travis DeWolf  and Yichuan Tang  and Daniel Rasmussen },
title = {A Large-Scale Model of the Functioning Brain},
journal = {Science},
volume = {338},
number = {6111},
pages = {1202-1205},
year = {2012},
doi = {10.1126/science.1225266},
URL = {https://www.science.org/doi/abs/10.1126/science.1225266},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1225266},
abstract = {Neurons are pretty complicated cells. They display an endless variety of shapes that sprout highly variable numbers of axons and dendrites; they sport time- and voltage-dependent ion channels along with an impressive array of neurotransmitter receptors; and they connect intimately with near neighbors as well as former neighbors who have since moved away. Simulating a sizeable chunk of brain tissue has recently become achievable, thanks to advances in computer hardware and software. Eliasmith et al. (p. 1202; see the Perspective by Machens) present their million-neuron model of the brain and show that it can recognize numerals, remember lists of digits, and write down those liststasks that seem effortless for a human but that encompass the triad of perception, cognition, and behavior. Two-and-a-half million model neurons recognize images, learn via reinforcement, and display fluid intelligence. A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called Spaun) that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.}}


@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@inproceedings{patel-etal-2022-revisiting,
    title = "Revisiting the Compositional Generalization Abilities of Neural Sequence Models",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Blunsom, Phil  and
      Goyal, Navin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.46",
    doi = "10.18653/v1/2022.acl-short.46",
    pages = "424--434",
}

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@book{marcus2003algebraic,
  title={The algebraic mind: Integrating connectionism and cognitive science},
  author={Marcus, Gary F},
  year={2003},
  publisher={MIT press}
}

@book{steele1990common,
  title={Common LISP: the language},
  author={Steele, Guy},
  year={1990},
  publisher={Elsevier}
}

@article{newell1982knowledge,
  title={The knowledge level},
  author={Newell, Allen},
  journal={Artificial intelligence},
  volume={18},
  number={1},
  pages={87--127},
  year={1982},
  publisher={Elsevier}
}

@article{schlag2018learning,
  title={Learning to reason with third order tensor products},
  author={Schlag, Imanol and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}