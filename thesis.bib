%%%% this sample bibliography has been taken from Overleaf

@article{einstein,
  author =       "Albert Einstein",
  title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
                 [{On} the electrodynamics of moving bodies]",
  journal =      "Annalen der Physik",
  volume =       "322",
  number =       "10",
  pages =        "891--921",
  year =         "1905",
  DOI =          "http://dx.doi.org/10.1002/andp.19053221004",
  keywords =     "physics"
}

@inproceedings{soulos-etal-2020-discovering,
    title = "Discovering the Compositional Structure of Vector Representations with Role Learning Networks",
    author = "Soulos, Paul  and
      McCoy, R. Thomas  and
      Linzen, Tal  and
      Smolensky, Paul",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa{\l}a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.23/",
    doi = "10.18653/v1/2020.blackboxnlp-1.23",
    pages = "238--254",
    abstract = "How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model`s output is changed in the way predicted by our analysis."
}

@InProceedings{pmlr-v202-soulos23a,
  title = 	 {Differentiable Tree Operations Promote Compositional Generalization},
  author =       {Soulos, Paul and Hu, Edward J and Mccurdy, Kate and Chen, Yunmo and Fernandez, Roland and Smolensky, Paul and Gao, Jianfeng},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32499--32520},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/soulos23a/soulos23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/soulos23a.html},
  abstract = 	 {In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine (DTM) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, DTM achieves 100\% while existing baselines such as Transformer, Tree Transformer, LSTM, and Tree2Tree LSTM achieve less than 30\%. DTM remains highly interpretable in addition to its perfect performance.}
}


@book{dirac,
  title={The Principles of Quantum Mechanics},
  author={Paul Adrien Maurice Dirac},
  isbn={9780198520115},
  series={International series of monographs on physics},
  year={1981},
  publisher={Clarendon Press},
  keywords = {physics}
}

@book{latexcompanion,
    author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
    title     = "The \LaTeX\ Companion",
    year      = "1993",
    publisher = "Addison-Wesley",
    address   = "Reading, Massachusetts",
    keywords  = "latex"
}
 
@online{knuthwebsite,
    author    = "Donald Knuth",
    title     = "Knuth: Computers and Typesetting",
    url       = "http://www-cs-faculty.stanford.edu/~uno/abcde.html",
    addendum = "(accessed: 01.09.2016)",
    keywords  = "latex,knuth"
}

@inbook{knuth-fa,
   author = "Donald E. Knuth",
   title = "Fundamental Algorithms",
   publisher = "Addison-Wesley",
   year = "1973",
   chapter = "1.2",
   keywords  = "knuth,programming"
}

@book{knuth-acp,
   author = "Donald E. Knuth",
   publisher = "Addison-Wesley",
   title = "The Art of Computer Programming",
   series = "Four volumes",
   year = "1968",
   note = "Seven volumes planned",
   keywords  = "knuth,programming"
}

@article{ctan,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351",
    keywords  = "latex"
}



@book{pearl2000causality,
  title={Causality},
  author={Judea Pearl},
  publisher={MIT Press},
  address={Cambridge, MA},
  year={2000}
}


@article{fodor1990connectionism,
  title={Connectionism and the problem of systematicity: Why {Smolensky's} solution doesn't work},
  author={Fodor, Jerry and McLaughlin, Brian P},
  journal={Cognition},
  volume={35},
  number={2},
  pages={183--204},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{giulianelli2018hood,
    title = "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information",
    author = "Giulianelli, Mario  and
      Harding, Jack  and
      Mohnert, Florian  and
      Hupkes, Dieuwke  and
      Zuidema, Willem",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5426",
    doi = "10.18653/v1/W18-5426",
    pages = "240--248",
}

@article{fodor1997connectionism,
  title={Connectionism and the problem of systematicity (continued): Why {Smolensky's} solution still doesn't work},
  author={Fodor, Jerry},
  journal={Cognition},
  volume={62},
  number={1},
  pages={109--119},
  year={1997},
  publisher={Elsevier}
}

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@article{smolensky1987constituent,
  title={The constituent structure of connectionist mental states: A reply to {Fodor and Pylyshyn}},
  author={Smolensky, Paul},
  journal={Southern Journal of Philosophy},
  volume={26},
  number={Supplement},
  pages={137--161},
  year={1987}
}

@incollection{smolensky1991connectionism,
  title={Connectionism, constituency, and the language of thought},
  author={Smolensky, Paul},
  booktitle={Meaning in Mind: Fodor and his Critics},
  editor={Barry Loewer and Georges Rey},
  pages={201--227},
  publisher={Basil Blackwell},
  address={Oxford},
  year={1991}
}

@inproceedings{mccoy,
title={{RNN}s implicitly implement tensor-product representations},
author={R. Thomas McCoy and Tal Linzen and Ewan Dunbar and Paul Smolensky},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJx0sjC5FX},
}

@inproceedings{
andreas2019measuring,
title={Measuring Compositionality in Representation Learning},
author={Jacob Andreas},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJz05o0qK7},
}

@inproceedings{abnar2019blackbox,
    title = "Blackbox Meets Blackbox: Representational Similarity {\&} Stability Analysis of Neural Language Models and Brains",
    author = "Abnar, Samira  and
      Beinborn, Lisa  and
      Choenni, Rochelle  and
      Zuidema, Willem",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4820",
    doi = "10.18653/v1/W19-4820",
    pages = "191--203",
}

@inproceedings{chrupala2019correlating,
    title = "Correlating Neural and Symbolic Representations of Language",
    author = "Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1283",
    doi = "10.18653/v1/P19-1283",
    pages = "2952--2962",
}

@article{hupkes2018visualisation,
  title={Visualisation and `diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
  author={Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={907--926},
  year={2018}
}


@article{hupkes2020compositionality,
  title={Compositionality Decomposed: How do Neural Networks Generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3104--3112},
  year={2014},
  url={https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}


@article{syntaxattention,
  title={Compositional generalization in a deep seq2seq model by separating syntax and semantics},
  author={Russin, Jake and Jo, Jason and O'Reilly, Randall C and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1904.09708},
  year={2019},
  url={https://arxiv.org/abs/1904.09708}
}

@inproceedings{mikolov2016roadmap,
  title={A roadmap towards machine intelligence},
  author={Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
  booktitle={International Conference on Intelligent Text Processing and Computational Linguistics},
  pages={29--61},
  year={2016},
  organization={Springer}
}

@book{Smolensky:2006:HMN:1205244,
 author = {Smolensky, Paul and Legendre, G{\'e}raldine},
 title = {The Harmonic Mind: From Neural Computation to Optimality-Theoretic GrammarVolume I: Cognitive Architecture (Bradford Books)},
 year = {2006},
 isbn = {0262195267},
 publisher = {The MIT Press},
}


@article{structure,
author = {McClelland, James and Botvinick, Matthew and Noelle, David and Plaut, David and Rogers, Timothy and Seidenberg, Mark and Smith, Linda},
year = {2010},
month = {08},
pages = {348-56},
title = {Letting Structure Emerge: Connectionist and Dynamical Systems Approaches to Cognition},
volume = {14},
journal = {Trends in cognitive sciences},
doi = {10.1016/j.tics.2010.06.002}
}

@inproceedings{palangi,
  title={Question-Answering with Grammatically-Interpretable Representations},
  author={Hamid Palangi and Paul Smolensky and Xiaodong He and Li Deng},
  booktitle={Proceedings of the Association for the Advancement of Artificial Intelligence},
  url={https://arxiv.org/pdf/1705.08432.pdf},
  year={2017}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{googlenmt,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016},
  url={https://arxiv.org/abs/1609.08144}
}


@inproceedings{kiros2015skip,
  title={Skip-thought vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3294--3302},
  year={2015},
  url={https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf}
}

@InProceedings{conneau2017supervised,
  author = 	"Conneau, Alexis
		and Kiela, Douwe
		and Schwenk, Holger
		and Barrault, Lo{\"i}c
		and Bordes, Antoine",
  title = 	"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"670--680",
  location = 	"Copenhagen, Denmark",
  url = 	"http://aclweb.org/anthology/D17-1070"
}

@inproceedings{socher2013recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}


@inproceedings{bowman2015large,
	Address = {Lisbon, Portugal},
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	Booktitle = {{Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}},
	Date-Added = {2015-11-06 18:15:56 +0000},
	Date-Modified = {2015-11-06 18:16:03 +0000},
	Month = {September},
	Pages = {632--642},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Url = {http://aclweb.org/anthology/D15-1075},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QaVBhcGVycy9TZW1hbnRpY3MvQm93bWFuIGV0IGFsIDIwMTUgLSBBIGxhcmdlIGFubm90YXRlZCBjb3JwdXMgZm9yIGxlYXJuaW5nIG5hdHVyYWwgbGFuZ3VhZ2UgaW5mZXJlbmNlLnBkZtIXCxgZV05TLmRhdGFPEQKaAAAAAAKaAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADVVNe4SCsAAAAN1SkfQm93bWFuIGV0IGFsIDIwMTUgLSAjMTg0MEU2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABhA5tJiWEYAAAAAAAAAAAABAAQAAAkgAAAAAAAAAAAAAAAAAAAACVNlbWFudGljcwAAEAAIAADVVQ/4AAAAEQAIAADSYp6WAAAAAQAUAA3VKQANvDIADbqKAA2SfQAGadwAAgBZTWFjaW50b3NoIEhEOlVzZXJzOgB0bGluemVuMToARHJvcGJveDoAUGFwZXJzOgBTZW1hbnRpY3M6AEJvd21hbiBldCBhbCAyMDE1IC0gIzE4NDBFNi5wZGYAAA4AsgBYAEIAbwB3AG0AYQBuACAAZQB0ACAAYQBsACAAMgAwADEANQAgAC0AIABBACAAbABhAHIAZwBlACAAYQBuAG4AbwB0AGEAdABlAGQAIABjAG8AcgBwAHUAcwAgAGYAbwByACAAbABlAGEAcgBuAGkAbgBnACAAbgBhAHQAdQByAGEAbAAgAGwAYQBuAGcAdQBhAGcAZQAgAGkAbgBmAGUAcgBlAG4AYwBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgCAVXNlcnMvdGxpbnplbjEvRHJvcGJveC9QYXBlcnMvU2VtYW50aWNzL0Jvd21hbiBldCBhbCAyMDE1IC0gQSBsYXJnZSBhbm5vdGF0ZWQgY29ycHVzIGZvciBsZWFybmluZyBuYXR1cmFsIGxhbmd1YWdlIGluZmVyZW5jZS5wZGYAEwABLwAAFQACAA///wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgD6AP8BBwOlA6cDrAO3A8ADzgPSA9kD4gPnA/QD9wQJBAwEEQAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAQT}}
	
@inproceedings{mccoy2019right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, R. Thomas  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
}

@inproceedings{jawahar-etal-2019-bert,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1356",
    doi = "10.18653/v1/P19-1356",
    pages = "3651--3657",
    abstract = "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",
}


@article{linzen2016assessing,
  title={Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the ACL},
  year={2016},
  url={https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00115}
}

@inproceedings{poliak2018collecting,
    title = "Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation",
    author = "Poliak, Adam  and
      Haldar, Aparajita  and
      Rudinger, Rachel  and
      Hu, J. Edward  and
      Pavlick, Ellie  and
      White, Aaron Steven  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1007",
    doi = "10.18653/v1/D18-1007",
    pages = "67--81",
}


@article{dasgupta2019analyzing,
  title={Analyzing machine-learned representations: A natural language case study},
  author={Dasgupta, Ishita and Guo, Demi and Gershman, Samuel J and Goodman, Noah D},
  journal={arXiv preprint arXiv:1909.05885},
  year={2019},
  url={https://arxiv.org/abs/1909.05885}
}

@inproceedings{kingma2015adam,
	Author = {Diederik Kingma and Jimmy Ba},
	Booktitle = {International Conference for Learning Representations},
	Title = {Adam: A Method for Stochastic Optimization},
	url={https://arxiv.org/pdf/1412.6980.pdf},
	Year = {2015}}


@inproceedings{wang2018glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
}


@InProceedings{manning-EtAl:2014:P14-5,
  author    = {Manning, Christopher D. and  Surdeanu, Mihai  and  Bauer, John  and  Finkel, Jenny  and  Bethard, Steven J. and  McClosky, David},
  title     = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
  booktitle = {Association for Computational Linguistics (ACL) System Demonstrations},
  year      = {2014},
  pages     = {55--60},
  url       = {http://www.aclweb.org/anthology/P/P14/P14-5010}
}

@incollection{mu2020compositional,
  title={Compositional Explanations of Neurons},
  author={Mu, Jesse and Andreas, Jacob},
  booktitle={Advances in Neural Information Processing Systems 33},
  year={2020},
  url={https://arxiv.org/pdf/2006.14032.pdf}
}

@inproceedings{
tenney2018what,
title={What do you learn from context? Probing for sentence structure in contextualized word representations},
author={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R. Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJzSgnRcKX},
}


@inproceedings{peters2018dissecting,
    title = "Dissecting Contextual Word Embeddings: Architecture and Representation",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Zettlemoyer, Luke  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1179",
    doi = "10.18653/v1/D18-1179",
    pages = "1499--1509",
}



@inproceedings{blevins2018hierarchical,
    title = "Deep {RNN}s Encode Soft Hierarchical Syntax",
    author = "Blevins, Terra  and
      Levy, Omer  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2003",
    doi = "10.18653/v1/P18-2003",
    pages = "14--19",
}


@inproceedings{belinkov2017evaluating,
    title = "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    author = "Belinkov, Yonatan  and
      M{\`a}rquez, Llu{\'\i}s  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Glass, James",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/I17-1001",
    pages = "1--10",
}


@inproceedings{conneau2018senteval,
    title = "{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations",
    author = "Conneau, Alexis  and
      Kiela, Douwe",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1269",
}

@inproceedings{conneau2018cram,
    title = "What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties",
    author = {Conneau, Alexis  and
      Kruszewski, German  and
      Lample, Guillaume  and
      Barrault, Lo{\"\i}c  and
      Baroni, Marco},
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1198",
    doi = "10.18653/v1/P18-1198",
    pages = "2126--2136",
}

@inproceedings{adi2016fine,
  title={Fine-grained analysis of sentence embeddings using auxiliary prediction tasks},
  author={Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
  booktitle = {International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/pdf?id=BJh6Ztuxl}
}

@article{wickelgren1969context,
  title={Context-sensitive coding, associative memory, and serial order in (speech) behavior.},
  author={Wickelgren, Wayne A.},
  journal={Psychological Review},
  volume={76},
  number={1},
  pages={1--15},
  year={1969},
  publisher={American Psychological Association}
}

@article{vanmassenhove2017investigating,
  title={Investigating `aspect' in {NMT} and {SMT}: Translating the {English} simple past and present perfect},
  author={Vanmassenhove, Eva and Du, Jinhua and Way, Andy},
  journal={Computational Linguistics in the Netherlands Journal},
  volume={7},
  pages={109--128},
  year={2017}
}

@inproceedings{bowman2016fast,
	Author = {Bowman, Samuel R. and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D. and Potts, Christopher},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Pages = {1466--1477},
	Publisher = {Association for Computational Linguistics},
	Title = {A Fast Unified Model for Parsing and Sentence Understanding},
	Url = {http://aclweb.org/anthology/P16-1139},
	Year = {2016}}

@inproceedings{vMeasure,
  added-at = {2011-09-18T23:01:04.000+0200},
  author = {Rosenberg, Andrew and Hirschberg, Julia},
  biburl = {https://www.bibsonomy.org/bibtex/238531ca6d31f767da8aef74bd6dadcf7/jil},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP-CoNLL)},
  description = {ESOM},
  interhash = {07bb9cbf44b156e34d57c40b9d2fd314},
  intrahash = {38531ca6d31f767da8aef74bd6dadcf7},
  keywords = {cluster clustering measure v-measure validation},
  pages = {410--420},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {{V}-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure},
  year = 2007
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019},
  url={https://www.aclweb.org/anthology/N19-1419/}
}

@article{Hochreiter:1997:LSM:1246443.1246450,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Computation},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@inproceedings{softattention,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  crossref  = {DBLP:conf/iclr/2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Smolensky:1990:TPV:102418.102425,
 author = {Smolensky, Paul},
 title = {Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
 journal = {Artif. Intell.},
 issue_date = {Nov. 1990},
 volume = {46},
 number = {1-2},
 month = nov,
 year = {1990},
 issn = {0004-3702},
 pages = {159--216},
 numpages = {58},
 url = {http://dx.doi.org/10.1016/0004-3702(90)90007-M},
 doi = {10.1016/0004-3702(90)90007-M},
 acmid = {102425},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
} 

@article{omlin1996extraction,
  title={Extraction of rules from discrete-time recurrent neural networks},
  author={Omlin, Christian W and Giles, C Lee},
  journal={Neural networks},
  volume={9},
  number={1},
  pages={41--52},
  year={1996},
  publisher={Elsevier}
}

@inproceedings{weiss2018extracting,
  author    = {Gail Weiss and
               Yoav Goldberg and
               Eran Yahav},
  title     = {Extracting Automata from Recurrent Neural Networks Using Queries and
               Counterexamples},
  booktitle = {{International Conference on Machine Learning}},
  pages     = {5244--5253},
  year      = {2018},
  url={http://proceedings.mlr.press/v80/weiss18a.html}
}

@article{belinkov2019analysis,
  title={Analysis methods in neural language processing: A survey},
  author={Belinkov, Yonatan and Glass, James},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={49--72},
  year={2019},
  publisher={MIT Press},
  url={https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00254}
}

@inproceedings{mikolov2013linguistic,
    title = "Linguistic Regularities in Continuous Space Word Representations",
    author = "Mikolov, Tomas  and
      Yih, Wen-tau  and
      Zweig, Geoffrey",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1090",
    pages = "746--751",
}

@inproceedings{parikh2016decomposable,
    title = "A Decomposable Attention Model for Natural Language Inference",
    author = {Parikh, Ankur  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Das, Dipanjan  and
      Uszkoreit, Jakob},
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1244",
    doi = "10.18653/v1/D16-1244",
    pages = "2249--2255",
}

@inproceedings{klein2003accurate,
  title={Accurate unlexicalized parsing},
  author={Klein, Dan and Manning, Christopher D},
  booktitle={Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1},
  pages={423--430},
  year={2003},
  url={https://www.aclweb.org/anthology/P03-1054.pdf},
  organization={Association for Computational Linguistics}
}


@inproceedings{lakretz2019emergence,
    title = "The emergence of number and syntax units in {LSTM} language models",
    author = "Lakretz, Yair  and
      Kruszewski, German  and
      Desbordes, Theo  and
      Hupkes, Dieuwke  and
      Dehaene, Stanislas  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1002",
    doi = "10.18653/v1/N19-1002",
    pages = "11--20",
}


@inproceedings{
shen2018ordered,
title={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},
author={Yikang Shen and Shawn Tan and Alessandro Sordoni and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1l6qiR5F7},
}

@article{Voita2020InformationTheoreticPW,
  title={Information-Theoretic Probing with Minimum Description Length},
  author={Elena Voita and Ivan Titov},
  journal={arXiv preprint arXiv:2003.12298},
  year={2020},
  url={https://arxiv.org/abs/2003.12298}
}

@inproceedings{li-etal-2019-compositional,
    title = "Compositional Generalization for Primitive Substitutions",
    author = "Li, Yuanpeng  and
      Zhao, Liang  and
      Wang, Jianyu  and
      Hestness, Joel",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1438",
    doi = "10.18653/v1/D19-1438",
    pages = "4293--4302",
    abstract = "Compositional generalization is a basic mechanism in human language learning, but current neural networks lack such ability. In this paper, we conduct fundamental research for encoding compositionality in neural networks. Conventional methods use a single representation for the input sentence, making it hard to apply prior knowledge of compositionality. In contrast, our approach leverages such knowledge with two representations, one generating attention maps, and the other mapping attended input words to output symbols. We reduce the entropy in each representation to improve generalization. Our experiments demonstrate significant improvements over the conventional methods in five NLP tasks including instruction learning and machine translation. In the SCAN domain, it boosts accuracies from 14.0{\%} to 98.8{\%} in Jump task, and from 92.0{\%} to 99.7{\%} in TurnLeft task. It also beats human performance on a few-shot learning task. We hope the proposed approach can help ease future research towards human-level compositional language learning.",
}

@article{warstadt2019blimp,
  title={{BLiMP}: A Benchmark of Linguistic Minimal Pairs for English},
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  journal={Proceedings of the
Society for Computation in Linguistics.},
  year={2020},
  url={https://scholarworks.umass.edu/scil/vol3/iss1/43/}
}


@inproceedings{hu2020systematic,
    title = "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
    author = "Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url="https://www.aclweb.org/anthology/2020.acl-main.158.pdf",
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020},
  url={https://arxiv.org/abs/2005.14165}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017},
  url={https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@inproceedings{goodwin2020probing,
    title = "Probing Linguistic Systematicity",
    author = "Goodwin, Emily  and
      Sinha, Koustuv  and
      O{'}Donnell, Timothy J.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.177",
    doi = "10.18653/v1/2020.acl-main.177",
    pages = "1958--1969",
    abstract = "Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.",
}

@article{ravichander2020probing,
  title={Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?},
  author={Ravichander, Abhilasha and Belinkov, Yonatan and Hovy, Eduard},
  journal={arXiv preprint arXiv:2005.00719},
  year={2020}
}

@misc{najoung,
  doi = {10.48550/ARXIV.2212.10769},
  url = {https://arxiv.org/abs/2212.10769},
  author = {Kim, Najoung and Linzen, Tal and Smolensky, Paul},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@inproceedings{csordas2021devil,
  title={The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, Juergen},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={619--634},
  year={2021}
}


@article{Smolensky1990TensorPV,
  title={Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
  author={Paul Smolensky},
  journal={Artif. Intell.},
  year={1990},
  volume={46},
  pages={159-216}
}

@article{Graves2014NeuralTM,
  title={Neural Turing Machines},
  author={Alex Graves and Greg Wayne and Ivo Danihelka},
  journal={ArXiv},
  year={2014},
  volume={abs/1410.5401}
}

@article{Newell1980PhysicalSS,
  title={Physical Symbol Systems},
  author={Allen Newell},
  journal={Cogn. Sci.},
  year={1980},
  volume={4},
  pages={135-183}
}

@article{Weston2014MemoryN,
  title={Memory Networks},
  author={Jason Weston and Sumit Chopra and Antoine Bordes},
  journal={CoRR},
  year={2014},
  volume={abs/1410.3916}
}

@article{Graves2016HybridCU,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Alex Graves and Greg Wayne and Malcolm Reynolds and Tim Harley and Ivo Danihelka and Agnieszka Grabska-Barwinska and Sergio Gomez Colmenarejo and Edward Grefenstette and Tiago Ramalho and John P. Agapiou and Adri{\`a} Puigdom{\`e}nech Badia and Karl Moritz Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and Christopher Summerfield and Phil Blunsom and Koray Kavukcuoglu and Demis Hassabis},
  journal={Nature},
  year={2016},
  volume={538},
  pages={471-476}
}

@inproceedings{palangi2018question,
  title={Question-answering with grammatically-interpretable representations},
  author={Palangi, Hamid and Smolensky, Paul and He, Xiaodong and Deng, Li},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{jiang2021enriching,
  title={Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization},
  author={Jiang, Yichen and Celikyilmaz, Asli and Smolensky, Paul and Soulos, Paul and Rao, Sudha and Palangi, Hamid and Fernandez, Roland and Smith, Caitlin and Bansal, Mohit and Gao, Jianfeng},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4780--4793},
  year={2021}
}

@article{soulos2021structural,
  title={Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages},
  author={Soulos, Paul and Rao, Sudha and Smith, Caitlin and Rosen, Eric and Celikyilmaz, Asli and McCoy, R Thomas and Jiang, Yichen and Haley, Coleman and Fernandez, Roland and Palangi, Hamid and others},
  journal={Proceedings of Machine Translation Summit XVIII},
  year={2021}
}

@article{Reed2015NeuralP,
  title={Neural Programmer-Interpreters},
  author={Scott E. Reed and Nando de Freitas},
  journal={CoRR},
  year={2015},
  volume={abs/1511.06279}
}

@inproceedings{tai-etal-2015-improved,
    title = "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
    author = "Tai, Kai Sheng  and
      Socher, Richard  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1150",
    doi = "10.3115/v1/P15-1150",
    pages = "1556--1566",
}

@inproceedings{wang-etal-2019-tree,
    title = "Tree Transformer: Integrating Tree Structures into Self-Attention",
    author = "Wang, Yaushian  and
      Lee, Hung-Yi  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1098",
    doi = "10.18653/v1/D19-1098",
    pages = "1061--1070",
    abstract = "Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed {``}Constituent Attention{''} module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.",
}


@InProceedings{bosnjak17a,
  title = 	 {Programming with a Differentiable Forth Interpreter},
  author =       {Matko Bo{\v{s}}njak and Tim Rockt{\"a}schel and Jason Naradowsky and Sebastian Riedel},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {547--556},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/bosnjak17a/bosnjak17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/bosnjak17a.html},
  abstract = 	 {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.}
}

@inproceedings{cho-etal-2014-properties,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@inproceedings{sutskever,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}



@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}

@article{shiv2019novel,
  title={Novel positional encodings to enable tree-based transformers},
  author={Shiv, Vighnesh and Quirk, Chris},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kleyko2022,
author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
title = {A Survey on Hyperdimensional Computing Aka Vector Symbolic Architectures, Part I: Models and Data Transformations},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3538531},
doi = {10.1145/3538531},
abstract = {This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and distributed vector representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent years, the necessity for a comprehensive survey of the field has become extremely important. Therefore, amongst other aspects of the field, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey [84] is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {130},
numpages = {40},
keywords = {tensor product representations, holographic reduced representations, machine learning, Artificial intelligence, modular composite representations, sparse binary distributed representations, sparse block codes, multiply-add-permute, geometric analogue of holographic reduced representations, data structures, hyperdimensional computing, binary spatter codes, distributed representations, matrix binding of additive terms, vector symbolic architectures}
}


@inproceedings{gayler2003vsa_jackendoff,
  title={Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience},
  author = {Ross W Gayler},
  booktitle = {Proceedings of the ICCS/ASCS Joint International Conference on Cognitive Science (ICCS/ASCS 2003)},
  pages={133--138},
  month = {07},
  year={2003},
  date={2003-07-17},
  publisher = {University of New South Wales},
  address = {Sydney, NSW, AU},
  editor = {Slezak, Peter},
  archivePrefix = {arXiv},
  arxivId = {cs/0412059v1},
  eprint = {0412059v1},
  primaryClass = {cs},
  url = {http://arxiv.org/abs/cs/0412059}
}

@article{kanerva2009hyperdimensional,
  title={Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors},
  author={Kanerva, Pentti},
  journal={Cognitive computation},
  volume={1},
  pages={139--159},
  year={2009},
  publisher={Springer}
}


@article{sartran2022transformer,
  title={Transformer Grammars: Augmenting Transformer language models with syntactic inductive biases at scale},
  author={Sartran, Laurent and Barrett, Samuel and Kuncoro, Adhiguna and Stanojevi{\'c}, Milo{\v{s}} and Blunsom, Phil and Dyer, Chris},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={1423--1439},
  year={2022},
  publisher={MIT Press}
}

@inproceedings{
maddison2017the,
title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1jE5L5gl}
}

@inproceedings{
jang2017categorical,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}
@inproceedings{dong2016language,
  title={Language to Logical Form with Neural Attention},
  author={Dong, Li and Lapata, Mirella},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={33--43},
  year={2016}
}

@InProceedings{chen,
  title = 	 {Mapping natural-language problems to formal-language solutions using structured neural representations},
  author =       {Chen, Kezhen and Huang, Qiuyuan and Palangi, Hamid and Smolensky, Paul and Forbus, Ken and Gao, Jianfeng},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1566--1575},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20g/chen20g.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20g.html},
  abstract = 	 {Generating formal-language programs represented by relational tuples, such as Lisp programs or mathematical operations, to solve problems stated in natural language is a challenging task because it requires explicitly capturing discrete symbolic structural information implicit in the input. However, most general neural sequence models do not explicitly capture such structural information, limiting their performance on these tasks. In this paper, we propose a new encoder-decoder model based on a structured neural representation, Tensor Product Representations (TPRs), for mapping Natural-language problems to Formal-language solutions, called TP-N2F. The encoder of TP-N2F employs TPR ‘binding’ to encode natural-language symbolic structure in vector space and the decoder uses TPR ‘unbinding’ to generate, in symbolic space, a sequential program represented by relational tuples, each consisting of a relation (or operation) and a number of arguments. TP-N2F considerably outperforms LSTM-based seq2seq models on two benchmarks and creates new state-of-the-art results. Ablation studies show that improvements can be attributed to the use of structured TPRs explicitly in both the encoder and decoder. Analysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F.}
}

@article{imanol,
  author    = {Imanol Schlag and
               Paul Smolensky and
               Roland Fernandez and
               Nebojsa Jojic and
               J{\"{u}}rgen Schmidhuber and
               Jianfeng Gao},
  title     = {Enhancing the Transformer with Explicit Relational Encoding for Math
               Problem Solving},
  journal   = {CoRR},
  volume    = {abs/1910.06611},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.06611},
  eprinttype = {arXiv},
  eprint    = {1910.06611},
  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-06611.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{soulos2020discovering,
  title={Discovering the Compositional Structure of Vector Representations with Role Learning Networks},
  author={Soulos, Paul and McCoy, R Thomas and Linzen, Tal and Smolensky, Paul},
  booktitle={Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  pages={238--254},
  year={2020}
}

@inproceedings{
mccoy2018rnns,
title={{RNN}s implicitly implement tensor-product representations},
author={R. Thomas McCoy and Tal Linzen and Ewan Dunbar and Paul Smolensky},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJx0sjC5FX},
}



@article{nram,
title	= {Neural Random Access Machines},
author	= {Karol Kurach and Marcin Andrychowicz and Ilya Sutskever},
year	= {2016},
URL	= {http://arxiv.org/abs/1511.06392},
journal	= {ICLR}
}

@article{Andreas2015NeuralMN,
  title={Neural Module Networks},
  author={Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Dan Klein},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={39-48}
}

@inproceedings{Joulin2015InferringAP,
  title={Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
  author={Armand Joulin and Tomas Mikolov},
  booktitle={NIPS},
  year={2015}
}

@article{Grefenstette2015LearningTT,
  title={Learning to Transduce with Unbounded Memory},
  author={Edward Grefenstette and Karl Moritz Hermann and Mustafa Suleyman and Phil Blunsom},
  journal={ArXiv},
  year={2015},
  volume={abs/1506.02516}
}

@inproceedings{kim-linzen-2020-cogs,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
}

@book{plate,
author = {Plate, Tony A.},
title = {Holographic Reduced Representation: Distributed Representation for Cognitive Structures},
year = {2003},
isbn = {1575864290},
publisher = {CSLI Publications},
address = {USA}
}


@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{
eliasmith,
author = {Chris Eliasmith  and Terrence C. Stewart  and Xuan Choo  and Trevor Bekolay  and Travis DeWolf  and Yichuan Tang  and Daniel Rasmussen },
title = {A Large-Scale Model of the Functioning Brain},
journal = {Science},
volume = {338},
number = {6111},
pages = {1202-1205},
year = {2012},
doi = {10.1126/science.1225266},
URL = {https://www.science.org/doi/abs/10.1126/science.1225266},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1225266},
abstract = {Neurons are pretty complicated cells. They display an endless variety of shapes that sprout highly variable numbers of axons and dendrites; they sport time- and voltage-dependent ion channels along with an impressive array of neurotransmitter receptors; and they connect intimately with near neighbors as well as former neighbors who have since moved away. Simulating a sizeable chunk of brain tissue has recently become achievable, thanks to advances in computer hardware and software. Eliasmith et al. (p. 1202; see the Perspective by Machens) present their million-neuron model of the brain and show that it can recognize numerals, remember lists of digits, and write down those lists—tasks that seem effortless for a human but that encompass the triad of perception, cognition, and behavior. Two-and-a-half million model neurons recognize images, learn via reinforcement, and display fluid intelligence. A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called “Spaun”) that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.}}


@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@inproceedings{patel-etal-2022-revisiting,
    title = "Revisiting the Compositional Generalization Abilities of Neural Sequence Models",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Blunsom, Phil  and
      Goyal, Navin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.46",
    doi = "10.18653/v1/2022.acl-short.46",
    pages = "424--434",
}

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@book{marcus2003algebraic,
  title={The algebraic mind: Integrating connectionism and cognitive science},
  author={Marcus, Gary F},
  year={2003},
  publisher={MIT press}
}

@book{steele1990common,
  title={Common LISP: the language},
  author={Steele, Guy},
  year={1990},
  publisher={Elsevier}
}

@article{newell1982knowledge,
  title={The knowledge level},
  author={Newell, Allen},
  journal={Artificial intelligence},
  volume={18},
  number={1},
  pages={87--127},
  year={1982},
  publisher={Elsevier}
}

@article{schlag2018learning,
  title={Learning to reason with third order tensor products},
  author={Schlag, Imanol and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{
soulos2024compositional,
title={Compositional Generalization Across Distributional Shifts with Sparse Tree Operations},
author={Paul Soulos and Henry Conklin and Mattia Opper and Paul Smolensky and Jianfeng Gao and Roland Fernandez},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=fOQunr2E0T}
}

@article{Smolensky1990TensorPV,
  title={Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
  author={Paul Smolensky},
  journal={Artif. Intell.},
  year={1990},
  volume={46},
  pages={159-216}
}

@misc{li_slog_2023,
	title = {{SLOG}: {A} {Structural} {Generalization} {Benchmark} for {Semantic} {Parsing}},
	shorttitle = {{SLOG}},
	url = {http://arxiv.org/abs/2310.15040},
	abstract = {The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well models can generalize. We introduce SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with 17 structural generalization cases. In our experiments, the generalization accuracy of Transformer models, including pretrained ones, only reaches 40.6\%, while a structure-aware parser only achieves 70.8\%. These results are far from the near-perfect accuracy existing models achieve on COGS, demonstrating the role of SLOG in foregrounding the large discrepancy between models' lexical and structural generalization capacities.},
	urldate = {2024-01-28},
	publisher = {arXiv},
	author = {Li, Bingzhi and Donatelli, Lucia and Koller, Alexander and Linzen, Tal and Yao, Yuekun and Kim, Najoung},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15040 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{Winters_Marra_Manhaeve_Raedt_2022, title={DeepStochLog: Neural Stochastic Logic Programming}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/21248}, DOI={10.1609/aaai.v36i9.21248}, abstractNote={Recent advances in neural-symbolic learning, such as DeepProbLog, extend probabilistic logic programs with neural predicates. Like graphical models, these probabilistic logic programs define a probability distribution over possible worlds, for which inference is computationally hard. We propose DeepStochLog, an alternative neural-symbolic framework based on stochastic definite clause grammars, a kind of stochastic logic program. More specifically, we introduce neural grammar rules into stochastic definite clause grammars to create a framework that can be trained end-to-end. We show that inference and learning in neural stochastic logic programming scale much better than for neural probabilistic logic programs. Furthermore, the experimental evaluation shows that DeepStochLog achieves state-of-the-art results on challenging neural-symbolic learning tasks.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Winters, Thomas and Marra, Giuseppe and Manhaeve, Robin and Raedt, Luc De}, year={2022}, month={Jun.}, pages={10090-10100} }

@inproceedings{rocktaschel-riedel-2016-learning,
    title = "Learning Knowledge Base Inference with Neural Theorem Provers",
    author = {Rockt{\"a}schel, Tim  and
      Riedel, Sebastian},
    editor = "Pujara, Jay  and
      Rocktaschel, Tim  and
      Chen, Danqi  and
      Singh, Sameer",
    booktitle = "Proceedings of the 5th Workshop on Automated Knowledge Base Construction",
    month = jun,
    year = "2016",
    address = "San Diego, CA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-1309",
    doi = "10.18653/v1/W16-1309",
    pages = "45--50",
}

@article{Shindo_Nishino_Yamamoto_2021, title={Differentiable Inductive Logic Programming for Structured Examples}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16637}, DOI={10.1609/aaai.v35i6.16637}, abstractNote={The differentiable implementation of logic yields a seamless combination of symbolic reasoning and deep neural networks. Recent research, which has developed a differentiable framework to learn logic programs from examples, can even acquire reasonable solutions from noisy datasets. However, this framework severely limits expressions for solutions, e.g., no function symbols are allowed, and the shapes of clauses are fixed. As a result, the framework cannot deal with structured examples. Therefore we propose a new framework to learn logic programs from noisy and structured examples, including the following contributions. First, we propose an adaptive clause search method by looking through structured space, which is defined by the generality of the clauses, to yield an efficient search space for differentiable solvers. Second, we propose for ground atoms an enumeration algorithm, which determines a necessary and sufficient set of ground atoms to perform differentiable inference functions. Finally, we propose a new method to compose logic programs softly, enabling the system to deal with complex programs consisting of several clauses. Our experiments show that our new framework can learn logic programs from noisy and structured examples, such as sequences or trees. Our framework can be scaled to deal with complex programs that consist of several clauses with function symbols.}, number={6}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shindo, Hikaru and Nishino, Masaaki and Yamamoto, Akihiro}, year={2021}, month={May}, pages={5034-5041} }

@article{Muggleton_1991_InductiveLogicProgramming,
	title = {Inductive logic programming},
	volume = {8},
	issn = {1882-7055},
	url = {https://doi.org/10.1007/BF03037089},
	doi = {10.1007/BF03037089},
	abstract = {A new research area, Inductive Logic Programming, is presently emerging. While inheriting various positive characteristics of the parent subjects of Logic Programming and Machine Learning, it is hoped that the new area will overcome many of the limitations of its forebears. The background to present developments within this area is discussed and various goals and aspirations for the increasing body of researchers are identified. Inductive Logic Programming needs to be based on sound principles from both Logic and Statistics. On the side of statistical justification of hypotheses we discuss the possible relationship between Algorithmic Complexity theory and Probably-Approximately-Correct (PAC) Learning. In terms of logic we provide a unifying framework for Muggleton and Buntine’s Inverse Resolution (IR) and Plotkin’s Relative Least General Generalisation (RLGG) by rederiving RLGG in terms of IR. This leads to a discussion of the feasibility of extending the RLGG framework to allow for the invention of new predicates, previously discussed only within the context of IR.},
	number = {4},
	journal = {New Generation Computing},
	author = {Muggleton, Stephen},
	month = feb,
	year = {1991},
	pages = {295--318},
}


@inproceedings{NEURIPS2023_bf215fa7,
 author = {Maene, Jaron and De Raedt, Luc},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {60804--60820},
 publisher = {Curran Associates, Inc.},
 title = {Soft-Unification in Deep Probabilistic Logic},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/bf215fa7fe70a38c5e967e59c44a99d0-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@InProceedings{pmlr-v216-de-smet23a,
  title = 	 {Neural probabilistic logic programming in discrete-continuous domains},
  author =       {De Smet, Lennert and Zuidberg Dos Martires, Pedro and Manhaeve, Robin and Marra, Giuseppe and Kimmig, Angelika and De Readt, Luc},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {529--538},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/de-smet23a/de-smet23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/de-smet23a.html},
  abstract = 	 {Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic background knowledge in the form of logic. It has been shown to aid learning in the limited data regime and to facilitate inference on out-of-distribution data. Probabilistic NeSy focuses on integrating neural networks with both logic and probability theory, which additionally allows learning under uncertainty. A major limitation of current probabilistic NeSy systems, such as DeepProbLog, is their restriction to finite probability distributions, i.e., discrete random variables. In contrast, deep probabilistic programming (DPP) excels in modelling and optimising continuous probability distributions. Hence, we introduce DeepSeaProbLog, a neural probabilistic logic programming language that incorporates DPP techniques into NeSy. Doing so results in the support of inference and learning of both discrete and continuous probability distributions under logical constraints. Our main contributions are 1) the semantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a proven asymptotically unbiased learning algorithm, and 3) a series of experiments that illustrate the versatility of our approach.}
}


@inproceedings{ijcai2020p243,
  title     = {NeurASP: Embracing Neural Networks into Answer Set Programming},
  author    = {Yang, Zhun and Ishay, Adam and Lee, Joohyung},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {1755--1762},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/243},
  url       = {https://doi.org/10.24963/ijcai.2020/243},
}


@article{BADREDDINE2022103649,
title = {Logic Tensor Networks},
journal = {Artificial Intelligence},
volume = {303},
pages = {103649},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103649},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221002009},
author = {Samy Badreddine and Artur {d'Avila Garcez} and Luciano Serafini and Michael Spranger},
keywords = {Neurosymbolic AI, Deep learning and reasoning, Many-valued logics},
abstract = {Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2020-06-19},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@article{partee1995lexical,
  title={Lexical semantics and compositionality},
  author={Partee, Barbara and others},
  journal={An invitation to cognitive science: Language},
  volume={1},
  pages={311--360},
  year={1995},
  publisher={MIT Press Cambridge, MA:}
}
@article{steedman1987combinatory,
  title={Combinatory grammars and parasitic gaps},
  author={Steedman, Mark},
  journal={Natural Language \& Linguistic Theory},
  volume={5},
  number={3},
  pages={403--439},
  year={1987},
  publisher={Springer}
}
@inproceedings{klein2002generative,
  title={A generative constituent-context model for improved grammar induction},
  author={Klein, Dan and Manning, Christopher D},
  booktitle={Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  pages={128--135},
  year={2002}
}
@article{furrer2020compositional,
  title={Compositional generalization in semantic parsing: Pre-training vs. specialized architectures},
  author={Furrer, Daniel and van Zee, Marc and Scales, Nathan and Sch{\"a}rli, Nathanael},
  journal={arXiv preprint arXiv:2007.08970},
  year={2020}
}
@article{kim2019compound,
  title={Compound probabilistic context-free grammars for grammar induction},
  author={Kim, Yoon and Dyer, Chris and Rush, Alexander M},
  journal={arXiv preprint arXiv:1906.10225},
  year={2019}
}

@article{devlin_compositional_2019,
	title = {Compositional {Generalization} in {Semantic} {Parsing}: {Pre}-training vs. {Specialized} {Architectures}},
	doi = {10.18653/v1/N19-1423},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186},
}
@article{mccarthy1960recursive,
  title={Recursive functions of symbolic expressions and their computation by machine, part I},
  author={McCarthy, John},
  journal={Communications of the ACM},
  volume={3},
  number={4},
  pages={184--195},
  year={1960},
  publisher={ACM New York, NY, USA}
}

@inproceedings{andreas_good-enough_2020,
	address = {Online},
	title = {Good-{Enough} {Compositional} {Data} {Augmentation}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.676},
	doi = {10.18653/v1/2020.acl-main.676},
	abstract = {We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87\% on diagnostic tasks from the SCAN dataset and 16\% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1\% on small corpora in several languages.},
	language = {en},
	urldate = {2021-01-30},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Andreas, Jacob},
	year = {2020},
	keywords = {\_tablet},
	pages = {7556--7566},
}

@inproceedings{sakai1961syntax,
  title={Syntax in universal translation},
  author={Sakai, Itiroo},
  booktitle={Proceedings of the International Conference on Machine Translation and Applied Language Analysis},
  year={1961}
}

@article{lopez2008statistical,
  title={Statistical machine translation},
  author={Lopez, Adam},
  journal={ACM Computing Surveys (CSUR)},
  volume={40},
  number={3},
  pages={1--49},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@article{guo_sequence-level_2020,
	title = {Sequence-{Level} {Mixed} {Sample} {Data} {Augmentation}},
	url = {http://arxiv.org/abs/2011.09039},
	abstract = {Despite their empirical success, neural networks still have difﬁculty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut (Wang et al., 2018) and word dropout (Sennrich et al., 2016), and show that these techniques are all approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on ﬁve different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.},
	language = {en},
	urldate = {2021-03-04},
	journal = {arXiv:2011.09039 [cs]},
	author = {Guo, Demi and Kim, Yoon and Rush, Alexander M.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.09039},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{
dusell2024stack,
title={Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns},
author={Brian DuSell and David Chiang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=XVhm3X8Fum}
}

@inproceedings{zelle1996learning,
  title={Learning to parse database queries using inductive logic programming},
  author={Zelle, John M and Mooney, Raymond J},
  booktitle={Proceedings of the national conference on artificial intelligence},
  pages={1050--1055},
  year={1996}
}

@article{kim2022uncontrolled,
  title={Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models},
  author={Kim, Najoung and Linzen, Tal and Smolensky, Paul},
  journal={arXiv preprint arXiv:2212.10769},
  year={2022}
}
@book{chomsky_aspects_1965,
	address = {Cambridge, Massachusetts},
	edition = {50th Anniversary Edition},
	series = {Massachusetts {Institute} of {Technology}. {Research} {Laboratory} of {Electronics}. {Special} technical report},
	title = {Aspects of the theory of syntax},
	isbn = {978-0-262-52740-8},
	number = {no. 11},
	publisher = {The MIT Press},
	author = {Chomsky, Noam},
	year = {1965},
	keywords = {Grammar, Comparative and general, Syntax},
}

@book{goldberg_constructions_2006,
	address = {Oxford; New York},
	title = {Constructions at work: the nature of generalization in language},
	shorttitle = {Constructions at work},
	url = {http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3052348},
	abstract = {Includes selected classic and contemporary papers in four areas, this text introduces each field, providing technical background for the non-specialist and explaining the underlying connections across the disciplines.},
	language = {English.},
	urldate = {2020-06-23},
	publisher = {Oxford University Press},
	author = {Goldberg, Adele E},
	year = {2006},
	note = {OCLC: 193697889},
}

@article{Griffiths2020UnderstandingHI,
  title={Understanding Human Intelligence through Human Limitations},
  author={Thomas L. Griffiths},
  journal={Trends in Cognitive Sciences},
  year={2020},
  volume={24},
  pages={873-883},
  url={https://api.semanticscholar.org/CorpusID:221996148}
}

@article{belinkov2019analysis,
  title={Analysis methods in neural language processing: A survey},
  author={Belinkov, Yonatan and Glass, James},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={49--72},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{blevins2018deep,
  title={Deep RNNs encode soft hierarchical syntax},
  author={Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1805.04218},
  year={2018}
}

@article{murty2022characterizing,
  title={Characterizing intrinsic compositionality in transformers with tree projections},
  author={Murty, Shikhar and Sharma, Pratyusha and Andreas, Jacob and Manning, Christopher D},
  journal={arXiv preprint arXiv:2211.01288},
  year={2022}
}

@article{soulos2019discovering,
  title={Discovering the compositional structure of vector representations with role learning networks},
  author={Soulos, Paul and McCoy, Tom and Linzen, Tal and Smolensky, Paul},
  journal={arXiv preprint arXiv:1910.09113},
  year={2019}
}
@article{mccoy2018rnns,
  title={RNNs implicitly implement tensor product representations},
  author={McCoy, R Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
  journal={arXiv preprint arXiv:1812.08718},
  year={2018}
}

@incollection{cuetos2013parsing,
  title={Parsing in different languages},
  author={Cuetos, Fernando and Mitchell, Don C and Corley, Martin MB},
  booktitle={Language processing in Spanish},
  pages={163--208},
  year={2013},
  publisher={Psychology Press}
}

@article{tai2015improved,
  title={Improved semantic representations from tree-structured long short-term memory networks},
  author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
  journal={arXiv preprint arXiv:1503.00075},
  year={2015}
}

@article{dong2016language,
  title={Language to logical form with neural attention},
  author={Dong, Li and Lapata, Mirella},
  journal={arXiv preprint arXiv:1601.01280},
  year={2016}
}

@article{wang2019tree,
  title={Tree transformer: Integrating tree structures into self-attention},
  author={Wang, Yau-Shian and Lee, Hung-Yi and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:1909.06639},
  year={2019}
}

@article{shiv2019novel,
  title={Novel positional encodings to enable tree-based transformers},
  author={Shiv, Vighnesh and Quirk, Chris},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{joulin2015inferring,
  title={Inferring algorithmic patterns with stack-augmented recurrent nets},
  author={Joulin, Armand and Mikolov, Tomas},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{grefenstette2015learning,
  title={Learning to transduce with unbounded memory},
  author={Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{patel2022revisiting,
  title={Revisiting the compositional generalization abilities of neural sequence models},
  author={Patel, Arkil and Bhattamishra, Satwik and Blunsom, Phil and Goyal, Navin},
  journal={arXiv preprint arXiv:2203.07402},
  year={2022}
}

@inproceedings{socher-etal-2012-semantic,
    title = "Semantic Compositionality through Recursive Matrix-Vector Spaces",
    author = "Socher, Richard  and
      Huval, Brody  and
      Manning, Christopher D.  and
      Ng, Andrew Y.",
    editor = "Tsujii, Jun{'}ichi  and
      Henderson, James  and
      Pa{\c{s}}ca, Marius",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D12-1110",
    pages = "1201--1211",
}

@article{conklin2021meta,
  title={Meta-learning to compositionally generalize},
  author={Conklin, Henry and Wang, Bailin and Smith, Kenny and Titov, Ivan},
  journal={arXiv preprint arXiv:2106.04252},
  year={2021}
}

@article{lindemann2023compositional,
  title={Compositional generalization without trees using multiset tagging and latent permutations},
  author={Lindemann, Matthias and Koller, Alexander and Titov, Ivan},
  journal={arXiv preprint arXiv:2305.16954},
  year={2023}
}

@article{russin2019compositional,
  title={Compositional generalization in a deep seq2seq model by separating syntax and semantics},
  author={Russin, Jake and Jo, Jason and O'Reilly, Randall C and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1904.09708},
  year={2019}
}

@article{lake2019compositional,
  title={Compositional generalization through meta sequence-to-sequence learning},
  author={Lake, Brenden M},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{dusell2021learning,
  title={Learning hierarchical structures with differentiable nondeterministic stacks},
  author={DuSell, Brian and Chiang, David},
  journal={arXiv preprint arXiv:2109.01982},
  year={2021}
}

@article{chen2020compositional,
  title={Compositional generalization via neural-symbolic stack machines},
  author={Chen, Xinyun and Liang, Chen and Yu, Adams Wei and Song, Dawn and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1690--1701},
  year={2020}
}

@book{pinker2003language,
  title={The language instinct: How the mind creates language},
  author={Pinker, Steven},
  year={2003},
  publisher={Penguin uK}
}

@inproceedings{huang2022directed,
  title={Directed acyclic transformer for non-autoregressive machine translation},
  author={Huang, Fei and Zhou, Hao and Liu, Yang and Li, Hang and Huang, Minlie},
  booktitle={International Conference on Machine Learning},
  pages={9410--9428},
  year={2022},
  organization={PMLR}
}

@article{Smolensky_1988, title={On the proper treatment of connectionism}, volume={11}, DOI={10.1017/S0140525X00052432}, number={1}, journal={Behavioral and Brain Sciences}, author={Smolensky, Paul}, year={1988}, pages={1–23}}

@book{Marcus2001-MARTAM-10,
	author = {Gary F. Marcus},
	editor = {},
	publisher = {MIT Press},
	title = {The Algebraic Mind: Integrating Connectionism and Cognitive Science},
	year = {2001}
}

@book{garcez2008neural,
  title={Neural-symbolic cognitive reasoning},
  author={Garcez, Artur SD'Avila and Lamb, Luis C and Gabbay, Dov M},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{t5,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {deep learning, attention based models, multi-task learning, natural language processing, transfer learning}
}

@article{nslr,
  author       = {Tarek R. Besold and
                  Artur S. d'Avila Garcez and
                  Sebastian Bader and
                  Howard Bowman and
                  Pedro M. Domingos and
                  Pascal Hitzler and
                  Kai{-}Uwe K{\"{u}}hnberger and
                  Lu{\'{\i}}s C. Lamb and
                  Daniel Lowd and
                  Priscila Machado Vieira Lima and
                  Leo de Penning and
                  Gadi Pinkas and
                  Hoifung Poon and
                  Gerson Zaverucha},
  title        = {Neural-Symbolic Learning and Reasoning: {A} Survey and Interpretation},
  journal      = {CoRR},
  volume       = {abs/1711.03902},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.03902},
  eprinttype    = {arXiv},
  eprint       = {1711.03902},
  timestamp    = {Mon, 13 Aug 2018 16:48:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-03902.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{GARNELO201917,
title = {Reconciling deep learning with symbolic artificial intelligence: representing objects and relations},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {17-23},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301943},
author = {Marta Garnelo and Murray Shanahan},
abstract = {In the history of the quest for human-level artificial intelligence, a number of rival paradigms have vied for supremacy. Symbolic artificial intelligence was dominant for much of the 20th century, but currently a connectionist paradigm is in the ascendant, namely machine learning with deep neural networks. However, both paradigms have strengths and weaknesses, and a significant challenge for the field today is to effect a reconciliation. A central tenet of the symbolic paradigm is that intelligence results from the manipulation of abstract compositional representations whose elements stand for objects and relations. If this is correct, then a key objective for deep learning is to develop architectures capable of discovering objects and relations in raw data, and learning how to represent them in ways that are useful for downstream processing. This short review highlights recent progress in this direction.}
}

@article{kim_cogs_2020,
	title = {{COGS}: {A} {Compositional} {Generalization} {Challenge} {Based} on {Semantic} {Interpretation}},
	shorttitle = {{COGS}},
	url = {http://arxiv.org/abs/2010.05465},
	abstract = {Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99\%), but generalization accuracy was substantially lower (16–35\%) and showed high sensitivity to random seed (±6–8\%). These ﬁndings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.},
	language = {en},
	urldate = {2021-01-25},
	journal = {arXiv:2010.05465 [cs]},
	author = {Kim, Najoung and Linzen, Tal},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.05465},
	keywords = {Computer Science - Computation and Language},
}

@misc{Soulos_2023_DifferentiableTreeOperations,
	title = {Differentiable {Tree} {Operations} {Promote} {Compositional} {Generalization}},
	url = {http://arxiv.org/abs/2306.00751},
	doi = {10.48550/arXiv.2306.00751},
	abstract = {In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine (DTM) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, DTM achieves 100\% while existing baselines such as Transformer, Tree Transformer, LSTM, and Tree2Tree LSTM achieve less than 30\%. DTM remains highly interpretable in addition to its perfect performance.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Soulos, Paul and Hu, Edward and McCurdy, Kate and Chen, Yunmo and Fernandez, Roland and Smolensky, Paul and Gao, Jianfeng},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00751 [cs]},
	annote = {Comment: ICML 2023. Code available at https://github.com/psoulos/dtm},
	file = {Soulos et al. - 2023 - Differentiable Tree Operations Promote Composition.pdf:/Users/psoulos/Zotero/storage/WSWQKEIJ/Soulos et al. - 2023 - Differentiable Tree Operations Promote Composition.pdf:application/pdf},
}


@InProceedings{pmlr-v97-lee19d,
  title = 	 {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks},
  author =       {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3744--3753},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/lee19d/lee19d.pdf},
  url = 	 {https://proceedings.mlr.press/v97/lee19d.html},
  abstract = 	 {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.}
}

@article{
li2023representations,
title={Representations and Computations in Transformers that Support Generalization on Structured Tasks},
author={Yuxuan Li and James McClelland},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=oFC2LAqS6Z},
note={}
}

@inproceedings{ruoss-etal-2023-randomized,
    title = "Randomized Positional Encodings Boost Length Generalization of Transformers",
    author = "Ruoss, Anian  and
      Del{\'e}tang, Gr{\'e}goire  and
      Genewein, Tim  and
      Grau-Moya, Jordi  and
      Csord{\'a}s, R{\'o}bert  and
      Bennani, Mehdi  and
      Legg, Shane  and
      Veness, Joel",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.161",
    doi = "10.18653/v1/2023.acl-short.161",
    pages = "1889--1903",
    abstract = "Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence{'}s length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0{\%} on average).",
}

@inproceedings{NEURIPS2019_6e091746,
 author = {Shiv, Vighnesh and Quirk, Chris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Novel positional encodings to enable tree-based transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf},
 volume = {32},
 year = {2019}
}


@book{plate,
author = {Plate, Tony A.},
title = {Holographic Reduced Representation: Distributed Representation for Cognitive Structures},
year = {2003},
isbn = {1575864290},
publisher = {CSLI Publications},
address = {USA}
}

@article{kanerva2009hyperdimensional,
  title={Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors},
  author={Kanerva, Pentti},
  journal={Cognitive computation},
  volume={1},
  pages={139--159},
  year={2009},
  publisher={Springer}
}

@article{kleyko2022,
author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
title = {A Survey on Hyperdimensional Computing Aka Vector Symbolic Architectures, Part I: Models and Data Transformations},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3538531},
doi = {10.1145/3538531},
abstract = {This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and distributed vector representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent years, the necessity for a comprehensive survey of the field has become extremely important. Therefore, amongst other aspects of the field, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey [84] is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {130},
numpages = {40},
keywords = {tensor product representations, holographic reduced representations, machine learning, Artificial intelligence, modular composite representations, sparse binary distributed representations, sparse block codes, multiply-add-permute, geometric analogue of holographic reduced representations, data structures, hyperdimensional computing, binary spatter codes, distributed representations, matrix binding of additive terms, vector symbolic architectures}
}

@inproceedings{
yogatama2018memory,
title={Memory Architectures in Recurrent Neural Network Language Models},
author={Dani Yogatama and Yishu Miao and Gabor Melis and Wang Ling and Adhiguna Kuncoro and Chris Dyer and Phil Blunsom},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SkFqf0lAZ},
}

@inproceedings{Sun_2023_ReplicationStudyCompositional,
	title = {A {Replication} {Study} of {Compositional} {Generalization} {Works} on {Semantic} {Parsing}},
	url = {https://openreview.net/forum?id=MF9uv95psps},
	abstract = {Reproducibility Summary Scope of Reproducibility — We examine the reproducibility of compositional generalization results from the task of semantic parsing. We aim to reproduce results from [1], [2], and [3] and seek to verify the claims that 1. A model shouldn't be expected to perform well on non-synthetic datasets just because it performs well on SCAN [1], 2. The approaches from [1] and [2] meet or exceed baseline performance on compositional generalization tests, and 3. NQG-T5 [1] outperforms baselines on both synthetic and natural data. 4. NQG [1] performs well on the instances that it is able to generate a prediction, but it faces the barrier of not being able to generate predictions for all instances. Methodology — We reuse the authors' code along with additional code to run extra experiments, and we re-implement scripts whose support is deprecated. Eight 32GB GPUs were used for experiments, with a detailed description in Section 3.3. Results — Claim 1 is verified: the model with the highest performance on SCAN does not maintain its high performance on other datasets (Section 4.1). Claims 2 and 3 are verified, with a comparison of performance between NQG-T5 and the selected baseline models in [1] and [2]. Claim 4 is also verified by computing the coverage and precision of NQG in Section 4.4. Overall, accuracy for most experiments reaches within 2\% of that reported in the original paper, with a deviation that our T5 achieves higher performance on some splits and slightly lower performance on one split than reported previously. What was easy — All papers provide clearly‐written code and informative documentation, as well as lists of hyperparameters that are used for experiments. The papers also describe their approaches clearly, making the experimental workflow easy to follow. What was difficult — The exact match evaluation metric is formulated somewhat differently across all three papers, leading to a non‐negligible value difference, as discussed in Section 5.2. We also had to re‐implement some training scripts because an original dependency is no longer supported. Moreover, some experiments are computationally expensive: [1] used TPUs for experiments, while our replication with GPUs take several days to train a single T5 model. Communication with original authors — The authors of all three papers provided us with useful instruction to work with their methods and constructive feedback on the draft.},
	language = {en},
	urldate = {2024-03-21},
	author = {Sun, Kaiser and Williams, Adina and Hupkes, Dieuwke},
	month = aug,
	year = {2023},
	file = {Full Text PDF:/Users/psoulos/Zotero/storage/A2K6CK23/Sun et al. - 2023 - A Replication Study of Compositional Generalizatio.pdf:application/pdf},
}


@inproceedings{csordas-etal-2021-devil,
    title = "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
    author = "Csord{\'a}s, R{\'o}bert  and
      Irie, Kazuki  and
      Schmidhuber, Juergen",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.49",
    doi = "10.18653/v1/2021.emnlp-main.49",
    pages = "619--634",
    abstract = "Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50{\%} to 85{\%} on the PCFG productivity split, and from 35{\%} to 81{\%} on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100{\%} accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results.",
}

@inproceedings{NEURIPS2018_d759175d,
 author = {Chen, Xinyun and Liu, Chang and Song, Dawn},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Tree-to-tree Neural Networks for Program Translation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{shaw-etal-2021-compositional,
    title = "Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?",
    author = "Shaw, Peter  and
      Chang, Ming-Wei  and
      Pasupat, Panupong  and
      Toutanova, Kristina",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.75",
    doi = "10.18653/v1/2021.acl-long.75",
    pages = "922--938",
    abstract = "Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization? To better assess this capability, we propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model. It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations. While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.",
}

@inproceedings{smith-eisner-2006-quasi,
    title = "Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies",
    author = "Smith, David  and
      Eisner, Jason",
    editor = "Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings on the Workshop on Statistical Machine Translation",
    month = jun,
    year = "2006",
    address = "New York City",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-3104",
    pages = "23--30",
}

@book{10.7551/mitpress/9780262527347.001.0001,
    author = {Chomsky, Noam},
    title = "{The Minimalist Program}",
    publisher = {The MIT Press},
    year = {2014},
    month = {12},
    abstract = "{A classic work that situates linguistic theory in the broader cognitive sciences, formulating and developing the minimalist program. In his foundational book, The Minimalist Program, published in 1995, Noam Chomsky offered a significant contribution to the generative tradition in linguistics. This twentieth-anniversary edition reissues this classic work with a new preface by the author. In four essays, Chomsky attempts to situate linguistic theory in the broader cognitive sciences, with the essays formulating and progressively developing the minimalist approach to linguistic theory. Building on the theory of principles and parameters and, in particular, on principles of economy of derivation and representation, the minimalist framework takes Universal Grammar as providing a unique computational system, with derivations driven by morphological properties, to which the syntactic variation of languages is also restricted. Within this theoretical framework, linguistic expressions are generated by optimally efficient derivations that must satisfy the conditions that hold on interface levels, the only levels of linguistic representation. The interface levels provide instructions to two types of performance systems, articulatory-perceptual and conceptual-intentional. All syntactic conditions, then, express properties of these interface levels, reflecting the interpretive requirements of language and keeping to very restricted conceptual resources. In the preface to this edition, Chomsky emphasizes that the minimalist approach developed in the book and in subsequent work “is a program, not a theory.” With this book, Chomsky built on pursuits from the earliest days of generative grammar to formulate a new research program that had far-reaching implications for the field.}",
    isbn = {9780262327282},
    doi = {10.7551/mitpress/9780262527347.001.0001},
    url = {https://doi.org/10.7551/mitpress/9780262527347.001.0001},
}


@article{Lake_2018_GeneralizationSystematicityCompositional,
	title = {Generalization without systematicity: {On} the compositional skills of sequence-to-sequence recurrent networks},
	volume = {7},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generaliza-tions when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
	journal = {35th International Conference on Machine Learning, ICML 2018},
	author = {Lake, Brenden and Baroni, Marco},
	year = {2018},
	note = {arXiv: 1711.00350
ISBN: 9781510867963},
	pages = {4487--4499},
	file = {Lake and Baroni - 2018 - Generalization without systematicity On the compo.pdf:/Users/psoulos/Zotero/storage/IWTYSLV4/Lake and Baroni - 2018 - Generalization without systematicity On the compo.pdf:application/pdf},
}

@inbook{gorn+1967+77+115,
url = {https://doi.org/10.3138/9781487592769-008},
title = {Explicit Definitions and Linguistic Dominoes},
booktitle = {Systems and Computer Science},
author = {Saul Gorn},
publisher = {University of Toronto Press},
address = {Toronto},
pages = {77--115},
doi = {doi:10.3138/9781487592769-008},
isbn = {9781487592769},
year = {1967},
lastchecked = {2024-05-07}
}

@ARTICLE{bishop-noise,
  author={Bishop, Chris M.},
  journal={Neural Computation}, 
  title={Training with Noise is Equivalent to Tikhonov Regularization}, 
  year={1995},
  volume={7},
  number={1},
  pages={108-116},
  keywords={},
  doi={10.1162/neco.1995.7.1.108}}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@inproceedings{kim-linzen-2020-cogs,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
    abstract = "Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\%}), but generalization accuracy was substantially lower (16{--}35{\%}) and showed high sensitivity to random seed (+-6{--}8{\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",
}

@inproceedings{
keysers2020measuring,
title={Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
author={Daniel Keysers and Nathanael Sch{\"a}rli and Nathan Scales and Hylke Buisman and Daniel Furrer and Sergii Kashubin and Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and Olivier Bousquet},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SygcCnNKwr}
}

@article{nye2020learning,
  title={Learning compositional rules via neural program synthesis},
  author={Nye, Maxwell and Solar-Lezama, Armando and Tenenbaum, Josh and Lake, Brenden M},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10832--10842},
  year={2020}
}

@inproceedings{
soulos2024recurrent,
title={Recurrent Transformers Trade-off Parallelism for Length Generalization on Regular Languages},
author={Paul Soulos and Aleksandar Terzic and Michael Hersche and Abbas Rahimi},
booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=6PjZA4Jvge}
}


@inproceedings{dusell_stack_2024,
	title = {Stack {Attention}: {Improving} the {Ability} of {Transformers} to {Model} {Hierarchical} {Patterns}},
	url = {https://openreview.net/forum?id=XVhm3X8Fum},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {DuSell, Brian and Chiang, David},
	year = {2024},
}

@inproceedings{nye_show_2021,
	title = {Show {Your} {Work}: {Scratchpads} for {Intermediate} {Computation} with {Language} {Models}},
	booktitle = {Deep {Learning} for {Code} {Workshop}},
	author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and {others}},
	year = {2021},
}

@inproceedings{soulos_differentiable_2023,
	title = {Differentiable tree operations promote compositional generalization},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Soulos, Paul and Hu, Edward J and McCurdy, Kate and Chen, Yunmo and Fernandez, Roland and Smolensky, Paul and Gao, Jianfeng},
	year = {2023},
	pages = {32499--32520},
}

@inproceedings{joulin_inferring_2015,
	title = {Inferring {Algorithmic} {Patterns} with {Stack}-{Augmented} {Recurrent} {Nets}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/26657d5ff9020d2abefe558796b99584-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Joulin, Armand and Mikolov, Tomas},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@inproceedings{zhou_transformers_nodate,
	title = {Transformers {Can} {Achieve} {Length} {Generalization} {But} {Not} {Robustly}},
	booktitle = {{ICLR} 2024 {Workshop} on {Mathematical} and {Empirical} {Understanding} of {Foundation} {Models}},
	author = {Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
}

@inproceedings{bahdanau_neural_2015,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015, {San} {Diego}, {CA}, {USA}, {May} 7-9, 2015, {Conference} {Track} {Proceedings}},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
}

@inproceedings{borenstein_what_2024,
	address = {Bangkok, Thailand},
	title = {What {Languages} are {Easy} to {Language}-{Model}? {A} {Perspective} from {Learning} {Probabilistic} {Regular} {Languages}},
	url = {https://aclanthology.org/2024.acl-long.807},
	abstract = {What can large language models learn? By definition, language models (LM) are distributionsover strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf—learning probabilistic languages—rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM. We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Borenstein, Nadav and Svete, Anej and Chan, Robin and Valvoda, Josef and Nowak, Franz and Augenstein, Isabelle and Chodroff, Eleanor and Cotterell, Ryan},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {15115--15134},
}

@article{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
	journal = {Advances in Neural Information Processing Systems},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{nowak_representational_2023,
	address = {Singapore},
	title = {On the {Representational} {Capacity} of {Recurrent} {Neural} {Language} {Models}},
	url = {https://aclanthology.org/2023.emnlp-main.434},
	doi = {10.18653/v1/2023.emnlp-main.434},
	abstract = {This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any deterministic probabilistic Turing machine (PTM) with rationally weighted transitions. Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.},
	urldate = {2024-09-20},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Nowak, Franz and Svete, Anej and Du, Li and Cotterell, Ryan},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {7011--7034},
}

@inproceedings{das1992learning,
  title={Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory},
  author={Das, Sreerupa and Giles, C Lee and Sun, Guo-Zheng},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={14},
  year={1992}
}

@inproceedings{chen_recurrent_2018,
	address = {New Orleans, Louisiana},
	title = {Recurrent {Neural} {Networks} as {Weighted} {Language} {Recognizers}},
	url = {https://aclanthology.org/N18-1205},
	doi = {10.18653/v1/N18-1205},
	abstract = {We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.},
	urldate = {2024-09-20},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Yining and Gilroy, Sorcha and Maletti, Andreas and May, Jonathan and Knight, Kevin},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {2261--2271},
}

@inproceedings{svete_recurrent_2023,
	address = {Singapore},
	title = {Recurrent {Neural} {Language} {Models} as {Probabilistic} {Finite}-state {Automata}},
	url = {https://aclanthology.org/2023.emnlp-main.502},
	doi = {10.18653/v1/2023.emnlp-main.502},
	abstract = {Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the expressive power of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages—rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with N states over an alphabet Σ, an RNN requires Ømegałeft(N {\textbar}{\textbackslash}Sigma{\textbar}{\textbackslash}right) neurons. These results present a first step towards characterizing the classes of distributions RNN LMs can represent and thus help us understand their capabilities and limitations.},
	urldate = {2024-09-20},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Svete, Anej and Cotterell, Ryan},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {8069--8086},
}

@article{siegelmann_computational_1995,
	title = {On the {Computational} {Power} of {Neural} {Nets}},
	volume = {50},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000085710136},
	doi = {https://doi.org/10.1006/jcss.1995.1013},
	abstract = {This paper deals with finite size networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a "sigmoidal" function to a linear combination of the previous states of all units. We prove that one may simulate all Turing machines by such nets. In particular, one can simulate any multi-stack Turing machine in real time, and there is a net made up of 886 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Non-deterministic Turing machines can be simulated by non-deterministic rational nets, also in real time. The simulation result has many consequences regarding the decidability, or more generally the complexity, of questions about recursive nets.},
	number = {1},
	journal = {Journal of Computer and System Sciences},
	author = {Siegelmann, H. T. and Sontag, E. D.},
	year = {1995},
	pages = {132--150},
}

@inproceedings{merrill_illusion_2024,
	title = {The illusion of state in state-space models},
	booktitle = {Forty-first {International} {Conference} on {Machine} {Learning}},
	author = {Merrill, William and Petty, Jackson and Sabharwal, Ashish},
	month = apr,
	year = {2024},
}

@inproceedings{fan_advancing_2024,
	address = {Mexico City, Mexico},
	title = {Advancing {Regular} {Language} {Reasoning} in {Linear} {Recurrent} {Neural} {Networks}},
	url = {https://aclanthology.org/2024.naacl-short.4},
	doi = {10.18653/v1/2024.naacl-short.4},
	abstract = {In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language and long-range modeling, while offering rapid parallel training and constant inference cost. With the resurgence of interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations in modeling regular language. Motivated by this analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN capable of performing length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic. The code is released at https://github.com/tinghanf/RegluarLRNN.},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fan, Ting-Han and Chi, Ta-Chung and Rudnicky, Alexander},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {45--53},
}

@inproceedings{lightman_lets_2024,
	title = {Let's {Verify} {Step} by {Step}},
	url = {https://openreview.net/forum?id=v8L0pN6EOi},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yuri and Edwards, Harrison and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
	year = {2024},
}

@inproceedings{wei_chain_2022,
	title = {Chain of {Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=_VjQlMeSB_J},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@misc{pang_iterative_2024,
	title = {Iterative {Reasoning} {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2404.19733},
	doi = {10.48550/arXiv.2404.19733},
	abstract = {Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer. We train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood term, which we find to be crucial. We show reasoning improves across repeated iterations of this scheme. While only relying on examples in the training set, our approach results in increasing accuracy on GSM8K, MATH, and ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based models not relying on additionally sourced datasets. For example, we see a large improvement from 55.6\% to 81.6\% on GSM8K and an accuracy of 88.7\% with majority voting out of 32 samples.},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Pang, Richard Yuanzhe and Yuan, Weizhe and Cho, Kyunghyun and He, He and Sukhbaatar, Sainbayar and Weston, Jason},
	month = jun,
	year = {2024},
	note = {arXiv:2404.19733 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{hoffmann_training_2022,
	title = {Training compute-optimal large language models},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Thomas and Noland, Eric and Millican, Katherine and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karén and Elsen, Erich and Vinyals, Oriol and Rae, Jack and Sifre, Laurent},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {30016--30030},
}

@incollection{jordan_serial_1997,
	title = {Serial order: {A} parallel distributed processing approach},
	volume = {121},
	booktitle = {Advances in psychology},
	publisher = {Elsevier},
	author = {Jordan, Michael I},
	year = {1997},
	pages = {471--495},
}

@inproceedings{hutchins_block-recurrent_2022,
	title = {Block-{Recurrent} {Transformers}},
	url = {https://openreview.net/forum?id=uloenYmLCAo},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hutchins, DeLesley and Schlag, Imanol and Wu, Yuhuai and Dyer, Ethan and Neyshabur, Behnam},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@inproceedings{didolkar_temporal_2022,
	title = {Temporal {Latent} {Bottleneck}: {Synthesis} of {Fast} and {Slow} {Processing} {Mechanisms} in {Sequence} {Learning}},
	shorttitle = {Temporal {Latent} {Bottleneck}},
	url = {https://openreview.net/forum?id=mq-8p5pUnEX},
	abstract = {Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of \$K\$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of \$K\$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.},
	language = {en},
	urldate = {2024-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Didolkar, Aniket Rajiv and Gupta, Kshitij and Goyal, Anirudh and Gundavarapu, Nitesh Bharadwaj and Lamb, Alex and Ke, Nan Rosemary and Bengio, Yoshua},
	month = oct,
	year = {2022},
}

@inproceedings{dehghani_universal_2018,
	title = {Universal {Transformers}},
	url = {https://openreview.net/forum?id=HyzdRiR9Y7},
	abstract = {Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.},
	language = {en},
	urldate = {2024-09-12},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
	month = sep,
	year = {2018},
}

@inproceedings{deletang_neural_2022,
	title = {Neural {Networks} and the {Chomsky} {Hierarchy}},
	url = {https://openreview.net/forum?id=WbxHAzkeQcn},
	abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (20'910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
	urldate = {2024-05-31},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Deletang, Gregoire and Ruoss, Anian and Grau-Moya, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
	month = sep,
	year = {2022},
}

@inproceedings{liu_transformers_2022,
	title = {Transformers {Learn} {Shortcuts} to {Automata}},
	url = {https://openreview.net/forum?id=De4FYqjFueZ},
	abstract = {Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are these shallow and non-recurrent models finding? We investigate this question in the setting of learning automata, discrete dynamical systems naturally suited to recurrent modeling and expressing algorithmic tasks. Our theoretical results completely characterize shortcut solutions, whereby a shallow Transformer with only \$o(T)\$ layers can exactly replicate the computation of an automaton on an input sequence of length \$T\$. By representing automata using the algebraic structure of their underlying transformation semigroups, we obtain \$O({\textbackslash}log T)\$-depth simulators for all automata and \$O(1)\$-depth simulators for all automata whose associated groups are solvable. Empirically, we perform synthetic experiments by training Transformers to simulate a wide variety of automata, and show that shortcut solutions can be learned via standard training. We further investigate the brittleness of these solutions and propose potential mitigations.},
	language = {en},
	urldate = {2024-06-04},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Liu, Bingbin and Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
	month = sep,
	year = {2022},
}

@article{joshi_convergence_1990,
	title = {The convergence of mildly context-sensitive grammar formalisms},
	author = {Joshi, Aravind K and Vijay-Shanker, Krishnamurti and Weir, David and {others}},
	year = {1990},
	note = {Publisher: Citeseer},
}

@article{joshi_tree_1985,
	title = {Tree adjoining grammars: {How} much context-sensitivity is required to provide reasonable structural descriptions?},
	journal = {Natural Language Parsing},
	author = {Joshi, Aravind K},
	year = {1985},
	note = {Publisher: Cambridge University Press},
	pages = {206--250},
}

@inproceedings{ju_staircase_2022,
	title = {Staircase {Attention} for {Recurrent} {Processing} of {Sequences}},
	url = {https://openreview.net/forum?id=NiCJDYpKaBj},
	abstract = {Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence). Thus our model can trade off performance and compute, by increasing the amount of recurrence through time and depth. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.},
	language = {en},
	urldate = {2024-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ju, Da and Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason E.},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	month = oct,
	year = {2022},
}

@inproceedings{press_train_2021,
	title = {Train {Short}, {Test} {Long}: {Attention} with {Linear} {Biases} {Enables} {Input} {Length} {Extrapolation}},
	shorttitle = {Train {Short}, {Test} {Long}},
	url = {https://openreview.net/forum?id=R8sQPpGCv0},
	abstract = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11\% faster and using 11\% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.},
	language = {en},
	urldate = {2024-09-17},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Press, Ofir and Smith, Noah and Lewis, Mike},
	month = oct,
	year = {2021},
}

@inproceedings{cho_properties_2014,
	address = {Doha, Qatar},
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}–{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/W14-4012},
	doi = {10.3115/v1/W14-4012},
	urldate = {2024-09-17},
	booktitle = {Proceedings of {SSST}-8, {Eighth} {Workshop} on {Syntax}, {Semantics} and {Structure} in {Statistical} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merriënboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	editor = {Wu, Dekai and Carpuat, Marine and Carreras, Xavier and Vecchi, Eva Maria},
	month = oct,
	year = {2014},
	pages = {103--111},
}

@article{strobl_what_2024,
	title = {What {Formal} {Languages} {Can} {Transformers} {Express}? {A} {Survey}},
	volume = {12},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00663},
	doi = {10.1162/tacl_a_00663},
	abstract = {As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.},
	urldate = {2024-09-16},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
	month = may,
	year = {2024},
	pages = {543--561},
}

@article{merrill_parallelism_2023,
	title = {The {Parallelism} {Tradeoff}: {Limitations} of {Log}-{Precision} {Transformers}},
	volume = {11},
	shorttitle = {The {Parallelism} {Tradeoff}},
	url = {https://aclanthology.org/2023.tacl-1.31},
	doi = {10.1162/tacl_a_00562},
	abstract = {Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if L{\textbackslash}mbox{\textbackslash}neqP (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture's high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it. Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.},
	urldate = {2024-09-16},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Merrill, William and Sabharwal, Ashish},
	year = {2023},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {531--545},
}

@book{hopcroft_introduction_2006,
	address = {USA},
	title = {Introduction to {Automata} {Theory}, {Languages}, and {Computation} (3rd {Edition})},
	isbn = {978-0-321-45536-9},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
	month = jun,
	year = {2006},
}

@article{hahn_theoretical_2020,
	title = {Theoretical {Limitations} of {Self}-{Attention} in {Neural} {Sequence} {Models}},
	volume = {8},
	url = {https://aclanthology.org/2020.tacl-1.11},
	doi = {10.1162/tacl_a_00306},
	abstract = {Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.},
	urldate = {2024-09-16},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Hahn, Michael},
	editor = {Johnson, Mark and Roark, Brian and Nenkova, Ani},
	year = {2020},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {156--171},
}

@inproceedings{huang_encoding_2022,
	title = {Encoding {Recurrence} into {Transformers}},
	url = {https://openreview.net/forum?id=7YfHla7IxBJ},
	abstract = {This paper novelly breaks down with ignorable loss an RNN layer into a sequence of simple RNNs, each of which can be further rewritten into a lightweight positional encoding matrix of a self-attention, named the Recurrence Encoding Matrix (REM). Thus, recurrent dynamics introduced by the RNN layer can be encapsulated into the positional encodings of a multihead self-attention, and this makes it possible to seamlessly incorporate these recurrent dynamics into a Transformer, leading to a new module, Self-Attention with Recurrence (RSA). The proposed module can leverage the recurrent inductive bias of REMs to achieve a better sample efficiency than its corresponding baseline Transformer, while the self-attention is used to model the remaining non-recurrent signals. The relative proportions of these two components are controlled by a data-driven gated mechanism, and the effectiveness of RSA modules are demonstrated by four sequential learning tasks.},
	language = {en},
	urldate = {2024-09-13},
	author = {Huang, Feiqing and Lu, Kexin and Cai, Yuxi and Qin, Zhen and Fang, Yanwen and Tian, Guangjian and Li, Guodong},
	month = sep,
	year = {2022},
}

@inproceedings{kasai_finetuning_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Finetuning {Pretrained} {Transformers} into {RNNs}},
	url = {https://aclanthology.org/2021.emnlp-main.830},
	doi = {10.18653/v1/2021.emnlp-main.830},
	abstract = {Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.},
	urldate = {2024-09-13},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kasai, Jungo and Peng, Hao and Zhang, Yizhe and Yogatama, Dani and Ilharco, Gabriel and Pappas, Nikolaos and Mao, Yi and Chen, Weizhu and Smith, Noah A.},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {10630--10643},
}

@article{chomsky_certain_1959,
	title = {On {Certain} {Formal} {Properties} of {Grammars}},
	volume = {2},
	url = {https://api.semanticscholar.org/CorpusID:16792674},
	journal = {Inf. Control.},
	author = {Chomsky, Noam},
	year = {1959},
	pages = {137--167},
}

@inproceedings{kazemnejad_impact_2023,
	title = {The {Impact} of {Positional} {Encoding} on {Length} {Generalization} in {Transformers}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4e85362c02172c0c6567ce593122d31c-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {24892--24928},
}

@article{su_roformer_2024,
	title = {Roformer: {Enhanced} transformer with rotary position embedding},
	volume = {568},
	journal = {Neurocomputing},
	author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
	year = {2024},
	note = {Publisher: Elsevier},
	pages = {127063},
}

@misc{zhou_transformers_2024,
	title = {Transformers {Can} {Achieve} {Length} {Generalization} {But} {Not} {Robustly}},
	url = {http://arxiv.org/abs/2402.09371},
	doi = {10.48550/arXiv.2402.09371},
	abstract = {Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.},
	urldate = {2024-09-12},
	publisher = {arXiv},
	author = {Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09371 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{shaw_self-attention_2018,
	address = {New Orleans, Louisiana},
	title = {Self-{Attention} with {Relative} {Position} {Representations}},
	url = {https://aclanthology.org/N18-2074},
	doi = {10.18653/v1/N18-2074},
	abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
	urldate = {2024-09-12},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {464--468},
}

@article{chomsky_three_1956,
	title = {Three models for the description of language},
	volume = {2},
	doi = {10.1109/TIT.1956.1056813},
	number = {3},
	journal = {IRE Transactions on Information Theory},
	author = {Chomsky, N.},
	year = {1956},
	keywords = {Impedance matching, Kernel, Laboratories, Markov processes, Natural languages, Research and development, Testing},
	pages = {113--124},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2024-09-12},
	journal = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@incollection{lashley_problem_1951,
	address = {Oxford, England},
	title = {The problem of serial order in behavior},
	abstract = {A discussion of "the logical and orderly arrangement of thought and action" from the point of view that "the input is never into a quiescent or static system, but always into a system which is already actively excited and organized" and that "behavior is the result of interaction of this background of excitation with input from any designated stimulus." Particular emphasis is placed on the time factor in behavior. A panel discussion is included between pages 136-146. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	booktitle = {Cerebral mechanisms in behavior; the {Hixon} {Symposium}},
	publisher = {Wiley},
	author = {Lashley, K. S.},
	year = {1951},
	pages = {112--146},
}

@inproceedings{csordas_neural_2021,
	title = {The {Neural} {Data} {Router}: {Adaptive} {Control} {Flow} in {Transformers} {Improves} {Systematic} {Generalization}},
	shorttitle = {The {Neural} {Data} {Router}},
	url = {https://openreview.net/forum?id=KBQP4A_J1K},
	abstract = {Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100\% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR’s attention and gating patterns tend to be interpretable as an intuitive form of neural routing},
	language = {en},
	urldate = {2024-07-26},
	author = {Csordás, Róbert and Irie, Kazuki and Schmidhuber, Jürgen},
	month = oct,
	year = {2021},
}

@misc{bhattamishra_separations_2024,
	title = {Separations in the {Representational} {Capabilities} of {Transformers} and {Recurrent} {Architectures}},
	url = {http://arxiv.org/abs/2406.09347},
	doi = {10.48550/arXiv.2406.09347},
	abstract = {Transformer architectures have been widely adopted in foundation models. Due to their high inference costs, there is renewed interest in exploring the potential of efficient recurrent architectures (RNNs). In this paper, we analyze the differences in the representational capabilities of Transformers and RNNs across several tasks of practical relevance, including index lookup, nearest neighbor, recognizing bounded Dyck languages, and string equality. For the tasks considered, our results show separations based on the size of the model required for different architectures. For example, we show that a one-layer Transformer of logarithmic width can perform index lookup, whereas an RNN requires a hidden state of linear size. Conversely, while constant-size RNNs can recognize bounded Dyck languages, we show that one-layer Transformers require a linear size for this task. Furthermore, we show that two-layer Transformers of logarithmic size can perform decision tasks such as string equality or disjointness, whereas both one-layer Transformers and recurrent models require linear size for these tasks. We also show that a log-size two-layer Transformer can implement the nearest neighbor algorithm in its forward pass; on the other hand recurrent models require linear size. Our constructions are based on the existence of \$N\$ nearly orthogonal vectors in \$O({\textbackslash}log N)\$ dimensional space and our lower bounds are based on reductions from communication complexity problems. We supplement our theoretical results with experiments that highlight the differences in the performance of these architectures on practical-size sequences.},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Bhattamishra, Satwik and Hahn, Michael and Blunsom, Phil and Kanade, Varun},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09347 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{liu_exposing_2023,
	title = {Exposing {Attention} {Glitches} with {Flip}-{Flop} {Language} {Modeling}},
	url = {https://openreview.net/forum?id=VzmpXQAn6E},
	abstract = {Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of \_attention glitches\_, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce \_flip-flop language modeling\_ (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs.},
	language = {en},
	urldate = {2024-07-23},
	author = {Liu, Bingbin and Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
	month = nov,
	year = {2023},
}

@inproceedings{kasai_finetuning_2021-1,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Finetuning {Pretrained} {Transformers} into {RNNs}},
	url = {https://aclanthology.org/2021.emnlp-main.830},
	doi = {10.18653/v1/2021.emnlp-main.830},
	abstract = {Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.},
	urldate = {2024-07-19},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kasai, Jungo and Peng, Hao and Zhang, Yizhe and Yogatama, Dani and Ilharco, Gabriel and Pappas, Nikolaos and Mao, Yi and Chen, Weizhu and Smith, Noah A.},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {10630--10643},
}

@inproceedings{ding_ernie-doc_2021,
	address = {Online},
	title = {{ERNIE}-{Doc}: {A} {Retrospective} {Long}-{Document} {Modeling} {Transformer}},
	shorttitle = {{ERNIE}-{Doc}},
	url = {https://aclanthology.org/2021.acl-long.227},
	doi = {10.18653/v1/2021.acl-long.227},
	abstract = {Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.},
	urldate = {2024-07-15},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ding, SiYu and Shang, Junyuan and Wang, Shuohuan and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {2914--2927},
}

@book{chomsky_aspects_1965,
	edition = {50},
	title = {Aspects of the {Theory} of {Syntax}},
	isbn = {978-0-262-52740-8},
	url = {https://www.jstor.org/stable/j.ctt17kk81z},
	abstract = {Noam Chomsky's \textit{Aspects of the Theory of Syntax} , published in 1965, was a landmark work in generative grammar that introduced certain technical innovations still drawn upon in contemporary work. The fiftieth anniversary edition of this influential book includes a new preface by the author that identifies proposals that seem to be of lasting significance, reviews changes and improvements in the formulation and implementation of basic ideas, and addresses some of the controversies that arose over the general framework.Beginning in the mid-fifties and emanating largely from MIT, linguists developed an approach to linguistic theory and to the study of the structure of particular languages that diverged in many respects from conventional modern linguistics. Although the new approach was connected to the traditional study of languages, it differed enough in its specific conclusions about the structure of language to warrant a name, "generative grammar." Various deficiencies were discovered in the first attempts to formulate a theory of transformational generative grammar and in the descriptive analysis of particular languages that motivated these formulations. At the same time, it became apparent that these formulations can be extended and deepened. In this book, Chomsky reviews these developments and proposes a reformulation of the theory of transformational generative grammar that takes them into account. The emphasis in this study is syntax; semantic and phonological aspects of the language structure are discussed only insofar as they bear on syntactic theory.},
	urldate = {2024-07-12},
	publisher = {The MIT Press},
	author = {Chomsky, Noam},
	year = {1965},
}

@book{hale_automaton_2014,
	series = {Studies in {Computational} {Linguistics}},
	title = {Automaton {Theories} of {Human} {Sentence} {Comprehension}},
	isbn = {978-1-57586-747-2},
	url = {https://press.uchicago.edu/ucp/books/book/distributed/A/bo20712531.html},
	abstract = {By relating grammar to cognitive architecture, John T. Hale shows step-by-step how incremental parsing works in models of perceptual processing and how specific learning rules might lead to frequency-sensitive preferences. Along the way, Hale reconsiders garden-pathing, the parallel/serial distinction, and information-theoretical complexity metrics, such as surprisal. This book is a must for cognitive scientists of language.},
	language = {en},
	urldate = {2024-07-12},
	publisher = {Center for the Study of Language and Information},
	author = {Hale, John T.},
	month = oct,
	year = {2014},
}

@misc{chomsky_minimalist_nodate,
	title = {The {Minimalist} {Program}},
	url = {https://mitpress.mit.edu/9780262531283/the-minimalist-program/},
	abstract = {The Minimalist Program consists of four recent essays that attempt to situate linguistic theory in the broader cognitive sciences. In these essays the minima...},
	language = {en-US},
	urldate = {2024-07-12},
	journal = {MIT Press},
	author = {Chomsky, Noam},
}

@misc{tai_improved_2015,
	title = {Improved {Semantic} {Representations} {From} {Tree}-{Structured} {Long} {Short}-{Term} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1503.00075},
	doi = {10.48550/arXiv.1503.00075},
	abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	month = may,
	year = {2015},
	note = {arXiv:1503.00075 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{socher_recursive_2013,
	address = {Seattle, Washington, USA},
	title = {Recursive {Deep} {Models} for {Semantic} {Compositionality} {Over} a {Sentiment} {Treebank}},
	url = {https://aclanthology.org/D13-1170},
	urldate = {2024-07-12},
	booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
	editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
	month = oct,
	year = {2013},
	pages = {1631--1642},
}

@inproceedings{hewitt_structural_2019,
	address = {Minneapolis, Minnesota},
	title = {A {Structural} {Probe} for {Finding} {Syntax} in {Word} {Representations}},
	url = {https://aclanthology.org/N19-1419},
	doi = {10.18653/v1/N19-1419},
	abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.},
	urldate = {2024-07-12},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Manning, Christopher D.},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4129--4138},
}

@misc{mccoy_rnns_2019,
	title = {{RNNs} {Implicitly} {Implement} {Tensor} {Product} {Representations}},
	url = {http://arxiv.org/abs/1812.08718},
	doi = {10.48550/arXiv.1812.08718},
	abstract = {Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations.},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {McCoy, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
	month = mar,
	year = {2019},
	note = {arXiv:1812.08718 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{gayler_vector_2004,
	title = {Vector {Symbolic} {Architectures} answer {Jackendoff}'s challenges for cognitive neuroscience},
	url = {http://arxiv.org/abs/cs/0412059},
	doi = {10.48550/arXiv.cs/0412059},
	abstract = {Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Gayler, Ross W.},
	month = dec,
	year = {2004},
	note = {arXiv:cs/0412059},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, I.2.0, I.2.6, I.5.1},
}

@misc{cho_parallel_2020,
	title = {Parallel parsing in a {Gradient} {Symbolic} {Computation} parser},
	url = {https://osf.io/utcgv},
	doi = {10.31234/osf.io/utcgv},
	abstract = {In many cognitive domains, comprehenders construct structured, discrete representations of the environment. Because information is distributed over time, and partial information may not unambiguously identify a single representation, multiple possible structures must be maintained during incremental comprehension. How can the continuous-time, continuous-state neural cognitive system address these challenges? We propose a neural network approach, building on previous research in the Gradient Symbolic Computation framework in the domain of sentence processing. We introduce brick roles, a neurally- plausible, scalable distributed representation encoding binary tree structures. The appropriate structure is computed via an optimization process implementing a probabilistic context-free grammar. In the face of structural uncertainty encountered during incremental parsing, optimization yields conjunctive blends: states where multiple possible structures are simultaneously present (vs. disjunctive representations such as probabilistic mixtures). The degree of blending is controlled via a commitment parameter which drives local parsing decisions. We introduce a novel training algorithm for learning optimization parameters, and an improved policy for controlling commitment over a range of grammars. This provides a computational foundation for developing proposals integrating continuous and discrete aspects of sentence processing.},
	language = {en-us},
	urldate = {2024-07-11},
	publisher = {OSF},
	author = {Cho, Pyeong Whan and Goldrick, Matthew and Smolensky, Paul},
	month = apr,
	year = {2020},
	keywords = {Gradient Symbolic Computation, Sentence processing, neural networks, parsing},
}

@article{bulatov_beyond_2024,
	title = {Beyond {Attention}: {Breaking} the {Limits} of {Transformer} {Context} {Length} with {Recurrent} {Memory}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Beyond {Attention}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29722},
	doi = {10.1609/aaai.v38i16.29722},
	abstract = {A major limitation for the broader scope of problems solvable by transformers is the quadratic scaling of computational complexity with input size. In this study, we investigate the recurrent memory augmentation of pre-trained transformer models to extend input context length while linearly scaling compute. Our approach demonstrates the capability to store information in memory for sequences of up to an unprecedented two million tokens while maintaining high retrieval accuracy. Experiments with language modeling tasks show perplexity improvement as the number of processed input segments increases. These results underscore the effectiveness of our method, which has significant potential to enhance long-term dependency handling in natural language understanding and generation tasks, as well as enable large-scale context processing for memory-intensive applications.},
	language = {en},
	number = {16},
	urldate = {2024-07-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bulatov, Aydar and Kuratov, Yuri and Kapushev, Yermek and Burtsev, Mikhail},
	month = mar,
	year = {2024},
	note = {Number: 16},
	keywords = {NLP: Generation},
	pages = {17700--17708},
}

@misc{kuratov_search_2024,
	title = {In {Search} of {Needles} in a {11M} {Haystack}: {Recurrent} {Memory} {Finds} {What} {LLMs} {Miss}},
	shorttitle = {In {Search} of {Needles} in a {11M} {Haystack}},
	url = {http://arxiv.org/abs/2402.10790},
	doi = {10.48550/arXiv.2402.10790},
	abstract = {This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to \$10{\textasciicircum}4\$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to \$11{\textbackslash}times 10{\textasciicircum}6\$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.},
	urldate = {2024-07-05},
	publisher = {arXiv},
	author = {Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10790 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{fan_addressing_2021,
	title = {Addressing {Some} {Limitations} of {Transformers} with {Feedback} {Memory}},
	url = {http://arxiv.org/abs/2002.09402},
	doi = {10.48550/arXiv.2002.09402},
	abstract = {Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Fan, Angela and Lavril, Thibaut and Grave, Edouard and Joulin, Armand and Sukhbaatar, Sainbayar},
	month = jan,
	year = {2021},
	note = {arXiv:2002.09402 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bulatov_recurrent_2022,
	title = {Recurrent {Memory} {Transformer}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/47e288629a6996a17ce50b90a056a0e1-Abstract-Conference.html},
	language = {en},
	urldate = {2024-07-03},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail},
	month = dec,
	year = {2022},
	pages = {11079--11091},
}

@misc{ju_staircase_2021,
	title = {Staircase {Attention} for {Recurrent} {Processing} of {Sequences}},
	url = {http://arxiv.org/abs/2106.04279},
	doi = {10.48550/arXiv.2106.04279},
	abstract = {Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.},
	urldate = {2024-06-28},
	publisher = {arXiv},
	author = {Ju, Da and Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason},
	month = jun,
	year = {2021},
	note = {arXiv:2106.04279 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{bhattamishra_separations_2024-1,
	title = {Separations in the {Representational} {Capabilities} of {Transformers} and {Recurrent} {Architectures}},
	url = {https://www.semanticscholar.org/paper/Separations-in-the-Representational-Capabilities-of-Bhattamishra-Hahn/1073f194e3714757bf9a237a83bc55502aa41b95?utm_source=alert_email&utm_content=PaperCitation&utm_campaign=AlertEmails_WEEKLY&utm_term=AuthorPaper+PaperCitation+LibraryFolder+AuthorCitation&email_index=2-0-2&utm_medium=35716914},
	abstract = {Transformer architectures have been widely adopted in foundation models. Due to their high inference costs, there is renewed interest in exploring the potential of efficient recurrent architectures (RNNs). In this paper, we analyze the differences in the representational capabilities of Transformers and RNNs across several tasks of practical relevance, including index lookup, nearest neighbor, recognizing bounded Dyck languages, and string equality. For the tasks considered, our results show separations based on the size of the model required for different architectures. For example, we show that a one-layer Transformer of logarithmic width can perform index lookup, whereas an RNN requires a hidden state of linear size. Conversely, while constant-size RNNs can recognize bounded Dyck languages, we show that one-layer Transformers require a linear size for this task. Furthermore, we show that two-layer Transformers of logarithmic size can perform decision tasks such as string equality or disjointness, whereas both one-layer Transformers and recurrent models require linear size for these tasks. We also show that a log-size two-layer Transformer can implement the nearest neighbor algorithm in its forward pass; on the other hand recurrent models require linear size. Our constructions are based on the existence of \$N\$ nearly orthogonal vectors in \$O({\textbackslash}log N)\$ dimensional space and our lower bounds are based on reductions from communication complexity problems. We supplement our theoretical results with experiments that highlight the differences in the performance of these architectures on practical-size sequences.},
	urldate = {2024-06-27},
	author = {Bhattamishra, S. and Hahn, Michael and Blunsom, Phil and Kanade, Varun},
	month = jun,
	year = {2024},
}

@inproceedings{park_discrete_2024,
	title = {Discrete {Dictionary}-based {Decomposition} {Layer} for {Structured} {Representation} {Learning}},
	url = {https://www.semanticscholar.org/paper/Discrete-Dictionary-based-Decomposition-Layer-for-Park-Kim/b250c272d4ed9df8c19a6cc8f060eb00ab0db567?utm_source=alert_email&utm_content=AuthorCitation&utm_campaign=AlertEmails_WEEKLY&utm_term=AuthorPaper+PaperCitation+LibraryFolder+AuthorCitation&email_index=0-0-0&utm_medium=35716914},
	abstract = {Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data.},
	urldate = {2024-06-27},
	author = {Park, Taewon and Kim, Hyun-Chul and Lee, Minho},
	month = jun,
	year = {2024},
}

@article{li_transformer_2024,
	title = {A {Transformer} with {Stack} {Attention}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2405.04515},
	doi = {10.48550/ARXIV.2405.04515},
	abstract = {Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-based attention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages.},
	urldate = {2024-06-27},
	author = {Li, Jiaoda and White, Jennifer C. and Sachan, Mrinmaya and Cotterell, Ryan},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 2},
}

@article{dave_investigating_2024,
	title = {Investigating {Symbolic} {Capabilities} of {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2405.13209},
	doi = {10.48550/ARXIV.2405.13209},
	abstract = {Prompting techniques have significantly enhanced the capabilities of Large Language Models (LLMs) across various complex tasks, including reasoning, planning, and solving math word problems. However, most research has predominantly focused on language-based reasoning and word problems, often overlooking the potential of LLMs in handling symbol-based calculations and reasoning. This study aims to bridge this gap by rigorously evaluating LLMs on a series of symbolic tasks, such as addition, multiplication, modulus arithmetic, numerical precision, and symbolic counting. Our analysis encompasses eight LLMs, including four enterprise-grade and four open-source models, of which three have been pre-trained on mathematical tasks. The assessment framework is anchored in Chomsky's Hierarchy, providing a robust measure of the computational abilities of these models. The evaluation employs minimally explained prompts alongside the zero-shot Chain of Thoughts technique, allowing models to navigate the solution process autonomously. The findings reveal a significant decline in LLMs' performance on context-free and context-sensitive symbolic tasks as the complexity, represented by the number of symbols, increases. Notably, even the fine-tuned GPT3.5 exhibits only marginal improvements, mirroring the performance trends observed in other models. Across the board, all models demonstrated a limited generalization ability on these symbol-intensive tasks. This research underscores LLMs' challenges with increasing symbolic complexity and highlights the need for specialized training, memory and architectural adjustments to enhance their proficiency in symbol-based reasoning tasks.},
	urldate = {2024-06-27},
	author = {Dave, Neisarg and Kifer, Daniel and Giles, C. Lee and Mali, Ankur},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
}

@misc{lampinen_learned_2024,
	title = {Learned feature representations are biased by complexity, learning order, position, and more},
	url = {http://arxiv.org/abs/2405.05847},
	doi = {10.48550/arXiv.2405.05847},
	abstract = {Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience. Both fields generally use representations as a means to understand or improve a system's computations. In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts. We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data. We train various deep learning architectures to compute these multiple abstract features about their inputs. We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs. For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well. We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly). Our results help to characterize the inductive biases of gradient-based representation learning. These results also highlight a key challenge for interpretability \$-\$ or for comparing the representations of models and brains \$-\$ disentangling extraneous biases from the computationally important aspects of a system's internal representations.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Lampinen, Andrew Kyle and Chan, Stephanie C. Y. and Hermann, Katherine},
	month = jun,
	year = {2024},
	note = {arXiv:2405.05847 [cs]},
}

@misc{huh_platonic_2024,
	title = {The {Platonic} {Representation} {Hypothesis}},
	url = {http://arxiv.org/abs/2405.07987},
	doi = {10.48550/arXiv.2405.07987},
	abstract = {We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
	month = may,
	year = {2024},
	note = {arXiv:2405.07987 [cs]},
}

@misc{kim_code_2024,
	title = {Code {Pretraining} {Improves} {Entity} {Tracking} {Abilities} of {Language} {Models}},
	url = {http://arxiv.org/abs/2405.21068},
	doi = {10.48550/arXiv.2405.21068},
	abstract = {Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language. In this work, we systematically test this claim by comparing pairs of language models on their entity tracking performance. Critically, the pairs consist of base models and models trained on top of these base models with additional code data. We extend this analysis to additionally examine the effect of math training, another highly structured data type, and alignment tuning, an important step for enhancing the usability of models. We find clear evidence that models additionally trained on large amounts of code outperform the base models. On the other hand, we find no consistent benefit of additional math training or alignment tuning across various model families.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Kim, Najoung and Schuster, Sebastian and Toshniwal, Shubham},
	month = may,
	year = {2024},
	note = {arXiv:2405.21068 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{li_transformer_2024-1,
	address = {Mexico City, Mexico},
	title = {A {Transformer} with {Stack} {Attention}},
	url = {https://aclanthology.org/2024.findings-naacl.269},
	abstract = {Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-basedattention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-freelanguages.},
	urldate = {2024-06-23},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Li, Jiaoda and White, Jennifer and Sachan, Mrinmaya and Cotterell, Ryan},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	keywords = {Computer Science - Computation and Language},
	pages = {4318--4335},
}

@inproceedings{shen_ordered_2019,
	title = {Ordered {Memory}},
	volume = {32},
	url = {https://papers.nips.cc/paper_files/paper/2019/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html},
	abstract = {Stack-augmented recurrent neural networks (RNNs)  have been of interest to the deep learning community for some time. However, the difficulty of training memory models remains a problem obstructing the widespread use of such models. In this paper, we propose the Ordered Memory architecture. Inspired by Ordered Neurons (Shen et al., 2018), we introduce a new attention-based mechanism and use its cumulative probability to control the writing and erasing operation of the memory. We also introduce a new Gated Recursive Cell to compose lower-level representations into higher-level representation. We demonstrate that our model achieves strong performance on the logical inference task (Bowman et al., 2015) and the ListOps (Nangia and Bowman, 2018) task. We can also interpret the model to retrieve the induced tree structure, and find that these induced structures align with the ground truth. Finally, we evaluate our model on the Stanford Sentiment Treebank tasks (Socher et al., 2013), and find that it performs comparatively with the state-of-the-art methods in the literature.},
	urldate = {2024-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shen, Yikang and Tan, Shawn and Hosseini, Arian and Lin, Zhouhan and Sordoni, Alessandro and Courville, Aaron C},
	year = {2019},
}

@inproceedings{hu_systematic_2020,
	address = {Online},
	title = {A {Systematic} {Assessment} of {Syntactic} {Generalization} in {Neural} {Language} {Models}},
	url = {https://aclanthology.org/2020.acl-main.158},
	doi = {10.18653/v1/2020.acl-main.158},
	abstract = {While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.},
	urldate = {2024-06-23},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {1725--1744},
}

@article{warstadt_blimp_2020,
	title = {{BLiMP}: {The} {Benchmark} of {Linguistic} {Minimal} {Pairs} for {English}},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {{BLiMP}},
	url = {https://doi.org/10.1162/tacl_a_00321},
	doi = {10.1162/tacl_a_00321},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
	urldate = {2024-06-23},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	month = jul,
	year = {2020},
	pages = {377--392},
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@inproceedings{lee_set_2019,
	title = {Set {Transformer}: {A} {Framework} for {Attention}-based {Permutation}-{Invariant} {Neural} {Networks}},
	shorttitle = {Set {Transformer}},
	url = {https://proceedings.mlr.press/v97/lee19d.html},
	abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
	language = {en},
	urldate = {2024-06-19},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3744--3753},
}

@inproceedings{alon_bottleneck_2020,
	title = {On the {Bottleneck} of {Graph} {Neural} {Networks} and its {Practical} {Implications}},
	url = {https://openreview.net/forum?id=i80OPhOCVH2},
	abstract = {Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .},
	language = {en},
	urldate = {2024-06-19},
	author = {Alon, Uri and Yahav, Eran},
	month = oct,
	year = {2020},
}

@inproceedings{petty_impact_2024,
	address = {Mexico City, Mexico},
	title = {The {Impact} of {Depth} on {Compositional} {Generalization} in {Transformer} {Language} {Models}},
	url = {https://aclanthology.org/2024.naacl-long.402},
	abstract = {To process novel sentences, language models (LMs) must generalize compositionally—combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by theoretical and empirical work, that deeper transformers generalize more compositionally. Simply adding layers increases the total number of parameters; to address this confound between depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize more compositionally than shallower models do, but the benefit of additional layers diminishes rapidly; (2) within each family, deeper models show better language modeling performance, but returns are similarly diminishing; (3) the benefits of depth for compositional generalization cannot be attributed solely to better performance on language modeling. Because model latency is approximately linear in the number of layers, these results lead us to the recommendation that, with a given total parameter budget, transformers can be made shallower than is typical without sacrificing performance.},
	urldate = {2024-06-19},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Petty, Jackson and Steenkiste, Sjoerd and Dasgupta, Ishita and Sha, Fei and Garrette, Dan and Linzen, Tal},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {7232--7245},
}

@inproceedings{csordas_approximating_2023,
	address = {Singapore},
	title = {Approximating {Two}-{Layer} {Feedforward} {Networks} for {Efficient} {Transformers}},
	url = {https://aclanthology.org/2023.findings-emnlp.49},
	doi = {10.18653/v1/2023.findings-emnlp.49},
	abstract = {How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that *unifies* various methods to *approximate two-layer NNs* (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the *compute-equal* condition, our evaluation condition is *parameter-equal*, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the *dense* Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.},
	urldate = {2024-06-12},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Csordás, Róbert and Irie, Kazuki and Schmidhuber, Jürgen},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {674--692},
}

@misc{csordas_switchhead_2023,
	title = {{SwitchHead}: {Accelerating} {Transformers} with {Mixture}-of-{Experts} {Attention}},
	shorttitle = {{SwitchHead}},
	url = {http://arxiv.org/abs/2312.07987},
	doi = {10.48550/arXiv.2312.07987},
	abstract = {The costly self-attention layers in modern Transformers require memory and compute quadratic in sequence length. Existing approximation methods usually underperform and fail to obtain significant speedups in practice. Here we present SwitchHead - a novel method that reduces both compute and memory requirements and achieves wall-clock speedup, while matching the language modeling performance of baseline Transformers with the same parameter budget. SwitchHead uses Mixture-of-Experts (MoE) layers for the value and output projections and requires 4 to 8 times fewer attention matrices than standard Transformers. Our novel attention can also be combined with MoE MLP layers, resulting in an efficient fully-MoE "SwitchAll" Transformer model. Our code is public.},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Csordás, Róbert and Piękos, Piotr and Irie, Kazuki and Schmidhuber, Jürgen},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07987 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{csordas_moeut_2024,
	title = {{MoEUT}: {Mixture}-of-{Experts} {Universal} {Transformers}},
	shorttitle = {{MoEUT}},
	url = {http://arxiv.org/abs/2405.16039},
	doi = {10.48550/arXiv.2405.16039},
	abstract = {Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio: it drastically reduces the parameter count compared to the non-shared model with the same dimensionality. Naively scaling up the layer size to compensate for the loss of parameters makes its computational resource requirements prohibitive. In practice, no previous work has succeeded in proposing a shared-layer Transformer design that is competitive in parameter count-dominated tasks such as language modeling. Here we propose MoEUT (pronounced "moot"), an effective mixture-of-experts (MoE)-based shared-layer Transformer architecture, which combines several recent advances in MoEs for both feedforward and attention layers of standard Transformers together with novel layer-normalization and grouping schemes that are specific and crucial to UTs. The resulting UT model, for the first time, slightly outperforms standard Transformers on language modeling tasks such as BLiMP and PIQA, while using significantly less compute and memory.},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Csordás, Róbert and Irie, Kazuki and Schmidhuber, Jürgen and Potts, Christopher and Manning, Christopher D.},
	month = may,
	year = {2024},
	note = {arXiv:2405.16039 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{thomm_terminating_nodate,
	title = {Terminating {Differentiable} {Tree} {Experts}},
	author = {Thomm, J and Rahimi, Abbas},
}

@misc{irie_practical_2023,
	title = {Practical {Computational} {Power} of {Linear} {Transformers} and {Their} {Recurrent} and {Self}-{Referential} {Extensions}},
	url = {http://arxiv.org/abs/2310.16076},
	doi = {10.48550/arXiv.2310.16076},
	abstract = {Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We show that many well-known results for the standard Transformer directly transfer to LTs/FWPs. Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT, e.g., allowing for generalisation on the parity problem. Our code is public.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Irie, Kazuki and Csordás, Róbert and Schmidhuber, Jürgen},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16076 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{fan_advancing_2024-1,
	title = {Advancing {Regular} {Language} {Reasoning} in {Linear} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2309.07412},
	doi = {10.48550/arXiv.2309.07412},
	abstract = {In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language and long-range modeling, while offering rapid parallel training and constant inference cost. With the resurgence of interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations in modeling regular language. Motivated by this analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN capable of performing length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic. The code is released at {\textbackslash}url\{https://github.com/tinghanf/RegluarLRNN\}.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Fan, Ting-Han and Chi, Ta-Chung and Rudnicky, Alexander I.},
	month = apr,
	year = {2024},
	note = {arXiv:2309.07412 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{cotterell_formal_2024,
	title = {Formal {Aspects} of {Language} {Modeling}},
	url = {http://arxiv.org/abs/2311.04329},
	doi = {10.48550/arXiv.2311.04329},
	abstract = {Large language models have become one of the most commonly deployed NLP inventions. In the past half-decade, their integration into core natural language processing tools has dramatically increased the performance of such tools, and they have entered the public discourse surrounding artificial intelligence. Consequently, it is important for both developers and researchers alike to understand the mathematical foundations of large language models, as well as how to implement them. These notes are the accompaniment to the theoretical portion of the ETH Z{\textbackslash}"urich course on large language models, covering what constitutes a language model from a formal, theoretical perspective.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Cotterell, Ryan and Svete, Anej and Meister, Clara and Liu, Tianyu and Du, Li},
	month = apr,
	year = {2024},
	note = {arXiv:2311.04329 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{strobl_what_2024-1,
	title = {What {Formal} {Languages} {Can} {Transformers} {Express}? {A} {Survey}},
	volume = {12},
	issn = {2307-387X},
	shorttitle = {What {Formal} {Languages} {Can} {Transformers} {Express}?},
	url = {https://doi.org/10.1162/tacl_a_00663},
	doi = {10.1162/tacl_a_00663},
	abstract = {As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.},
	urldate = {2024-05-31},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
	month = may,
	year = {2024},
	pages = {543--561},
}

@inproceedings{bhattamishra_ability_2020,
	address = {Online},
	title = {On the {Ability} and {Limitations} of {Transformers} to {Recognize} {Formal} {Languages}},
	url = {https://aclanthology.org/2020.emnlp-main.576},
	doi = {10.18653/v1/2020.emnlp-main.576},
	abstract = {Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.},
	urldate = {2024-05-31},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {7096--7116},
}

@misc{chowdhury_investigating_2024,
	title = {Investigating {Recurrent} {Transformers} with {Dynamic} {Halt}},
	url = {http://arxiv.org/abs/2402.00976},
	doi = {10.48550/arXiv.2402.00976},
	abstract = {In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Chowdhury, Jishnu Ray and Caragea, Cornelia},
	month = mar,
	year = {2024},
	note = {arXiv:2402.00976 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{chi_transformer_2023,
	address = {Singapore},
	title = {Transformer {Working} {Memory} {Enables} {Regular} {Language} {Reasoning} {And} {Natural} {Language} {Length} {Extrapolation}},
	url = {https://aclanthology.org/2023.findings-emnlp.397},
	doi = {10.18653/v1/2023.findings-emnlp.397},
	abstract = {Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.},
	urldate = {2024-05-31},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Chi, Ta-Chung and Fan, Ting-Han and Rudnicky, Alexander and Ramadge, Peter},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {5972--5984},
}

@misc{hu_generative_2024,
	title = {Generative {Pretrained} {Structured} {Transformers}: {Unsupervised} {Syntactic} {Language} {Models} at {Scale}},
	shorttitle = {Generative {Pretrained} {Structured} {Transformers}},
	url = {http://arxiv.org/abs/2403.08293},
	doi = {10.48550/arXiv.2403.08293},
	abstract = {A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with \$9\$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering both language understanding and language generation. Meanwhile, GPST also significantly outperforms existing unsupervised SLMs on left-to-right grammar induction, while holding a substantial acceleration on training.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Hu, Xiang and Ji, Pengyu and Zhu, Qingyang and Wu, Wei and Tu, Kewei},
	month = mar,
	year = {2024},
	note = {arXiv:2403.08293 [cs]},
}

@article{merrill_finding_2019,
	title = {Finding {Hierarchical} {Structure} in {Neural} {Stacks} {Using} {Unsupervised} {Parsing}},
	url = {https://www.aclweb.org/anthology/W19-4823},
	doi = {10.18653/v1/W19-4823},
	abstract = {Neural network architectures have been augmented with differentiable stacks in order to introduce a bias toward learning hierarchy-sensitive regularities. It has, however, proven difficult to assess the degree to which such a bias is effective, as the operation of the differentiable stack is not always interpretable. In this paper, we attempt to detect the presence of latent representations of hierarchical structure through an exploration of the unsupervised learning of constituency structure. Using a technique due to Shen et al. (2018a,b), we extract syntactic trees from the pushing behavior of stack RNNs trained on language modeling and classification objectives. We find that our models produce parses that reflect natural language syntactic constituencies, demonstrating that stack RNNs do indeed infer linguistically relevant hierarchical structure.},
	language = {en},
	urldate = {2024-04-03},
	journal = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
	author = {Merrill, William and Khazan, Lenny and Amsel, Noah and Hao, Yiding and Mendelsohn, Simon and Frank, Robert},
	year = {2019},
	note = {Conference Name: Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP
Place: Florence, Italy
Publisher: Association for Computational Linguistics},
	pages = {224--232},
}

@inproceedings{
merrill2024the,
title={The Expressive Power of Transformers with Chain of Thought},
author={William Merrill and Ashish Sabharwal},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NjNGlPh8Wh}
}

@misc{franzluebbers_multipath_2024,
	title = {Multipath parsing in the brain},
	url = {http://arxiv.org/abs/2401.18046},
	doi = {10.48550/arXiv.2401.18046},
	abstract = {Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Franzluebbers, Berta and Dunagan, Donald and Stanojević, Miloš and Buys, Jan and Hale, John T.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.18046 [cs]},
}

@inproceedings{buys_neural_2018,
	address = {New Orleans, Louisiana},
	title = {Neural {Syntactic} {Generative} {Models} with {Exact} {Marginalization}},
	url = {https://aclanthology.org/N18-1086},
	doi = {10.18653/v1/N18-1086},
	abstract = {We present neural syntactic generative models with exact marginalization that support both dependency parsing and language modeling. Exact marginalization is made tractable through dynamic programming over shift-reduce parsing and minimal RNN-based feature sets. Our algorithms complement previous approaches by supporting batched training and enabling online computation of next word probabilities. For supervised dependency parsing, our model achieves a state-of-the-art result among generative approaches. We also report empirical results on unsupervised syntactic models and their role in language modeling. We find that our model formulation of latent dependencies with exact marginalization do not lead to better intrinsic language modeling performance than vanilla RNNs, and that parsing accuracy is not correlated with language modeling perplexity in stack-based models.},
	urldate = {2024-04-02},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Buys, Jan and Blunsom, Phil},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {942--952},
}

@misc{stern_effective_2017,
	title = {Effective {Inference} for {Generative} {Neural} {Parsing}},
	url = {http://arxiv.org/abs/1707.08976},
	doi = {10.48550/arXiv.1707.08976},
	abstract = {Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Stern, Mitchell and Fried, Daniel and Klein, Dan},
	month = jul,
	year = {2017},
	note = {arXiv:1707.08976 [cs]},
}

@inproceedings{choe_parsing_2016,
	address = {Austin, Texas},
	title = {Parsing as {Language} {Modeling}},
	url = {https://aclanthology.org/D16-1257},
	doi = {10.18653/v1/D16-1257},
	urldate = {2024-04-02},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Choe, Do Kook and Charniak, Eugene},
	editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
	month = nov,
	year = {2016},
	pages = {2331--2336},
}

@inproceedings{qian_structural_2021,
	address = {Online},
	title = {Structural {Guidance} for {Transformer} {Language} {Models}},
	url = {https://aclanthology.org/2021.acl-long.289},
	doi = {10.18653/v1/2021.acl-long.289},
	abstract = {Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The “Generative Parsing” idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The “Structural Scaffold” idea guides the language model's representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models' syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Qian, Peng and Naseem, Tahira and Levy, Roger and Fernandez Astudillo, Ramón},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {3735--3745},
}

@inproceedings{kim_sequence--sequence_2021,
	title = {Sequence-to-{Sequence} {Learning} with {Latent} {Neural} {Grammars}},
	url = {https://openreview.net/forum?id=0vaPiltED1N},
	abstract = {Sequence-to-sequence learning with neural networks has become the de facto standard for sequence modeling. This approach typically models the local distribution over the next element with a powerful neural network that can condition on arbitrary context. While flexible and performant, these models often require large datasets for training and can fail spectacularly on benchmarks designed to test for compositional generalization. This work explores an alternative, hierarchical approach to sequence-to-sequence learning with synchronous grammars, where each node in the target tree is transduced by a subset of nodes in the source tree. The source and target trees are treated as fully latent and marginalized out during training. We develop a neural parameterization of the grammar which enables parameter sharing over combinatorial structures without the need for manual feature engineering. We apply this latent neural grammar to various domains---a diagnostic language navigation task designed to test for compositional generalization (SCAN), style transfer, and small-scale machine translation---and find that it performs respectably compared to standard baselines.},
	language = {en},
	urldate = {2024-03-21},
	author = {Kim, Yoon},
	month = nov,
	year = {2021},
}

@inproceedings{shaw_compositional_2021,
	address = {Online},
	title = {Compositional {Generalization} and {Natural} {Language} {Variation}: {Can} a {Semantic} {Parsing} {Approach} {Handle} {Both}?},
	shorttitle = {Compositional {Generalization} and {Natural} {Language} {Variation}},
	url = {https://aclanthology.org/2021.acl-long.75},
	doi = {10.18653/v1/2021.acl-long.75},
	abstract = {Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization? To better assess this capability, we propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model. It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations. While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Shaw, Peter and Chang, Ming-Wei and Pasupat, Panupong and Toutanova, Kristina},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {922--938},
}

@article{charniak2000bllip,
  title={Bllip 1987-89 wsj corpus release 1},
  author={Charniak, Eugene and Blaheta, Don and Ge, Niyu and Hall, Keith and Hale, John and Johnson, Mark},
  journal={(No Title)},
  year={2000},
  publisher={Linguistic Data Consortium}
}

@article{marcus-etal-1993-building,
    title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
    author = "Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann",
    editor = "Hirschberg, Julia",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J93-2004/",
    pages = "313--330"
}

@inproceedings{sun_replication_2023,
	title = {A {Replication} {Study} of {Compositional} {Generalization} {Works} on {Semantic} {Parsing}},
	url = {https://openreview.net/forum?id=MF9uv95psps},
	abstract = {Reproducibility Summary Scope of Reproducibility — We examine the reproducibility of compositional generalization results from the task of semantic parsing. We aim to reproduce results from [1], [2], and [3] and seek to verify the claims that 1. A model shouldn't be expected to perform well on non-synthetic datasets just because it performs well on SCAN [1], 2. The approaches from [1] and [2] meet or exceed baseline performance on compositional generalization tests, and 3. NQG-T5 [1] outperforms baselines on both synthetic and natural data. 4. NQG [1] performs well on the instances that it is able to generate a prediction, but it faces the barrier of not being able to generate predictions for all instances. Methodology — We reuse the authors' code along with additional code to run extra experiments, and we re-implement scripts whose support is deprecated. Eight 32GB GPUs were used for experiments, with a detailed description in Section 3.3. Results — Claim 1 is verified: the model with the highest performance on SCAN does not maintain its high performance on other datasets (Section 4.1). Claims 2 and 3 are verified, with a comparison of performance between NQG-T5 and the selected baseline models in [1] and [2]. Claim 4 is also verified by computing the coverage and precision of NQG in Section 4.4. Overall, accuracy for most experiments reaches within 2\% of that reported in the original paper, with a deviation that our T5 achieves higher performance on some splits and slightly lower performance on one split than reported previously. What was easy — All papers provide clearly‐written code and informative documentation, as well as lists of hyperparameters that are used for experiments. The papers also describe their approaches clearly, making the experimental workflow easy to follow. What was difficult — The exact match evaluation metric is formulated somewhat differently across all three papers, leading to a non‐negligible value difference, as discussed in Section 5.2. We also had to re‐implement some training scripts because an original dependency is no longer supported. Moreover, some experiments are computationally expensive: [1] used TPUs for experiments, while our replication with GPUs take several days to train a single T5 model. Communication with original authors — The authors of all three papers provided us with useful instruction to work with their methods and constructive feedback on the draft.},
	language = {en},
	urldate = {2024-03-21},
	author = {Sun, Kaiser and Williams, Adina and Hupkes, Dieuwke},
	month = aug,
	year = {2023},
}

@inproceedings{yogatama_memory_2018,
	title = {Memory {Architectures} in {Recurrent} {Neural} {Network} {Language} {Models}},
	url = {https://openreview.net/forum?id=SkFqf0lAZ},
	abstract = {We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin \& Mikolov,2015; Grefenstette et al., 2015) to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.},
	language = {en},
	urldate = {2024-03-12},
	author = {Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil},
	month = feb,
	year = {2018},
}

@inproceedings{ruoss_randomized_2023,
	address = {Toronto, Canada},
	title = {Randomized {Positional} {Encodings} {Boost} {Length} {Generalization} of {Transformers}},
	url = {https://aclanthology.org/2023.acl-short.161},
	doi = {10.18653/v1/2023.acl-short.161},
	abstract = {Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence's length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0\% on average).},
	urldate = {2024-02-15},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ruoss, Anian and Delétang, Grégoire and Genewein, Tim and Grau-Moya, Jordi and Csordás, Róbert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {1889--1903},
}

@article{doi:10.1137/0202025,
author = {Greibach, Sheila A.},
title = {The Hardest Context-Free Language},
journal = {SIAM Journal on Computing},
volume = {2},
number = {4},
pages = {304-310},
year = {1973},
doi = {10.1137/0202025},

URL = { 
    
        https://doi.org/10.1137/0202025
    
    

},
eprint = { 
    
        https://doi.org/10.1137/0202025
    
    

}
,
    abstract = { There is a context-free language \$L\_0 \$ such that every context-free language is an inverse homomorphic image of \$L\_0 \$ or \$L\_0 - \{ e\} \$. Hence the time complexity of recognition of \$L\_0 \$ is the least upper bound for time complexity of recognition of context-free languages. A similar result holds for quasirealtime Turing machine languages. Several languages are given such that deterministic and nondeterministic polynomial time acceptance are equivalent if and only if any one of them is deterministic polynomial time acceptable. }
}



@misc{weisenhorn_compositional_2022,
	title = {Compositional {Generalization} {Requires} {Compositional} {Parsers}},
	url = {http://arxiv.org/abs/2202.11937},
	doi = {10.48550/arXiv.2202.11937},
	abstract = {A rapidly growing body of research on compositional generalization investigates the ability of a semantic parser to dynamically recombine linguistic elements seen in training into unseen sequences. We present a systematic comparison of sequence-to-sequence models and models guided by compositional principles on the recent COGS corpus (Kim and Linzen, 2020). Though seq2seq models can perform well on lexical tasks, they perform with near-zero accuracy on structural generalization tasks that require novel syntactic structures; this holds true even when they are trained to predict syntax instead of semantics. In contrast, compositional models achieve near-perfect accuracy on structural generalization; we present new results confirming this from the AM parser (Groschwitz et al., 2021). Our findings show structural generalization is a key measure of compositional generalization and requires models that are aware of complex structure.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Weißenhorn, Pia and Yao, Yuekun and Donatelli, Lucia and Koller, Alexander},
	month = feb,
	year = {2022},
	note = {arXiv:2202.11937 [cs]},
	keywords = {Compositionality, Parsing},
}

@article{tenenbaum2011grow,
  title={How to grow a mind: Statistics, structure, and abstraction},
  author={Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L and Goodman, Noah D},
  journal={science},
  volume={331},
  number={6022},
  pages={1279--1285},
  year={2011},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{kim_compound_2019,
	address = {Florence, Italy},
	title = {Compound {Probabilistic} {Context}-{Free} {Grammars} for {Grammar} {Induction}},
	url = {https://aclanthology.org/P19-1228},
	doi = {10.18653/v1/P19-1228},
	abstract = {We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.},
	urldate = {2024-02-09},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Yoon and Dyer, Chris and Rush, Alexander},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	keywords = {Parsing},
	pages = {2369--2385},
}

@inproceedings{liu_learning_2021,
	address = {Online},
	title = {Learning {Algebraic} {Recombination} for {Compositional} {Generalization}},
	url = {https://aclanthology.org/2021.findings-acl.97},
	doi = {10.18653/v1/2021.findings-acl.97},
	urldate = {2024-02-09},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Chenyao and An, Shengnan and Lin, Zeqi and Liu, Qian and Chen, Bei and Lou, Jian-Guang and Wen, Lijie and Zheng, Nanning and Zhang, Dongmei},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {1129--1144},
}

@article{lake_human-like_2023,
	title = {Human-like systematic generalization through a meta-learning neural network},
	volume = {623},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06668-3},
	doi = {10.1038/s41586-023-06668-3},
	abstract = {The power of human language and thought arises from systematic compositionality—the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn1 famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn’s challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.},
	language = {en},
	number = {7985},
	urldate = {2023-11-07},
	journal = {Nature},
	author = {Lake, Brenden M. and Baroni, Marco},
	month = nov,
	year = {2023},
	note = {Number: 7985
Publisher: Nature Publishing Group},
	keywords = {Compositionality},
	pages = {115--121},
}

@inproceedings{menet_mimonets_2023,
	title = {{MIMONets}: {Multiple}-{Input}-{Multiple}-{Output} {Neural} {Networks} {Exploiting} {Computation} in {Superposition}},
	shorttitle = {{MIMONets}},
	url = {https://openreview.net/forum?id=ox7aynitoW},
	abstract = {With the advent of deep learning, progressively larger neural networks have been designed to solve complex tasks. We take advantage of these capacity-rich models to lower the cost of inference by exploiting computation in superposition. To reduce the computational burden per input, we propose Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling many inputs at once. MIMONets augment various deep neural network architectures with variable binding mechanisms to represent an arbitrary number of inputs in a compositional data structure via fixed-width distributed representations. Accordingly, MIMONets adapt nonlinear neural transformations to process the data structure holistically, leading to a speedup nearly proportional to the number of superposed input items in the data structure. After processing in superposition, an unbinding mechanism recovers each transformed input of interest. MIMONets also provide a dynamic trade-off between accuracy and throughput by an instantaneous on-demand switching between a set of accuracy-throughput operating points, yet within a single set of fixed parameters. We apply the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical evaluations show that MIMOConv achieves \${\textbackslash}approx 2\$–\$4{\textbackslash}times\$ speedup at an accuracy delta within [+0.68, -3.18]\% compared to WideResNet CNNs on CIFAR10 and CIFAR100. Similarly, MIMOFormer can handle \$2\$–\$4\$ inputs at once while maintaining a high average accuracy within a [-1.07, -3.43]\% delta on the long range arena benchmark. Finally, we provide mathematical bounds on the interference between superposition channels in MIMOFormer. Our code is available at https://github.com/IBM/multiple-input-multiple-output-nets.},
	language = {en},
	urldate = {2023-11-21},
	author = {Menet, Nicolas and Hersche, Michael and Karunaratne, Geethan and Benini, Luca and Sebastian, Abu and Rahimi, Abbas},
	month = nov,
	year = {2023},
	keywords = {VSA},
}

@inproceedings{csordas_devil_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {The {Devil} is in the {Detail}: {Simple} {Tricks} {Improve} {Systematic} {Generalization} of {Transformers}},
	shorttitle = {The {Devil} is in the {Detail}},
	url = {https://aclanthology.org/2021.emnlp-main.49},
	doi = {10.18653/v1/2021.emnlp-main.49},
	abstract = {Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50\% to 85\% on the PCFG productivity split, and from 35\% to 81\% on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100\% accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results.},
	urldate = {2024-01-09},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Csordás, Róbert and Irie, Kazuki and Schmidhuber, Juergen},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	keywords = {Compositionality, Transformer},
	pages = {619--634},
}

@inproceedings{patel_revisiting_2022,
	address = {Dublin, Ireland},
	title = {Revisiting the {Compositional} {Generalization} {Abilities} of {Neural} {Sequence} {Models}},
	url = {https://aclanthology.org/2022.acl-short.46},
	doi = {10.18653/v1/2022.acl-short.46},
	abstract = {Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future.},
	urldate = {2024-01-10},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Patel, Arkil and Bhattamishra, Satwik and Blunsom, Phil and Goyal, Navin},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	keywords = {Compositionality},
	pages = {424--434},
}

@misc{chen_compositional_2020,
	title = {Compositional {Generalization} via {Neural}-{Symbolic} {Stack} {Machines}},
	url = {http://arxiv.org/abs/2008.06662},
	doi = {10.48550/arXiv.2008.06662},
	abstract = {Despite achieving tremendous success, existing deep learning models have exposed limitations in compositional generalization, the capability to learn compositional rules and apply them to unseen cases in a systematic manner. To tackle this issue, we propose the Neural-Symbolic Stack Machine (NeSS). It contains a neural network to generate traces, which are then executed by a symbolic stack machine enhanced with sequence manipulation operations. NeSS combines the expressive power of neural sequence models with the recursion supported by the symbolic stack machine. Without training supervision on execution traces, NeSS achieves 100\% generalization performance in four domains: the SCAN benchmark of language-driven navigation tasks, the task of few-shot learning of compositional instructions, the compositional machine translation benchmark, and context-free grammar parsing tasks.},
	urldate = {2024-01-26},
	publisher = {arXiv},
	author = {Chen, Xinyun and Liang, Chen and Yu, Adams Wei and Song, Dawn and Zhou, Denny},
	month = oct,
	year = {2020},
	note = {arXiv:2008.06662 [cs, stat]},
	keywords = {Compositionality, Differentiable Computing},
}

@inproceedings{newman_eos_2020,
	address = {Online},
	title = {The {EOS} {Decision} and {Length} {Extrapolation}},
	url = {https://aclanthology.org/2020.blackboxnlp-1.26},
	doi = {10.18653/v1/2020.blackboxnlp-1.26},
	abstract = {Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process through the use of a special end-of-sequence (EOS) vocabulary item. We study an oracle setting - forcing models to generate to the correct sequence length at test time - to compare the length-extrapolative behavior of networks trained to predict EOS (+EOS) with networks not trained to (-EOS). We find that -EOS substantially outperforms +EOS, for example extrapolating well to lengths 10 times longer than those seen at training time in a bracket closing task, as well as achieving a 40\% improvement over +EOS in the difficult SCAN dataset length generalization task. By comparing the hidden states and dynamics of -EOS and +EOS models, we observe that +EOS models fail to generalize because they (1) unnecessarily stratify their hidden states by their linear position is a sequence (structures we call length manifolds) or (2) get stuck in clusters (which we refer to as length attractors) once the EOS token is the highest-probability prediction.},
	urldate = {2024-01-25},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Newman, Benjamin and Hewitt, John and Liang, Percy and Manning, Christopher D.},
	editor = {Alishahi, Afra and Belinkov, Yonatan and Chrupa{\textbackslash}la, Grzegorz and Hupkes, Dieuwke and Pinter, Yuval and Sajjad, Hassan},
	month = nov,
	year = {2020},
	keywords = {Compositionality},
	pages = {276--291},
}

@inproceedings{murty_pushdown_2023,
	address = {Singapore},
	title = {Pushdown {Layers}: {Encoding} {Recursive} {Structure} in {Transformer} {Language} {Models}},
	shorttitle = {Pushdown {Layers}},
	url = {https://aclanthology.org/2023.emnlp-main.195},
	doi = {10.18653/v1/2023.emnlp-main.195},
	abstract = {Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix. Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens—for instance, learning to “skip” over closed constituents. When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities. Pushdown Layers are a drop-in replacement for standard self-attention. We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks.},
	urldate = {2024-01-04},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Murty, Shikhar and Sharma, Pratyusha and Andreas, Jacob and Manning, Christopher},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	keywords = {Transformer, Tree},
	pages = {3233--3247},
}

@inproceedings{freivalds_neural_2019,
	title = {Neural {Shuffle}-{Exchange} {Networks} - {Sequence} {Processing} in {O}(n log n) {Time}},
	volume = {32},
	url = {https://papers.neurips.cc/paper_files/paper/2019/hash/9001ca429212011f4a4fda6c778cc318-Abstract.html},
	abstract = {A key requirement in sequence to sequence processing is the modeling of long range dependencies. To this end, a vast majority of the state-of-the-art models use attention mechanism which is of O(n{\textasciicircum}2) complexity that leads to slow execution for long sequences.},
	urldate = {2024-01-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Freivalds, Karlis and Ozoliņš, Emīls and Šostaks, Agris},
	year = {2019},
	keywords = {Differentiable Computing},
}

@article{furrer_compositional_2020,
	title = {Compositional {Generalization} in {Semantic} {Parsing}: {Pre}-training vs. {Specialized} {Architectures}},
	url = {http://arxiv.org/abs/2007.08970},
	abstract = {While mainstream machine learning methods are known to have limited ability to compositionally generalize, new architectures and techniques continue to be proposed to address this limitation. We investigate state-of-the-art techniques and architectures in order to assess their effectiveness in improving compositional generalization in semantic parsing tasks based on the SCAN and CFQ datasets. We show that masked language model (MLM) pre-training rivals SCAN-inspired architectures on primitive holdout splits. On a more complex compositional task, we show that pre-training leads to significant improvements in performance vs. comparable non-pre-trained models, whereas architectures proposed to encourage compositional generalization on SCAN or in the area of algorithm learning fail to lead to significant improvements. We establish a new state of the art on the CFQ compositional generalization benchmark using MLM pre-training together with an intermediate representation.},
	author = {Furrer, Daniel and van Zee, Marc and Scales, Nathan and Schärli, Nathanael},
	year = {2020},
	note = {arXiv: 2007.08970},
	keywords = {Compositionality},
}

@inproceedings{noauthor_attention-based_2023,
	title = {Attention-based {Iterative} {Decomposition} for {Tensor} {Product} {Representation}},
	url = {https://openreview.net/forum?id=FDb2JQZsFH},
	abstract = {In recent research, Tensor Product Representation (TPR) is applied for the systematic generalization task of deep neural networks by learning the compositional structure of data. However, such prior works show limited performance in discovering and representing the symbolic structure from unseen test data because of the incomplete bindings to the structural representations. In this work, we propose an Attention-based Iterative Decomposition (AID) module that can effectively improve the binding for the structured representations encoded from the sequential input features with TPR. Our AID can be easily adapted to any TPR-based model and provides enhanced systematic decomposition through a competitive attention mechanism between input features and structured representations. In our experiments, AID shows effectiveness by significantly improving the performance of TPR-based prior works on the series of systematic generalization tasks. Moreover, in the quantitative and qualitative evaluations, AID produces more compositional and well-bound structural representations than other works.},
	language = {en},
	urldate = {2024-01-04},
	month = oct,
	year = {2023},
}

@misc{soulos_disentangled_2023,
	title = {Disentangled deep generative models reveal coding principles of the human face processing network},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.02.15.528489v2},
	doi = {10.1101/2023.02.15.528489},
	abstract = {Despite decades of research, much is still unknown about the computations carried out in the human face processing network. Recently deep networks have been proposed as a computational account of human visual processing, but while they provide a good match to neural data throughout visual cortex, they lack interpretability. We introduce a method for interpreting brain activity using a new class of deep generative models, disentangled representation learning models, which learn a low-dimensional latent space that “disentangles” different semantically meaningful dimensions of faces, such as rotation, lighting, or hairstyle, in an unsupervised manner by enforcing statistical independence between dimensions. We find that the majority of our model’s learned latent dimensions are interpretable by human raters. Further, these latent dimensions serve as a good encoding model for human fMRI data. We next investigated the representation of different latent dimensions across face-selective voxels. We find a gradient from low- to high-level face feature representations along posterior to anterior face-selective regions, corroborating prior models of human face recognition. Interestingly, though, we find no spatial segregation between identity-relevant and irrelevant face features. Finally, we provide new insight into the few “entangled” (uninterpretable) dimensions in our model by showing that they match responses across the ventral stream and carry significant information about facial identity. Disentangled face encoding models provide an exciting alternative to standard “black box” deep learning approaches for modeling and interpreting human brain data.},
	language = {en},
	urldate = {2023-11-02},
	publisher = {bioRxiv},
	author = {Soulos, Paul and Isik, Leyla},
	month = mar,
	year = {2023},
	note = {Pages: 2023.02.15.528489
Section: New Results},
}

@misc{madaan_self-refine_2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	doi = {10.48550/arXiv.2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2023-10-23},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
}

@inproceedings{welleck_generating_2022,
	title = {Generating {Sequences} by {Learning} to {Self}-{Correct}},
	url = {https://openreview.net/forum?id=hH36JeQZDaO},
	abstract = {Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.},
	language = {en},
	urldate = {2023-10-23},
	author = {Welleck, Sean and Lu, Ximing and West, Peter and Brahman, Faeze and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin},
	month = sep,
	year = {2022},
}

@inproceedings{hu_augmenting_2023,
	title = {Augmenting transformers with recursively composed multi-grained representations},
	url = {https://www.semanticscholar.org/paper/Augmenting-transformers-with-recursively-composed-Hu-Zhu/6554e57df8595fec9392c03501ec1ec005f3c574?utm_source=alert_email&utm_content=LibraryFolder&utm_campaign=AlertEmails_WEEKLY&utm_term=AuthorPaper+LibraryFolder+AuthorCitation&email_index=4-0-4&utm_medium=22295770},
	abstract = {We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained with Transformers, making ReCAT enjoy scaling ability, strong performance, and interpretability at the same time. We conduct experiments on various sentence-level and span-level tasks. Evaluation results indicate that ReCAT can significantly outperform vanilla Transformer models on all span-level tasks and baselines that combine recursive networks with Transformers on natural language inference tasks. More interestingly, the hierarchical structures induced by ReCAT exhibit strong consistency with human-annotated syntactic trees, indicating good interpretability brought by the CIO layers.},
	urldate = {2023-10-10},
	author = {Hu, Xiang and Zhu, Qingyang and Tu, Kewei and Wu, Wei},
	month = sep,
	year = {2023},
}

@article{li_representations_2023,
	title = {Representations and {Computations} in {Transformers} that {Support} {Generalization} on {Structured} {Tasks}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=oFC2LAqS6Z},
	abstract = {Transformers have shown remarkable success in natural language processing and computer vision, serving as the foundation of large language and multimodal models. These networks can capture nuanced context sensitivity across high-dimensional language tokens or image pixels, but it remains unclear how highly structured behavior and systematic generalization can arise in these systems. Here, we explore the solution process a causal transformer discovers as it learns to solve a set of algorithmic tasks involving copying, sorting, and hierarchical compositions of these operations. We search for the minimal layer and head configuration sufficient to solve these tasks and unpack the roles of the attention heads, as well as how token representations are reweighted across layers to complement these roles. Our results provide new insights into how attention layers in transformers support structured computation within and across tasks: 1) Replacing fixed position labels with labels sampled from a larger set enables strong length generalization and faster learning. The learnable embeddings of these labels develop different representations, capturing sequence order if necessary, depending on task demand. 2) Two-layer transformers can learn reliable solutions to the multi-level problems we explore. The first layer tends to transform the input representation to allow the second layer to share computation across repeated components within a task or across related tasks. 3) We introduce an analysis pipeline that quantifies how the representation space in a given layer prioritizes different aspects of each item. We show that these representations prioritize information needed to guide attention relative to information that only requires downstream readout.},
	language = {en},
	urldate = {2023-10-03},
	journal = {Transactions on Machine Learning Research},
	author = {Li, Yuxuan and McClelland, James},
	month = jun,
	year = {2023},
}

@article{mcclelland_capturing_2022,
	title = {Capturing advanced human cognitive abilities with deep neural networks},
	volume = {26},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(22)00239-X},
	doi = {10.1016/j.tics.2022.09.018},
	language = {English},
	number = {12},
	urldate = {2023-09-29},
	journal = {Trends in Cognitive Sciences},
	author = {McClelland, James L.},
	month = dec,
	year = {2022},
	pmid = {36335015},
	note = {Publisher: Elsevier},
	pages = {1047--1050},
}

@article{firth_technique_1935,
	title = {The {Technique} of {Semantics}.},
	volume = {34},
	issn = {1467-968X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-968X.1935.tb01254.x},
	doi = {10.1111/j.1467-968X.1935.tb01254.x},
	language = {en},
	number = {1},
	urldate = {2023-09-28},
	journal = {Transactions of the Philological Society},
	author = {Firth, J. R.},
	year = {1935},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-968X.1935.tb01254.x},
	pages = {36--73},
}

@misc{dziri_faith_2023,
	title = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
	shorttitle = {Faith and {Fate}},
	url = {http://arxiv.org/abs/2305.18654},
	doi = {10.48550/arXiv.2305.18654},
	abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how Transformers' performance will rapidly decay with increased task complexity.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
	month = jun,
	year = {2023},
	note = {arXiv:2305.18654 [cs]},
}

@misc{hooker_hardware_2020,
	title = {The {Hardware} {Lottery}},
	url = {http://arxiv.org/abs/2009.06489},
	doi = {10.48550/arXiv.2009.06489},
	abstract = {Hardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which make it increasingly costly to stray off of the beaten path of research ideas. This essay posits that the gains from progress in computing are likely to become even more uneven, with certain research directions moving into the fast-lane while progress on others is further obstructed.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Hooker, Sara},
	month = sep,
	year = {2020},
	note = {arXiv:2009.06489 [cs]},
}

@article{Plate1995HolographicRR,
	title = {Holographic reduced representations},
	volume = {6 3},
	url = {https://api.semanticscholar.org/CorpusID:2352281},
	journal = {IEEE transactions on neural networks},
	author = {Plate, Tony A.},
	year = {1995},
	pages = {623--41},
}

@article{blank_what_2023,
	title = {What are large language models supposed to model?},
	volume = {0},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(23)00202-4},
	doi = {10.1016/j.tics.2023.08.006},
	language = {English},
	number = {0},
	urldate = {2023-09-13},
	journal = {Trends in Cognitive Sciences},
	author = {Blank, Idan A.},
	month = aug,
	year = {2023},
	pmid = {37659920},
	note = {Publisher: Elsevier},
}

@inproceedings{kitaev_learned_2022,
	address = {Dublin, Ireland},
	title = {Learned {Incremental} {Representations} for {Parsing}},
	url = {https://aclanthology.org/2022.acl-long.220},
	doi = {10.18653/v1/2022.acl-long.220},
	abstract = {We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence, where the label is predicted using strictly incremental processing of a prefix of the sentence, and the sequence of labels for a sentence fully determines a parse tree. Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input, in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses. Our learned representations achieve 93.72 F1 on the Penn Treebank with as few as 5 bits per word, and at 8 bits per word they achieve 94.97 F1, which is comparable with other state of the art parsing models when using the same pre-trained embeddings. We also provide an analysis of the representations learned by our system, investigating properties such as the interpretable syntactic features captured by the system and mechanisms for deferred resolution of syntactic ambiguities.},
	urldate = {2023-08-31},
	publisher = {Association for Computational Linguistics},
	author = {Kitaev, Nikita and Lu, Thomas and Klein, Dan},
	month = may,
	year = {2022},
	keywords = {Parsing},
	pages = {3086--3095},
}

@article{hebb_organization_nodate,
	title = {The {Organization} of {Behavior}},
	issn = {1098-6596},
	author = {Hebb, Donald},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9788578110796},
}

@article{landau_developmental_2019,
	title = {{DEVELOPMENTAL} {COGNITIVE} {NEUROSCIENCE} {Syllabus}},
	issn = {1098-6596},
	author = {Landau, Barbara},
	year = {2019},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9788578110796},
}

@article{searle_minds_2019,
	title = {Minds , {Brains}, and {Programs}},
	volume = {53},
	issn = {1098-6596},
	number = {9},
	author = {Searle, John},
	year = {2019},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9788578110796},
	pages = {1689--1699},
}

@article{lashley_search_2007,
	title = {In search of the engram},
	volume = {13},
	issn = {1548-9213},
	number = {1},
	journal = {Physiology (Bethesda, Md.)},
	author = {Lashley, K},
	year = {2007},
	pmid = {17911210},
	note = {ISBN: 1314362704},
	pages = {279--306},
}

@article{turing_computing_2007,
	title = {{COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}},
	volume = {13},
	issn = {1548-9213},
	abstract = {Inferences about how the complex somatosensory systems of anthropoid primates evolved are based on comparative studies of such systems in extant mammals. Experimental studies of members of the major clades of extant mammals suggest that somatosensory cortex of early mammals consisted of only a few areas, including a primary area, S1, bordered by strip-like rostral and caudal somatosensory fields, SR and SC. In addition, the second somatosensory area, S2, and the parietal ventral area, PV, were probably present. S1, S2, and PV were activated independently via parallel projections from the ventroposterior nucleus, VP. Little posterior parietal cortex existed, and it was unlikely that a separate primary motor area, M1, existed until placental mammals evolved. Early primates retained this basic organization and also had a larger posterior parietal region that mediated sensorimotor functions via connections with motor and premotor areas. The frontal cortex included M1, dorsal and ventral premotor areas, supplementary motor area, and cingulate motor fields. Ventroposterior superior and ventroposterior inferior nuclei were distinct from the ventroposterior nucleus in the thalamus. In early anthropoid primates, areas S1, SR, and SC had differentiated into the fields now recognized as areas 3b, 3a, and 1. Areas 3b and 1 contained parallel mirror-image representations of cutaneous receptors and a parallel representation in area 2 was probable. Serial processing became dominant, so that neurons in areas 1, S2, and PV became dependent on area 3b for activation. Posterior parietal cortex expanded into more areas that related to frontal cortex. Less is known about changes that might have occurred with the emergence of apes and humans, but their brains were larger and posed scaling problems most likely solved by increasing the number of cortical areas and reducing the proportion of long connections.},
	number = {1},
	journal = {Physiology (Bethesda, Md.)},
	author = {Turing, Alan},
	year = {2007},
	pmid = {17911210},
	note = {ISBN: 1314362704},
	pages = {279--306},
}

@article{newport_revisiting_nodate,
	title = {Revisiting {Lenneberg}’s {Hypotheses} {About} {Early} {Developmental} {Plasticity}: {Language} {Organization} {After} {Left}-{Hemisphere} {Perinatal} {Stroke}},
	author = {Newport, Elissa and Landau, Barbara and Seydell-Greenwald, Anna and Turkeltaub, Peter and Chambers, Catherine and Dromerick, Alexander and Carpenter, Jessica and Berl, Madison and Gaillard, William Davis},
	note = {ISBN: 2163684814},
}

@article{woalder_language_2017,
	title = {Language and {Thought} {Are} {Not} the same thing},
	volume = {176},
	number = {1},
	journal = {Physiology \& behavior},
	author = {{Woalder}},
	year = {2017},
	note = {ISBN: 2163684814},
	pages = {139--148},
}

@inproceedings{weiss_thinking_2021,
	title = {Thinking {Like} {Transformers}},
	url = {https://proceedings.mlr.press/v139/weiss21a.html},
	abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
	language = {en},
	urldate = {2023-08-24},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Differentiable Computing, Transformer},
	pages = {11080--11090},
}

@article{pollack_recursive_1990,
	title = {Recursive distributed representations},
	volume = {46},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/000437029090005K},
	doi = {10.1016/0004-3702(90)90005-K},
	abstract = {A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in f…},
	language = {en},
	number = {1-2},
	urldate = {2023-02-10},
	journal = {Artificial Intelligence},
	author = {Pollack, Jordan},
	month = nov,
	year = {1990},
	note = {Publisher: Elsevier},
	pages = {77--105},
}

@article{li_implicit_2021,
	title = {Implicit {Representations} of {Meaning} in {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2106.00737},
	abstract = {Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as models of entities and situations as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data. Code and data are available at https://github.com/belindal/state-probes .},
	urldate = {2021-07-29},
	journal = {arXiv:2106.00737 [cs]},
	author = {Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.00737},
}

@inproceedings{mccoy_right_2019,
	address = {Florence, Italy},
	title = {Right for the {Wrong} {Reasons}: {Diagnosing} {Syntactic} {Heuristics} in {Natural} {Language} {Inference}},
	shorttitle = {Right for the {Wrong} {Reasons}},
	url = {https://aclanthology.org/P19-1334},
	doi = {10.18653/v1/P19-1334},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.},
	urldate = {2021-12-21},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
	month = jul,
	year = {2019},
	pages = {3428--3448},
}

@article{dyer_recurrent_2016,
	title = {Recurrent neural network grammars},
	url = {http://arxiv.org/abs/1602.07776},
	doi = {10.18653/v1/n16-1024},
	abstract = {We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.},
	journal = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
	author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
	year = {2016},
	note = {arXiv: 1602.07776
ISBN: 9781941643914},
	pages = {199--209},
}

@article{goldberg_constructions_2003,
	title = {Constructions: a new theoretical approach to language},
	volume = {7},
	issn = {1364-6613},
	shorttitle = {Constructions},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661303000809},
	doi = {10.1016/S1364-6613(03)00080-9},
	abstract = {A new theoretical approach to language has emerged in the past 10–15 years that allows linguistic observations about form–meaning pairings, known as ‘constructions’, to be stated directly. Constructionist approaches aim to account for the full range of facts about language, without assuming that a particular subset of the data is part of a privileged ‘core’. Researchers in this field argue that unusual constructions shed light on more general issues, and can illuminate what is required for a complete account of language.},
	number = {5},
	urldate = {2023-08-29},
	journal = {Trends in Cognitive Sciences},
	author = {Goldberg, Adele E},
	month = may,
	year = {2003},
	keywords = {Linguistics},
	pages = {219--224},
}

@inproceedings{kim_cogs_2020,
	address = {Online},
	title = {{COGS}: {A} {Compositional} {Generalization} {Challenge} {Based} on {Semantic} {Interpretation}},
	shorttitle = {{COGS}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.731},
	doi = {10.18653/v1/2020.emnlp-main.731},
	abstract = {Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99\%), but generalization accuracy was substantially lower (16–35\%) and showed high sensitivity to random seed (+-6–8\%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Najoung and Linzen, Tal},
	month = nov,
	year = {2020},
	pages = {9087--9105},
}

@inproceedings{parkhi_deep_2015,
	title = {Deep face recognition},
	booktitle = {{BMVC} 2015 - {Proceedings} of the {British} {Machine} {Vision} {Conference} 2015},
	publisher = {British Machine Vision Association},
	author = {Parkhi, O and Vedaldi, A and Zisserman, A},
	year = {2015},
	pages = {1--12},
}

@article{popel_training_2018,
	title = {Training {Tips} for the {Transformer} {Model}},
	volume = {110},
	issn = {1804-0462},
	url = {http://arxiv.org/abs/1804.00247},
	doi = {10.2478/pralin-2018-0002},
	abstract = {This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra "more data and larger models", we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.},
	number = {1},
	urldate = {2023-02-07},
	journal = {The Prague Bulletin of Mathematical Linguistics},
	author = {Popel, Martin and Bojar, Ondřej},
	month = apr,
	year = {2018},
	note = {arXiv:1804.00247 [cs]},
	pages = {43--70},
}

@article{pater_generative_2019,
	title = {Generative linguistics and neural networks at 60: {Foundation}, friction, and fusion},
	shorttitle = {Generative linguistics and neural networks at 60},
	doi = {10.1353/LAN.2019.0009},
	abstract = {Abstract:The birthdate of both generative linguistics and neural networks can be taken as 1957, the year of the publication of foundational work by both Noam Chomsky and Frank Rosenblatt. This article traces the development of these two approaches to cognitive science, from their largely autonomous early development in the first thirty years, through their collision in the 1980s around the past-tense debate (Rumelhart \& McClelland 1986, Pinker \& Prince 1988) and their integration in much subsequent work up to the present. Although this integration has produced a considerable body of results, the continued general gulf between these two lines of research is likely impeding progress in both: on learning in generative linguistics, and on the representation of language in neural modeling. The article concludes with a brief argument that generative linguistics is unlikely to fulfill its promise of accounting for language learning if it continues to maintain its distance from neural and statistical approaches to learning.},
	journal = {Language},
	author = {Pater, Joe},
	year = {2019},
}

@inproceedings{akyurek_lexsym_2023,
	address = {Toronto, Canada},
	title = {{LexSym}: {Compositionality} as {Lexical} {Symmetry}},
	shorttitle = {{LexSym}},
	url = {https://aclanthology.org/2023.acl-long.38},
	doi = {10.18653/v1/2023.acl-long.38},
	abstract = {In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme — a procedure for transforming examples into other well-formed examples — that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LexSym that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LexSym can be deployed agnostically across text, structured data, and even images. It matches or surpasses state-of-the-art, task-specific models on COGS semantic parsing, SCAN and Alchemy instruction following, and CLEVR-CoGenT visual question answering datasets.},
	urldate = {2023-08-24},
	publisher = {Association for Computational Linguistics},
	author = {Akyurek, Ekin and Andreas, Jacob},
	month = jul,
	year = {2023},
	keywords = {Compositionality},
	pages = {639--657},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
	shorttitle = {Climbing towards {NLU}},
	url = {https://aclanthology.org/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We've Been and Where We're Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
	urldate = {2021-08-02},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	month = jul,
	year = {2020},
	pages = {5185--5198},
}

@misc{friedman_learning_2023,
	title = {Learning {Transformer} {Programs}},
	url = {http://arxiv.org/abs/2306.01128},
	doi = {10.48550/arXiv.2306.01128},
	abstract = {Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then be automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck-languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the ``circuits'' used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Friedman, Dan and Wettig, Alexander and Chen, Danqi},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01128 [cs]},
}

@misc{kim_uncontrolled_2022,
	title = {Uncontrolled {Lexical} {Exposure} {Leads} to {Overestimation} of {Compositional} {Generalization} in {Pretrained} {Models}},
	url = {http://arxiv.org/abs/2212.10769},
	doi = {10.48550/arXiv.2212.10769},
	abstract = {Human linguistic capacity is often characterized by compositionality and the generalization it enables -- human learners can produce and comprehend novel complex expressions by composing known parts. Several benchmarks exploit distributional control across training and test to gauge compositional generalization, where certain lexical items only occur in limited contexts during training. While recent work using these benchmarks suggests that pretrained models achieve impressive generalization performance, we argue that exposure to pretraining data may break the aforementioned distributional control. Using the COGS benchmark of Kim and Linzen (2020), we test two modified evaluation setups that control for this issue: (1) substituting context-controlled lexical items with novel character sequences, and (2) substituting them with special tokens represented by novel embeddings. We find that both of these setups lead to lower generalization performance in T5 (Raffel et al., 2020), suggesting that previously reported results have been overestimated due to uncontrolled lexical exposure during pretraining. The performance degradation is more extreme with novel embeddings, and the degradation increases with the amount of pretraining data, highlighting an interesting case of inverse scaling.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Kim, Najoung and Linzen, Tal and Smolensky, Paul},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10769 [cs]},
}

@inproceedings{qiu_evaluating_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Evaluating the {Impact} of {Model} {Scale} for {Compositional} {Generalization} in {Semantic} {Parsing}},
	url = {https://aclanthology.org/2022.emnlp-main.624},
	abstract = {Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve compositional generalization in semantic parsing? We evaluate encoder-decoder models up to 11B parameters and decoder-only models up to 540B parameters, and compare model scaling curves for three different methods for applying a pre-trained language model to a new task: fine-tuning all parameters, prompt tuning, and in-context learning. We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations. In-context learning has positive scaling curves, but is generally outperformed by much smaller fine-tuned models. Prompt-tuning can outperform fine-tuning, suggesting further potential improvements from scaling as it exhibits a more positive scaling curve. Additionally, we identify several error trends that vary with model scale. For example, larger models are generally better at modeling the syntax of the output space, but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work.},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Qiu, Linlu and Shaw, Peter and Pasupat, Panupong and Shi, Tianze and Herzig, Jonathan and Pitler, Emily and Sha, Fei and Toutanova, Kristina},
	month = dec,
	year = {2022},
	pages = {9157--9179},
}

@article{yang_one_2022,
	title = {One model for the learning of language},
	volume = {119},
	copyright = {Copyright © 2022 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/119/5/e2021865119},
	doi = {10.1073/pnas.2021865119},
	abstract = {A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g., an, (ab)n(ab)n{\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mo stretchy="false"{\textgreater}({\textless}/mml:mo{\textgreater}{\textless}mml:mi{\textgreater}a{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}b{\textless}/mml:mi{\textgreater}{\textless}mml:mo stretchy="false"{\textgreater}){\textless}/mml:mo{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}, and \{a,b\}+\{a,b\}+{\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mo{\textgreater}\{{\textless}/mml:mo{\textgreater}{\textless}mml:mi{\textgreater}a{\textless}/mml:mi{\textgreater}{\textless}mml:mo{\textgreater},{\textless}/mml:mo{\textgreater}{\textless}mml:mi{\textgreater}b{\textless}/mml:mi{\textgreater}{\textless}mml:mo{\textgreater}\}{\textless}/mml:mo{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}mml:mo{\textgreater}+{\textless}/mml:mo{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}), context-free (e.g., anbn,anbn+manbn, anbn+m{\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}a{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}b{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:mo{\textgreater},{\textless}/mml:mo{\textgreater}{\textless}mml:mo{\textgreater} {\textless}/mml:mo{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}a{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}b{\textless}/mml:mi{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}mml:mo{\textgreater}+{\textless}/mml:mo{\textgreater}{\textless}mml:mi{\textgreater}m{\textless}/mml:mi{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}, and xxRxxR{\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mi{\textgreater}x{\textless}/mml:mi{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}x{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}R{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}), and context-sensitive (e.g., anbncn,anbmcndmanbncn, anbmcndm{\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}a{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}b{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}c{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:mo{\textgreater},{\textless}/mml:mo{\textgreater}{\textless}mml:mo{\textgreater} {\textless}/mml:mo{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}a{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}b{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}m{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}c{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}n{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}mml:msup{\textgreater}{\textless}mml:mi{\textgreater}d{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}m{\textless}/mml:mi{\textgreater}{\textless}/mml:msup{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}, and xx) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
	language = {en},
	number = {5},
	urldate = {2022-02-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Yuan and Piantadosi, Steven T.},
	month = feb,
	year = {2022},
	pmid = {35074868},
	note = {Publisher: National Academy of Sciences
Section: Social Sciences},
}

@misc{tai_improved_2015,
	title = {Improved {Semantic} {Representations} {From} {Tree}-{Structured} {Long} {Short}-{Term} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1503.00075},
	doi = {10.48550/arXiv.1503.00075},
	abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	month = may,
	year = {2015},
	note = {arXiv:1503.00075 [cs]},
}

@misc{thrush_winoground_2022,
	title = {Winoground: {Probing} {Vision} and {Language} {Models} for {Visio}-{Linguistic} {Compositionality}},
	shorttitle = {Winoground},
	url = {http://arxiv.org/abs/2204.03162},
	doi = {10.48550/arXiv.2204.03162},
	abstract = {We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly - but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03162 [cs]},
}

@article{pinker_language_1988,
	title = {On language and connectionism: {Analysis} of a parallel distributed processing model of language acquisition},
	volume = {28},
	issn = {00100277},
	doi = {10.1016/0010-0277(88)90032-7},
	abstract = {Does knowledge of language consist of mentally-represented rules? Rumelhart and McClelland have described a connectionist (parallel distributed processing) model of the acquisition of the past tense in English which successfully maps many stems onto their past tense forms, both regular (walk/walked) and irregular (go/went), and which mimics some of the errors and sequences of development of children. Yet the model contains no explicit rules, only a set of neuronstyle units which stand for trigrams of phonetic features of the stem, a set of units which stand for trigrams of phonetic features of the past form, and an array of connections between the two sets of units whose strengths are modified during learning. Rumelhart and McClelland conclude that linguistic rules may be merely convenient approximate fictions and that the real causal processes in language use and acquisition must be characterized as the transfer of activation levels among units and the modification of the weights of their connections. We analyze both the linguistic and the developmental assumptions of the model in detail and discover that (1) it cannot represent certain words, (2) it cannot learn many rules, (3) it can learn rules found in no human language, (4) it cannot explain morphological and phonological regularities, (5) it cannot explain the differences between irregular and regular forms, (6) it fails at its assigned task of mastering the past tense of English, (7) it gives an incorrect explanation for two developmental phenomena: stages of overregularization of irregular forms such as bringed, and the appearance of doubly-marked forms such as ated and (8) it gives accounts of two others (infrequent overregularization of verbs ending in t/d, and the order of acquisition of different irregular subclasses) that are indistinguishable from those of rule-based theories. In addition, we show how many failures of the model can be attributed to its connectionist architecture. We conclude that connectionists' claims about the dispensability of rules in explanations in the psychology of language must be rejected, and that, on the contrary, the linguistic and developmental facts provide good evidence for such rules. © 1988.},
	number = {1-2},
	journal = {Cognition},
	author = {Pinker, Steven and Prince, Alan},
	year = {1988},
	pages = {73--193},
}

@misc{sutton_principles_2012,
	title = {Principles of {Research} {Code}},
	url = {https://www.theexclusive.org/2012/08/principles-of-research-code.html},
	abstract = {Ali Eslami has just writen a terrific page on organizing your experimental code and output. I pretty much agree with everything he says. I’ve thought quite a bit about this and would like to add some background. Programming for research is very different than programming for industry. There are several reasons for this, which I will call Principles of Research Code. These principles underly all of the advice in Ali’s post and in this post. These principles are:},
	urldate = {2023-08-30},
	journal = {The Exclusive Or},
	author = {Sutton, Charles},
	month = aug,
	year = {2012},
}

@inproceedings{he_deberta_2020,
	title = {{DeBERTa}: {Decoding}-enhanced {BERT} with {Disentangled} {Attention}},
	shorttitle = {{DEBERTA}},
	url = {https://openreview.net/forum?id=XPZIaotutsD},
	abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models’ generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand(NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus 89.8). The pre-trained DeBERTa models and the source code were released at: https://github.com/microsoft/DeBERTa.},
	language = {en},
	urldate = {2023-08-29},
	author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	month = oct,
	year = {2020},
	keywords = {Transformer},
}

@article{zador_catalyzing_2023,
	title = {Catalyzing next-generation {Artificial} {Intelligence} through {NeuroAI}},
	volume = {14},
	copyright = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-37180-x},
	doi = {10.1038/s41467-023-37180-x},
	abstract = {Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities – inherited from over 500 million years of evolution – that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.},
	language = {en},
	number = {1},
	urldate = {2023-08-29},
	journal = {Nature Communications},
	author = {Zador, Anthony and Escola, Sean and Richards, Blake and Ölveczky, Bence and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and DiCarlo, James and Ganguli, Surya and Hawkins, Jeff and Körding, Konrad and Koulakov, Alexei and LeCun, Yann and Lillicrap, Timothy and Marblestone, Adam and Olshausen, Bruno and Pouget, Alexandre and Savin, Cristina and Sejnowski, Terrence and Simoncelli, Eero and Solla, Sara and Sussillo, David and Tolias, Andreas S. and Tsao, Doris},
	month = mar,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Neuroscience},
	pages = {1597},
}

@article{goldberg_nature_2009,
	title = {The nature of generalization in language},
	volume = {20},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1613-3641},
	url = {https://www.degruyter.com/document/doi/10.1515/COGL.2009.005/html?lang=en},
	doi = {10.1515/COGL.2009.005},
	abstract = {This paper provides a concise overview of Constructions at Work (Goldberg 2006). The book aims to investigate the relevant levels of generalization in adult language, how and why generalizations are learned by children, and how to account for cross-linguistic generalizations.},
	language = {en},
	number = {1},
	urldate = {2023-08-29},
	author = {Goldberg, Adele E.},
	month = feb,
	year = {2009},
	note = {Publisher: De Gruyter Mouton
Section: Cognitive Linguistics},
	keywords = {Linguistics},
	pages = {93--127},
}

@misc{sachan_syntax_2021,
	title = {Do {Syntax} {Trees} {Help} {Pre}-trained {Transformers} {Extract} {Information}?},
	url = {http://arxiv.org/abs/2008.09084},
	doi = {10.48550/arXiv.2008.09084},
	abstract = {Much recent work suggests that incorporating syntax information from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these models implicitly encode syntax. In this work, we systematically study the utility of incorporating dependency trees into pre-trained transformers on three representative information extraction tasks: semantic role labeling (SRL), named entity recognition, and relation extraction. We propose and investigate two distinct strategies for incorporating dependency structure: a late fusion approach, which applies a graph neural network on the output of a transformer, and a joint fusion approach, which infuses syntax structure into the transformer attention layers. These strategies are representative of prior work, but we introduce additional model design elements that are necessary for obtaining improved performance. Our empirical analysis demonstrates that these syntax-infused transformers obtain state-of-the-art results on SRL and relation extraction tasks. However, our analysis also reveals a critical shortcoming of these models: we find that their performance gains are highly contingent on the availability of human-annotated dependency parses, which raises important questions regarding the viability of syntax-augmented transformers in real-world applications.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Sachan, Devendra Singh and Zhang, Yuhao and Qi, Peng and Hamilton, William},
	month = jan,
	year = {2021},
	note = {arXiv:2008.09084 [cs]},
	keywords = {Transformer, Tree},
}

@misc{deletang_neural_2023,
	title = {Neural {Networks} and the {Chomsky} {Hierarchy}},
	url = {http://arxiv.org/abs/2207.02098},
	doi = {10.48550/arXiv.2207.02098},
	abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (20'910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Delétang, Grégoire and Ruoss, Anian and Grau-Moya, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
	month = feb,
	year = {2023},
	note = {arXiv:2207.02098 [cs]},
}

@article{velickovic_neural_2021,
	title = {Neural {Algorithmic} {Reasoning}},
	volume = {2},
	issn = {26663899},
	url = {http://arxiv.org/abs/2105.02761},
	doi = {10.1016/j.patter.2021.100273},
	abstract = {Algorithms have been fundamental to recent global technological advances and, in particular, they have been the cornerstone of technical advances in one field rapidly being applied to another. We argue that algorithms possess fundamentally different qualities to deep learning methods, and this strongly suggests that, were deep learning methods better able to mimic algorithms, generalisation of the sort seen with algorithms would become possible with deep learning -- something far out of the reach of current machine learning methods. Furthermore, by representing elements in a continuous space of learnt algorithms, neural networks are able to adapt known algorithms more closely to real-world problems, potentially finding more efficient and pragmatic solutions than those proposed by human computer scientists. Here we present neural algorithmic reasoning -- the art of building neural networks that are able to execute algorithmic computation -- and provide our opinion on its transformative potential for running classical algorithms on inputs previously considered inaccessible to them.},
	number = {7},
	urldate = {2023-08-24},
	journal = {Patterns},
	author = {Veličković, Petar and Blundell, Charles},
	month = jul,
	year = {2021},
	note = {arXiv:2105.02761 [cs, math, stat]},
	pages = {100273},
}

@article{elman_finding_1990,
	title = {Finding structure in time},
	volume = {14},
	issn = {0364-0213},
	url = {https://www.sciencedirect.com/science/article/pii/036402139090002E},
	doi = {10.1016/0364-0213(90)90002-E},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
	number = {2},
	urldate = {2023-08-23},
	journal = {Cognitive Science},
	author = {Elman, Jeffrey L.},
	month = apr,
	year = {1990},
	pages = {179--211},
}

@article{manning_human_2022,
	title = {Human {Language} {Understanding} \& {Reasoning}},
	volume = {151},
	issn = {0011-5266},
	url = {https://doi.org/10.1162/daed_a_01905},
	doi = {10.1162/daed_a_01905},
	abstract = {The last decade has yielded dramatic and quite surprising breakthroughs in natural
language processing through the use of simple artificial neural network computations,
replicated on a very large scale and trained over exceedingly large amounts of data. The
resulting pretrained language models, such as BERT and GPT-3, have provided a powerful
universal language understanding and generation base, which can easily be adapted to many
understanding, writing, and reasoning tasks. These models show the first inklings of a
more general form of artificial intelligence, which may lead to powerful foundation models
in domains of sensory experience beyond just language.},
	number = {2},
	urldate = {2023-08-22},
	journal = {Daedalus},
	author = {Manning, Christopher D.},
	month = may,
	year = {2022},
	pages = {127--138},
}

@misc{hewitt_backpack_2023,
	title = {Backpack {Language} {Models}},
	url = {http://arxiv.org/abs/2305.16765},
	doi = {10.48550/arXiv.2305.16765},
	abstract = {We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model's behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM's word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Hewitt, John and Thickstun, John and Manning, Christopher D. and Liang, Percy},
	month = may,
	year = {2023},
	note = {arXiv:2305.16765 [cs]},
	keywords = {Transformer},
}

@article{hill_why_nodate,
	title = {Why transformers are obviously good models of language},
	abstract = {Nobody knows how language works, but many theories abound. Transformers are a class of neural network that process language automatically with more success than alternatives, both those based on neural computations and those that rely on other (e.g. more symbolic) mechanisms. Here, I highlight direct connections between the transformer architecture and certain theoretical perspectives on language. The empirical success of transformers relative to alternative models provides circumstantial evidence that the linguistic approaches that transformers embody should be, at least, evaluated with greater scrutiny by the linguistics community and, at best, considered to be the currently best available theories.},
	language = {en},
	author = {Hill, Felix},
	keywords = {Linguistics, Transformer},
}

@misc{qiu_graph_2023,
	title = {Graph {Embeddings} via {Tensor} {Products} and {Approximately} {Orthonormal} {Codes}},
	url = {http://arxiv.org/abs/2208.10917},
	doi = {10.48550/arXiv.2208.10917},
	abstract = {We propose a dynamic graph representation method, showcasing its rich representational capacity and establishing some of its theoretical properties. Our representation falls under the bind-and-sum approach in hyperdimensional computing (HDC), and we show that the tensor product is the most general binding operation that respects the superposition principle employed in HDC. We also establish some precise results characterizing the behavior of our method, including a memory vs. size analysis of how our representation's size must scale with the number of edges in order to retain accurate graph operations. True to its HDC roots, we also compare our graph representation to another typical HDC representation, the Hadamard-Rademacher scheme, showing that these two graph representations have the same memory-capacity scaling. We establish a link to adjacency matrices, showing that our method is a pseudo-orthogonal generalization of adjacency matrices. In light of this, we briefly discuss its applications toward a dynamic compressed representation of large sparse graphs.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Qiu, Frank},
	month = jun,
	year = {2023},
	note = {arXiv:2208.10917 [cs, stat]},
	keywords = {VSA},
}

@misc{frady_variable_2020,
	title = {Variable {Binding} for {Sparse} {Distributed} {Representations}: {Theory} and {Applications}},
	shorttitle = {Variable {Binding} for {Sparse} {Distributed} {Representations}},
	url = {http://arxiv.org/abs/2009.06734},
	doi = {10.48550/arXiv.2009.06734},
	abstract = {Symbolic reasoning and neural networks are often considered incompatible approaches. Connectionist models known as Vector Symbolic Architectures (VSAs) can potentially bridge this gap. However, classical VSAs and neural networks are still considered incompatible. VSAs encode symbols by dense pseudo-random vectors, where information is distributed throughout the entire neuron population. Neural networks encode features locally, often forming sparse vectors of neural activation. Following Rachkovskij (2001); Laiho et al. (2015), we explore symbolic reasoning with sparse distributed representations. The core operations in VSAs are dyadic operations between vectors to express variable binding and the representation of sets. Thus, algebraic manipulations enable VSAs to represent and process data structures in a vector space of fixed dimensionality. Using techniques from compressed sensing, we first show that variable binding between dense vectors in VSAs is mathematically equivalent to tensor product binding between sparse vectors, an operation which increases dimensionality. This result implies that dimensionality-preserving binding for general sparse vectors must include a reduction of the tensor matrix into a single sparse vector. Two options for sparsity-preserving variable binding are investigated. One binding method for general sparse vectors extends earlier proposals to reduce the tensor product into a vector, such as circular convolution. The other method is only defined for sparse block-codes, block-wise circular convolution. Our experiments reveal that variable binding for block-codes has ideal properties, whereas binding for general sparse vectors also works, but is lossy, similar to previous proposals. We demonstrate a VSA with sparse block-codes in example applications, cognitive reasoning and classification, and discuss its relevance for neuroscience and neural networks.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Frady, E. Paxon and Kleyko, Denis and Sommer, Friedrich T.},
	month = sep,
	year = {2020},
	note = {arXiv:2009.06734 [cs]},
	keywords = {VSA},
}

@inproceedings{sahlgren_permutations_2008,
	title = {Permutations as a means to encode order in word space},
	url = {https://www.semanticscholar.org/paper/Permutations-as-a-means-to-encode-order-in-word-Sahlgren-Holst/673b15746dd4ba0e70f038cf52f56dae1faeb744},
	abstract = {We show that sequence information can be encoded into high-dimensional fixed-width vectors using permutations of coordinates. Computational models of language often represent words with high-dimensional semantic vectors compiled from word-use statistics. A word's semantic vector usually encodes the contexts in which the word appears in a large body of text but ignores word order. However, word order often signals a word's grammatical role in a sentence and thus tells of the word's meaning. Jones and Mewhort (2007) show that word order can be included in the semantic vectors using holographic reduced representation and convolution. We show here that the order information can be captured also by permuting of vector coordinates, thus providing a general and computationally light alternative to convolution.},
	urldate = {2023-08-21},
	author = {Sahlgren, Magnus and Holst, Anders and Kanerva, P.},
	year = {2008},
	keywords = {VSA},
}

@misc{orvieto_resurrecting_2023,
	title = {Resurrecting {Recurrent} {Neural} {Networks} for {Long} {Sequences}},
	url = {http://arxiv.org/abs/2303.06349},
	doi = {10.48550/arXiv.2303.06349},
	abstract = {Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.},
	urldate = {2023-07-28},
	publisher = {arXiv},
	author = {Orvieto, Antonio and Smith, Samuel L. and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
	month = mar,
	year = {2023},
	note = {arXiv:2303.06349 [cs]},
}

@inproceedings{hu_r2d2_2021,
	address = {Online},
	title = {{R2D2}: {Recursive} {Transformer} based on {Differentiable} {Tree} for {Interpretable} {Hierarchical} {Language} {Modeling}},
	shorttitle = {{R2D2}},
	url = {https://aclanthology.org/2021.acl-long.379},
	doi = {10.18653/v1/2021.acl-long.379},
	abstract = {Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.},
	urldate = {2023-04-24},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Xiang and Mi, Haitao and Wen, Zujie and Wang, Yafang and Su, Yi and Zheng, Jing and de Melo, Gerard},
	month = aug,
	year = {2021},
	pages = {4897--4908},
}

@article{pavlick_symbols_2023,
	title = {Symbols and grounding in large language models},
	volume = {381},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2022.0041},
	doi = {10.1098/rsta.2022.0041},
	abstract = {Large language models (LLMs) are one of the most impressive achievements of artificial intelligence in recent years. However, their relevance to the study of language more broadly remains unclear. This article considers the potential of LLMs to serve as models of language understanding in humans. While debate on this question typically centres around models’ performance on challenging language understanding tasks, this article argues that the answer depends on models’ underlying competence, and thus that the focus of the debate should be on empirical work which seeks to characterize the representations and processing algorithms that underlie model behaviour. From this perspective, the article offers counterarguments to two commonly cited reasons why LLMs cannot serve as plausible models of language in humans: their lack of symbolic structure and their lack of grounding. For each, a case is made that recent empirical trends undermine the common assumptions about LLMs, and thus that it is premature to draw conclusions about LLMs’ ability (or lack thereof) to offer insights on human language representation and understanding.

This article is part of a discussion meeting issue ‘Cognitive artificial intelligence’.},
	number = {2251},
	urldate = {2023-07-27},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Pavlick, Ellie},
	month = jun,
	year = {2023},
	note = {Publisher: Royal Society},
	pages = {20220041},
}

@article{sanchez-lengeling_gentle_2021,
	title = {A {Gentle} {Introduction} to {Graph} {Neural} {Networks}},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/gnn-intro},
	doi = {10.23915/distill.00033},
	abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
	language = {en},
	number = {9},
	urldate = {2023-07-19},
	journal = {Distill},
	author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
	month = sep,
	year = {2021},
	pages = {e33},
}

@misc{piantadosi_modern_2023,
	title = {Modern language models refute {Chomsky}’s approach to language},
	url = {https://lingbuzz.net/lingbuzz/007180},
	abstract = {The rise and success of large language models undermines virtually every strong claim for the innateness of language that has been proposed by generative linguistics. Modern machine learning has subverted and bypassed the entire theoretical framework of Chomsky's approach, including its core claims to particular insights, principles, structures, and processes. I describe the sense in which modern language models implement genuine theories of language, including representations of syntactic and semantic structure. I highlight the relationship between contemporary models and prior approaches in linguistics, namely those based on gradient computations and memorized constructions. I also respond to several critiques of large language models, including claims that they can't answer ``why'' questions, and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable success at discovering grammar without using any of the methods that some in linguistics insisted were necessary for a science of language to progress.},
	urldate = {2023-07-18},
	publisher = {LingBuzz},
	author = {Piantadosi, Steven},
	month = mar,
	year = {2023},
	note = {LingBuzz Published In:},
}

@misc{alam_recasting_2023,
	title = {Recasting {Self}-{Attention} with {Holographic} {Reduced} {Representations}},
	url = {http://arxiv.org/abs/2305.19534},
	doi = {10.48550/arXiv.2305.19534},
	abstract = {In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the \${\textbackslash}mathcal\{O\}(T{\textasciicircum}2)\$ memory and \${\textbackslash}mathcal\{O\}(T{\textasciicircum}2 H)\$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of \$T {\textbackslash}geq 100,000\$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including \${\textbackslash}mathcal\{O\}(T H {\textbackslash}log H)\$ time complexity, \${\textbackslash}mathcal\{O\}(T H)\$ space complexity, and convergence in \$10{\textbackslash}times\$ fewer epochs. Nevertheless, the Hrrformer achieves near state-of-the-art accuracy on LRA benchmarks and we are able to learn with just a single layer. Combined, these benefits make our Hrrformer the first viable Transformer for such long malware classification sequences and up to \$280{\textbackslash}times\$ faster to train on the Long Range Arena benchmark. Code is available at {\textbackslash}url\{https://github.com/NeuromorphicComputationResearchProgram/Hrrformer\}},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Alam, Mohammad Mahmudul and Raff, Edward and Biderman, Stella and Oates, Tim and Holt, James},
	month = may,
	year = {2023},
	note = {arXiv:2305.19534 [cs, stat]},
}

@misc{whittington_disentanglement_2023,
	title = {Disentanglement with {Biological} {Constraints}: {A} {Theory} of {Functional} {Cell} {Types}},
	shorttitle = {Disentanglement with {Biological} {Constraints}},
	url = {http://arxiv.org/abs/2210.01768},
	doi = {10.48550/arXiv.2210.01768},
	abstract = {Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentanglement in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why single neurons in the brain often represent single human-interpretable factors, and steps towards an understanding task structure shapes the structure of brain representation.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Whittington, James C. R. and Dorrell, Will and Ganguli, Surya and Behrens, Timothy E. J.},
	month = mar,
	year = {2023},
	note = {arXiv:2210.01768 [cs, q-bio]},
}

@misc{mueller_coloring_2022,
	title = {Coloring the {Blank} {Slate}: {Pre}-training {Imparts} a {Hierarchical} {Inductive} {Bias} to {Sequence}-to-sequence {Models}},
	shorttitle = {Coloring the {Blank} {Slate}},
	url = {http://arxiv.org/abs/2203.09397},
	doi = {10.48550/arXiv.2203.09397},
	abstract = {Relations between words are governed by hierarchical structure rather than linear ordering. Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations - for example, transforming declarative sentences into questions. However, syntactic evaluations of seq2seq models have only observed models that were not pre-trained on natural language data before being trained to perform syntactic transformations, in spite of the fact that pre-training has been found to induce hierarchical linguistic generalizations in language models; in other words, the syntactic capabilities of seq2seq models may have been greatly understated. We address this gap using the pre-trained seq2seq models T5 and BART, as well as their multilingual variants mT5 and mBART. We evaluate whether they generalize hierarchically on two transformations in two languages: question formation and passivization in English and German. We find that pre-trained seq2seq models generalize hierarchically when performing syntactic transformations, whereas models trained from scratch on syntactic transformations do not. This result presents evidence for the learnability of hierarchical syntactic information from non-annotated natural language text while also demonstrating that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Mueller, Aaron and Frank, Robert and Linzen, Tal and Wang, Luheng and Schuster, Sebastian},
	month = mar,
	year = {2022},
	note = {arXiv:2203.09397 [cs]},
}

@misc{ryoo_token_2023,
	title = {Token {Turing} {Machines}},
	url = {http://arxiv.org/abs/2211.09119},
	doi = {10.48550/arXiv.2211.09119},
	abstract = {We propose Token Turing Machines (TTM), a sequential, autoregressive Transformer model with memory for real-world sequential visual understanding. Our model is inspired by the seminal Neural Turing Machine, and has an external memory consisting of a set of tokens which summarise the previous history (i.e., frames). This memory is efficiently addressed, read and written using a Transformer as the processing unit/controller at each step. The model's memory module ensures that a new observation will only be processed with the contents of the memory (and not the entire history), meaning that it can efficiently process long sequences with a bounded computational cost at each step. We show that TTM outperforms other alternatives, such as other Transformer models designed for long sequences and recurrent neural networks, on two real-world sequential visual understanding tasks: online temporal activity detection from videos and vision-based robot action policy learning. Code is publicly available at: https://github.com/google-research/scenic/tree/main/scenic/projects/token\_turing},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Ryoo, Michael S. and Gopalakrishnan, Keerthana and Kahatapitiya, Kumara and Xiao, Ted and Rao, Kanishka and Stone, Austin and Lu, Yao and Ibarz, Julian and Arnab, Anurag},
	month = apr,
	year = {2023},
	note = {arXiv:2211.09119 [cs]},
}

@misc{wu_recogs_2023,
	title = {{ReCOGS}: {How} {Incidental} {Details} of a {Logical} {Form} {Overshadow} an {Evaluation} of {Semantic} {Interpretation}},
	shorttitle = {{ReCOGS}},
	url = {http://arxiv.org/abs/2303.13716},
	doi = {10.48550/arXiv.2303.13716},
	abstract = {Compositional generalization benchmarks seek to assess whether models can accurately compute meanings for novel sentences, but operationalize this in terms of logical form (LF) prediction. This raises the concern that semantically irrelevant details of the chosen LFs could shape model performance. We argue that this concern is realized for the COGS benchmark (Kim and Linzen, 2020). COGS poses generalization splits that appear impossible for present-day models, which could be taken as an indictment of those models. However, we show that the negative results trace to incidental features of COGS LFs. Converting these LFs to semantically equivalent ones and factoring out capabilities unrelated to semantic interpretation, we find that even baseline models get traction. A recent variable-free translation of COGS LFs suggests similar conclusions, but we observe this format is not semantically equivalent; it is incapable of accurately representing some COGS meanings. These findings inform our proposal for ReCOGS, a modified version of COGS that comes closer to assessing the target semantic capabilities while remaining very challenging. Overall, our results reaffirm the importance of compositional generalization and careful benchmark task design.},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Wu, Zhengxuan and Manning, Christopher D. and Potts, Christopher},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13716 [cs]},
}

@article{dedhe_cognitive_2023,
	title = {Cognitive {Mechanisms} {Underlying} {Recursive} {Pattern} {Processing} in {Human} {Adults}},
	volume = {47},
	copyright = {© 2023 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13273},
	doi = {10.1111/cogs.13273},
	abstract = {The capacity to generate recursive sequences is a marker of rich, algorithmic cognition, and perhaps unique to humans. Yet, the precise processes driving recursive sequence generation remain mysterious. We investigated three potential cognitive mechanisms underlying recursive pattern processing: hierarchical reasoning, ordinal reasoning, and associative chaining. We developed a Bayesian mixture model to quantify the extent to which these three cognitive mechanisms contribute to adult humans’ performance in a sequence generation task. We further tested whether recursive rule discovery depends upon relational information, either perceptual or semantic. We found that the presence of relational information facilitates hierarchical reasoning and drives the generation of recursive sequences across novel depths of center embedding. In the absence of relational information, the use of ordinal reasoning predominates. Our results suggest that hierarchical reasoning is an important cognitive mechanism underlying recursive pattern processing and can be deployed across embedding depths and relational domains.},
	language = {en},
	number = {4},
	urldate = {2023-07-11},
	journal = {Cognitive Science},
	author = {Dedhe, Abhishek M. and Piantadosi, Steven T. and Cantlon, Jessica F.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13273},
	pages = {e13273},
}

@misc{zhong_clock_2023,
	title = {The {Clock} and the {Pizza}: {Two} {Stories} in {Mechanistic} {Explanation} of {Neural} {Networks}},
	shorttitle = {The {Clock} and the {Pizza}},
	url = {http://arxiv.org/abs/2306.17844},
	doi = {10.48550/arXiv.2306.17844},
	abstract = {Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for characterizing the behavior of neural networks across their algorithmic phase space.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17844 [cs]},
}

@article{graehl_training_2008,
	title = {Training {Tree} {Transducers}},
	volume = {34},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli.2008.07-051-R2-03-57},
	doi = {10.1162/coli.2008.07-051-R2-03-57},
	abstract = {Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finite-state) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to-tree and tree-to-string transducers.},
	number = {3},
	urldate = {2023-07-05},
	journal = {Computational Linguistics},
	author = {Graehl, Jonathan and Knight, Kevin and May, Jonathan},
	month = sep,
	year = {2008},
	pages = {391--427},
}

@misc{wong_word_2023,
	title = {From {Word} {Models} to {World} {Models}: {Translating} from {Natural} {Language} to the {Probabilistic} {Language} of {Thought}},
	shorttitle = {From {Word} {Models} to {World} {Models}},
	url = {http://arxiv.org/abs/2306.12672},
	doi = {10.48550/arXiv.2306.12672},
	abstract = {How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Wong, Lionel and Grand, Gabriel and Lew, Alexander K. and Goodman, Noah D. and Mansinghka, Vikash K. and Andreas, Jacob and Tenenbaum, Joshua B.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.12672 [cs]},
}

@misc{dziri_faith_2023-1,
	title = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
	shorttitle = {Faith and {Fate}},
	url = {http://arxiv.org/abs/2305.18654},
	doi = {10.48550/arXiv.2305.18654},
	abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how Transformers' performance will rapidly decay with increased task complexity.},
	urldate = {2023-06-23},
	publisher = {arXiv},
	author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
	month = jun,
	year = {2023},
	note = {arXiv:2305.18654 [cs]},
}

@misc{valvoda_learning_2022,
	title = {Learning {Transductions} to {Test} {Systematic} {Compositionality}},
	url = {http://arxiv.org/abs/2208.08195},
	doi = {10.48550/arXiv.2208.08195},
	abstract = {Recombining known primitive concepts into larger novel combinations is a quintessentially human cognitive capability. Whether large neural models in NLP acquire this ability while learning from data is an open question. In this paper, we look at this problem from the perspective of formal languages. We use deterministic finite-state transducers to make an unbounded number of datasets with controllable properties governing compositionality. By randomly sampling over many transducers, we explore which of their properties (number of states, alphabet size, number of transitions etc.) contribute to learnability of a compositional relation by a neural network. In general, we find that the models either learn the relations completely or not at all. The key is transition coverage, setting a soft learnability limit at 400 examples per transition.},
	urldate = {2022-08-27},
	publisher = {arXiv},
	author = {Valvoda, Josef and Saphra, Naomi and Rawski, Jonathan and Cotterell, Ryan and Williams, Adina},
	month = aug,
	year = {2022},
	note = {arXiv:2208.08195 [cs]},
}

@misc{sun_neural_1993,
	title = {The {Neural} {Network} {Pushdown} {Automaton}: {Model}, {Stack} and {Learning} {Simulations}},
	shorttitle = {The {Neural} {Network} {Pushdown} {Automaton}},
	url = {http://arxiv.org/abs/1711.05738},
	doi = {10.48550/arXiv.1711.05738},
	abstract = {In order for neural networks to learn complex languages or grammars, they must have sufficient computational power or resources to recognize or generate such languages. Though many approaches have been discussed, one ob- vious approach to enhancing the processing power of a recurrent neural network is to couple it with an external stack memory - in effect creating a neural network pushdown automata (NNPDA). This paper discusses in detail this NNPDA - its construction, how it can be trained and how useful symbolic information can be extracted from the trained network. In order to couple the external stack to the neural network, an optimization method is developed which uses an error function that connects the learning of the state automaton of the neural network to the learning of the operation of the external stack. To minimize the error function using gradient descent learning, an analog stack is designed such that the action and storage of information in the stack are continuous. One interpretation of a continuous stack is the probabilistic storage of and action on data. After training on sample strings of an unknown source grammar, a quantization procedure extracts from the analog stack and neural network a discrete pushdown automata (PDA). Simulations show that in learning deterministic context-free grammars - the balanced parenthesis language, 1*n0*n, and the deterministic Palindrome - the extracted PDA is correct in the sense that it can correctly recognize unseen strings of arbitrary length. In addition, the extracted PDAs can be shown to be identical or equivalent to the PDAs of the source grammars which were used to generate the training strings.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Sun, G. Z. and Giles, C. L. and Chen, H. H. and Lee, Y. C.},
	year = {1993},
	note = {arXiv:1711.05738 [cs]},
}

@inproceedings{shiv_novel_2019,
	title = {Novel positional encodings to enable tree-based transformers},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/6e0917469214d8fbd8c517dcdc6b8dcf-Abstract.html},
	abstract = {Neural models optimized for tree-based problems are of great value in tasks like SQL query extraction and program synthesis.
On sequence-structured data, transformers have been shown to learn relationships across arbitrary pairs of positions more reliably than recurrent models.
Motivated by this property, we propose a method to extend transformers to tree-structured data, enabling sequence-to-tree, tree-to-sequence, and tree-to-tree mappings.
Our approach abstracts the transformer's sinusoidal positional encodings, allowing us to instead use a novel positional encoding scheme to represent node positions within trees.
We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over both sequence-to-sequence transformers and state-of-the-art tree-based LSTMs on several datasets.
In particular, our results include a 22\% absolute increase in accuracy on a JavaScript to CoffeeScript translation dataset.},
	urldate = {2023-02-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shiv, Vighnesh and Quirk, Chris},
	year = {2019},
	keywords = {Transformer, Tree},
}

@misc{papadimitriou_pretrain_2023,
	title = {Pretrain on just structure: {Understanding} linguistic inductive biases using transfer learning},
	shorttitle = {Pretrain on just structure},
	url = {http://arxiv.org/abs/2304.13060},
	doi = {10.48550/arXiv.2304.13060},
	abstract = {Both humans and transformer language models are able to learn language without explicit structural supervision. What inductive learning biases make this learning possible? In this study, we examine the effect of different inductive learning biases by predisposing language models with structural biases through pretraining on artificial structured data, and then evaluating by fine-tuning on English. Our experimental setup gives us the ability to actively control the inductive bias of language models. With our experiments, we investigate the comparative success of three types of inductive bias: 1) an inductive bias for recursive, hierarchical processing 2) an inductive bias for unrestricted token-token dependencies that can't be modeled by context-free grammars, and 3) an inductive bias for a Zipfian power-law vocabulary distribution. We show that complex token-token interactions form the best inductive biases, and that this is strongest in the non-context-free case. We also show that a Zipfian vocabulary distribution forms a good inductive bias independently from grammatical structure. Our study leverages the capabilities of transformer models to run controlled language learning experiments that are not possible to run in humans, and surfaces hypotheses about the structures that facilitate language learning in both humans and machines.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Papadimitriou, Isabel and Jurafsky, Dan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13060 [cs]},
	keywords = {Inductive Bias, Transfer Learning},
}

@article{raffel_exploring_2020,
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	volume = {21},
	issn = {1532-4435},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jan,
	year = {2020},
	pages = {140:5485--140:5551},
}

@article{velickovic_everything_2023,
	title = {Everything is connected: {Graph} neural networks},
	volume = {79},
	issn = {0959-440X},
	shorttitle = {Everything is connected},
	url = {https://www.sciencedirect.com/science/article/pii/S0959440X2300012X},
	doi = {10.1016/j.sbi.2023.102538},
	abstract = {In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years—images, text and speech processing—can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.},
	language = {en},
	urldate = {2023-06-16},
	journal = {Current Opinion in Structural Biology},
	author = {Veličković, Petar},
	month = apr,
	year = {2023},
	keywords = {Graph NN},
	pages = {102538},
}

@inproceedings{hale_probabilistic_2001,
	address = {Pittsburgh, Pennsylvania},
	title = {A probabilistic earley parser as a psycholinguistic model},
	url = {http://portal.acm.org/citation.cfm?doid=1073336.1073357},
	doi = {10.3115/1073336.1073357},
	abstract = {In human sentence processing, cognitive load can be defined many ways. This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at some point in a sentence: the surprisal of word wi given its prefix wo...i-1 on a phrase-structural language model. These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpus-frequency data, the operation of Stolcke's probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.},
	language = {en},
	urldate = {2023-06-15},
	booktitle = {Second meeting of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics} on {Language} technologies 2001  - {NAACL} '01},
	publisher = {Association for Computational Linguistics},
	author = {Hale, John},
	year = {2001},
	keywords = {Psycholinguistics},
	pages = {1--8},
}

@article{levy_expectation-based_2008,
	title = {Expectation-based syntactic comprehension},
	volume = {106},
	issn = {0010-0277},
	url = {https://www.sciencedirect.com/science/article/pii/S0010027707001436},
	doi = {10.1016/j.cognition.2007.05.006},
	abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159–166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
	language = {en},
	number = {3},
	urldate = {2023-06-15},
	journal = {Cognition},
	author = {Levy, Roger},
	month = mar,
	year = {2008},
	keywords = {Psycholinguistics},
	pages = {1126--1177},
}

@misc{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	url = {http://arxiv.org/abs/1903.02428},
	doi = {10.48550/arXiv.1903.02428},
	abstract = {We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	month = apr,
	year = {2019},
	note = {arXiv:1903.02428 [cs, stat]},
	keywords = {Graph NN},
}

@inproceedings{jiang_enriching_2021,
	address = {Online},
	title = {Enriching {Transformers} with {Structured} {Tensor}-{Product} {Representations} for {Abstractive} {Summarization}},
	url = {https://aclanthology.org/2021.naacl-main.381},
	doi = {10.18653/v1/2021.naacl-main.381},
	language = {en},
	urldate = {2023-06-15},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Yichen and Celikyilmaz, Asli and Smolensky, Paul and Soulos, Paul and Rao, Sudha and Palangi, Hamid and Fernandez, Roland and Smith, Caitlin and Bansal, Mohit and Gao, Jianfeng},
	year = {2021},
	pages = {4780--4793},
}

@article{velickovic_graph_2018,
	title = {Graph attention networks},
	copyright = {All rights reserved},
	url = {https://www.repository.cam.ac.uk/handle/1810/301348},
	doi = {10.17863/CAM.48429},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	urldate = {2023-06-14},
	author = {Veličković, P and Casanova, A and Liò, P and Cucurull, G and Romero, A and Bengio, Y},
	collaborator = {Apollo-University Of Cambridge Repository and University Of Cambridge},
	year = {2018},
	note = {Publisher: OpenReview.net},
	keywords = {Graph NN},
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {https://www.semanticscholar.org/paper/Relational-inductive-biases%2C-deep-learning%2C-and-Battaglia-Hamrick/19b7769dab4e6092aa4b7eeb8aa078a7b725c9b4},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. 
The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2023-06-14},
	journal = {ArXiv},
	author = {Battaglia, P. and Hamrick, Jessica B. and Bapst, V. and Sanchez-Gonzalez, Alvaro and Zambaldi, V. and Malinowski, Mateusz and Tacchetti, A. and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gülçehre, Çaglar and Song, H. F. and Ballard, A. J. and Gilmer, J. and Dahl, George E. and Vaswani, Ashish and Allen, Kelsey R. and Nash, C. and Langston, Victoria and Dyer, Chris and Heess, N. and Wierstra, Daan and Kohli, Pushmeet and Botvinick, M. and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = jun,
	year = {2018},
	keywords = {Graph NN},
}

@article{zhou_graph_2020,
	title = {Graph neural networks: {A} review of methods and applications},
	volume = {1},
	issn = {26666510},
	shorttitle = {Graph neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651021000012},
	doi = {10.1016/j.aiopen.2021.01.001},
	abstract = {Semantic Scholar extracted view of "Graph Neural Networks: A Review of Methods and Applications" by Jie Zhou et al.},
	language = {en},
	urldate = {2023-06-14},
	journal = {AI Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	year = {2020},
	keywords = {Graph NN},
	pages = {57--81},
}

@article{wu_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Graph} {Neural} {Networks}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9046288/},
	doi = {10.1109/TNNLS.2020.2978386},
	abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
	number = {1},
	urldate = {2023-06-14},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	month = jan,
	year = {2021},
	keywords = {Graph NN},
	pages = {4--24},
}

@inproceedings{hu_fast-r2d2_2022,
	title = {Fast-{R2D2}: {A} {Pretrained} {Recursive} {Neural} {Network} based on {Pruned} {CKY} for {Grammar} {Induction} and {Text} {Representation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Fast-{R2D2}},
	url = {https://arxiv.org/abs/2203.00281},
	doi = {10.48550/ARXIV.2203.00281},
	abstract = {Recently CKY-based models show great potential in unsupervised grammar induction thanks to their human-like encoding paradigm, which runs recursively and hierarchically, but requires \$O(n{\textasciicircum}3)\$ time-complexity. Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pre-training even with complex tree encoder by introducing a heuristic pruning method. However, the rule-based pruning approach suffers from local optimum and slow inference issues. In this paper, we fix those issues in a unified method. We propose to use a top-down parser as a model-based pruning method, which also enables parallel encoding during inference. Typically, our parser casts parsing as a split point scoring task, which first scores all split points for a given sentence, and then recursively splits a span into two by picking a split point with the highest score in the current span. The reverse order of the splits is considered as the order of pruning in R2D2 encoder. Beside the bi-directional language model loss, we also optimize the parser by minimizing the KL distance between tree probabilities from parser and R2D2. Our experiments show that our Fast-R2D2 improves performance significantly in grammar induction and achieves competitive results in downstream classification tasks.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Hu, Xiang and Mi, Haitao and Li, Liang and de Melo, Gerard},
	year = {2022},
	note = {Version Number: 3},
}

@article{astle_toward_2023,
	title = {Toward computational neuroconstructivism: a framework for developmental systems neuroscience},
	volume = {0},
	issn = {1364-6613, 1879-307X},
	shorttitle = {Toward computational neuroconstructivism},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(23)00099-2},
	doi = {10.1016/j.tics.2023.04.009},
	language = {English},
	number = {0},
	urldate = {2023-06-12},
	journal = {Trends in Cognitive Sciences},
	author = {Astle, Duncan E. and Johnson, Mark H. and Akarca, Danyal},
	month = may,
	year = {2023},
	pmid = {37263856},
	note = {Publisher: Elsevier},
}

@misc{karpathy_survival_nodate,
	title = {A {Survival} {Guide} to a {PhD}},
	url = {http://karpathy.github.io/2016/09/07/phd/},
	urldate = {2023-06-08},
	author = {Karpathy, Andrej},
}

@misc{yang_vector-based_2023,
	title = {Vector-based {Representation} is the {Key}: {A} {Study} on {Disentanglement} and {Compositional} {Generalization}},
	shorttitle = {Vector-based {Representation} is the {Key}},
	url = {http://arxiv.org/abs/2305.18063},
	doi = {10.48550/arXiv.2305.18063},
	abstract = {Recognizing elementary underlying concepts from observations (disentanglement) and generating novel combinations of these concepts (compositional generalization) are fundamental abilities for humans to support rapid knowledge learning and generalize to new tasks, with which the deep learning models struggle. Towards human-like intelligence, various works on disentangled representation learning have been proposed, and recently some studies on compositional generalization have been presented. However, few works study the relationship between disentanglement and compositional generalization, and the observed results are inconsistent. In this paper, we study several typical disentangled representation learning works in terms of both disentanglement and compositional generalization abilities, and we provide an important insight: vector-based representation (using a vector instead of a scalar to represent a concept) is the key to empower both good disentanglement and strong compositional generalization. This insight also resonates the neuroscience research that the brain encodes information in neuron population activity rather than individual neurons. Motivated by this observation, we further propose a method to reform the scalar-based disentanglement works (\${\textbackslash}beta\$-TCVAE and FactorVAE) to be vector-based to increase both capabilities. We investigate the impact of the dimensions of vector-based representation and one important question: whether better disentanglement indicates higher compositional generalization. In summary, our study demonstrates that it is possible to achieve both good concept recognition and novel concept composition, contributing an important step towards human-like intelligence.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Yang, Tao and Wang, Yuwang and Lan, Cuiling and Lu, Yan and Zheng, Nanning},
	month = may,
	year = {2023},
	note = {arXiv:2305.18063 [cs]},
}

@misc{murty_grokking_2023,
	title = {Grokking of {Hierarchical} {Structure} in {Vanilla} {Transformers}},
	url = {http://arxiv.org/abs/2305.18741},
	abstract = {For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods -- far beyond the point when in-domain accuracy has saturated. We call this phenomenon {\textbackslash}emph\{structural grokking\}. On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediate-depth models generalize better than both very deep and very shallow transformers. When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of {\textbackslash}citet\{murty2023projections\}. Overall, our work provides strong evidence that, with extended training, vanilla transformers discover and use hierarchical structure.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Murty, Shikhar and Sharma, Pratyusha and Andreas, Jacob and Manning, Christopher D.},
	month = may,
	year = {2023},
	note = {arXiv:2305.18741 [cs]},
}

@article{minsky_logical_1991,
	title = {Logical {Versus} {Analogical} or {Symbolic} {Versus} {Connectionist} or {Neat} {Versus} {Scruffy}},
	volume = {12},
	copyright = {Copyright (c)},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/894},
	doi = {10.1609/aimag.v12i2.894},
	abstract = {Engineering and scientific education condition us to expect everything, including intelligence, to have a simple, compact explanation. Accordingly, when people new to AI ask "What's AI all about," they seem to expect an answer that defines AI in terms of a few basic mathematical laws. Today, some researchers who seek a simple, compact explanation hope that systems modeled on neural nets or some other connectionist idea will quickly overtake more traditional systems based on symbol manipulation. Others believe that symbol manipulation, with a history that goes back millennia, remains the only viable approach. Marvin Minsky subscribes to neither of these extremist views. Instead, he argues that AI must use many approaches. AI is not like circuit theory and electromagnetism. There is nothing wonderfully unifying like Kirchhoff's laws are to circuit theory or Maxwell's equations are to electromagnetism. Instead of looking for a "right way," the time has come to build systems out of diverse components, some connectionist and some symbolic, each with its own diverse justification." - Patrick Winston},
	language = {en},
	number = {2},
	urldate = {2023-06-06},
	journal = {AI Magazine},
	author = {Minsky, Marvin L.},
	month = jun,
	year = {1991},
	note = {Number: 2},
	pages = {34--34},
}

@misc{qiu_tensor_2023,
	title = {Tensor {Products} and {Hyperdimensional} {Computing}},
	url = {http://arxiv.org/abs/2305.10572},
	doi = {10.48550/arXiv.2305.10572},
	abstract = {Following up on a previous analysis of graph embeddings, we generalize and expand some results to the general setting of vector symbolic architectures (VSA) and hyperdimensional computing (HDC). Importantly, we explore the mathematical relationship between superposition, orthogonality, and tensor product. We establish the tensor product representation as the central representation, with a suite of unique properties. These include it being the most general and expressive representation, as well as being the most compressed representation that has errorrless unbinding and detection.},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Qiu, Frank},
	month = may,
	year = {2023},
	note = {arXiv:2305.10572 [cs, stat]},
}

@article{doerig_neuroconnectionist_2023,
	title = {The neuroconnectionist research programme},
	copyright = {2023 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-023-00705-w},
	doi = {10.1038/s41583-023-00705-w},
	abstract = {Artificial neural networks (ANNs) inspired by biology are beginning to be widely used to model behavioural and neural data, an approach we call ‘neuroconnectionism’. ANNs have been not only lauded as the current best models of information processing in the brain but also criticized for failing to account for basic cognitive functions. In this Perspective article, we propose that arguing about the successes and failures of a restricted set of current ANNs is the wrong approach to assess the promise of neuroconnectionism for brain science. Instead, we take inspiration from the philosophy of science, and in particular from Lakatos, who showed that the core of a scientific research programme is often not directly falsifiable but should be assessed by its capacity to generate novel insights. Following this view, we present neuroconnectionism as a general research programme centred around ANNs as a computational language for expressing falsifiable theories about brain computation. We describe the core of the programme, the underlying computational framework and its tools for testing specific neuroscientific hypotheses and deriving novel understanding. Taking a longitudinal view, we review past and present neuroconnectionist projects and their responses to challenges and argue that the research programme is highly progressive, generating new and otherwise unreachable insights into the workings of the brain.},
	language = {en},
	urldate = {2023-06-01},
	journal = {Nature Reviews Neuroscience},
	author = {Doerig, Adrien and Sommers, Rowan P. and Seeliger, Katja and Richards, Blake and Ismael, Jenann and Lindsay, Grace W. and Kording, Konrad P. and Konkle, Talia and van Gerven, Marcel A. J. and Kriegeskorte, Nikolaus and Kietzmann, Tim C.},
	month = may,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	pages = {1--20},
}

@article{frady_resonator_2020,
	title = {Resonator {Networks}, 1: {An} {Efficient} {Solution} for {Factoring} {High}-{Dimensional}, {Distributed} {Representations} of {Data} {Structures}},
	volume = {32},
	issn = {0899-7667},
	shorttitle = {Resonator {Networks}, 1},
	doi = {10.1162/neco_a_01331},
	abstract = {The ability to encode and manipulate data structures with distributed neural representations could qualitatively enhance the capabilities of traditional neural networks by supporting rule-based symbolic reasoning, a central property of cognition. Here we show how this may be accomplished within the framework of Vector Symbolic Architectures (VSAs) (Plate, 1991; Gayler, 1998; Kanerva, 1996), whereby data structures are encoded by combining high-dimensional vectors with operations that together form an algebra on the space of distributed representations. In particular, we propose an efficient solution to a hard combinatorial search problem that arises when decoding elements of a VSA data structure: the factorization of products of multiple codevectors. Our proposed algorithm, called a resonator network, is a new type of recurrent neural network that interleaves VSA multiplication operations and pattern completion. We show in two examples—parsing of a tree-like data structure and parsing of a visual scene—how the factorization problem arises and how the resonator network can solve it. More broadly, resonator networks open the possibility of applying VSAs to myriad artificial intelligence problems in real-world domains. The companion article in this issue (Kent, Frady, Sommer, \& Olshausen, 2020) presents a rigorous analysis and evaluation of the performance of resonator networks, showing it outperforms alternative approaches.},
	number = {12},
	journal = {Neural Computation},
	author = {Frady, E. Paxon and Kent, Spencer J. and Olshausen, Bruno A. and Sommer, Friedrich T.},
	month = dec,
	year = {2020},
	note = {Conference Name: Neural Computation},
	pages = {2311--2331},
}

@misc{noauthor_conference_nodate,
	title = {Conference {Commando}},
}

@misc{dong_language_2016,
	title = {Language to {Logical} {Form} with {Neural} {Attention}},
	url = {http://arxiv.org/abs/1601.01280},
	doi = {10.48550/arXiv.1601.01280},
	abstract = {Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Dong, Li and Lapata, Mirella},
	month = jun,
	year = {2016},
	note = {arXiv:1601.01280 [cs]},
}

@misc{ullman_bayesian_2020,
	title = {Bayesian {Models} of {Conceptual} {Development}: {Learning} as {Building} {Models} of the {World}},
	shorttitle = {Bayesian {Models} of {Conceptual} {Development}},
	url = {https://psyarxiv.com/aq3rp/},
	doi = {10.31234/osf.io/aq3rp},
	abstract = {A Bayesian framework helps to address, in computational terms, what knowledge
children start with and how they construct and adapt models of the world
during childhood. Within this framework, inference over hierarchies of probabilistic
generative programs in particular offers a normative and descriptive
account of children's model-building. We consider two classic settings in which
cognitive development has been framed as model-building: (i) Core knowledge
in infancy, and (ii) The child as scientist. We interpret learning in both of these
settings as resource-constrained, hierarchical Bayesian program induction with
different primitives and constraints. We examine what mechanisms children
could use to meet the algorithmic challenges of navigating large spaces of potential
models, in particular the proposal of {\textbackslash}the child as hacker" and how it
might be realized drawing on recent computational advances. We also discuss
prospects for a unifying account of model building across scientific theories and
intuitive theories, and in biological and cultural evolution more generally.},
	language = {en-us},
	urldate = {2023-05-05},
	publisher = {PsyArXiv},
	author = {Ullman, Tomer and Tenenbaum, Joshua},
	month = jul,
	year = {2020},
}

@article{kleyko_survey_2023,
	title = {A {Survey} on {Hyperdimensional} {Computing} aka {Vector} {Symbolic} {Architectures}, {Part} {I}: {Models} and {Data} {Transformations}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {A {Survey} on {Hyperdimensional} {Computing} aka {Vector} {Symbolic} {Architectures}, {Part} {I}},
	url = {http://arxiv.org/abs/2111.06077},
	doi = {10.1145/3538531},
	abstract = {This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary area with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the area. However, due to a surge of new researchers joining the area in recent years, the necessity for a comprehensive survey of the area has become extremely important. Therefore, amongst other aspects of the area, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.},
	number = {6},
	urldate = {2023-04-17},
	journal = {ACM Computing Surveys},
	author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
	month = jul,
	year = {2023},
	note = {arXiv:2111.06077 [cs]},
	pages = {1--40},
}

@article{kleyko_survey_2023-1,
	title = {A {Survey} on {Hyperdimensional} {Computing} aka {Vector} {Symbolic} {Architectures}, {Part} {II}: {Applications}, {Cognitive} {Models}, and {Challenges}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {A {Survey} on {Hyperdimensional} {Computing} aka {Vector} {Symbolic} {Architectures}, {Part} {II}},
	url = {http://arxiv.org/abs/2112.15424},
	doi = {10.1145/3558000},
	abstract = {This is Part II of the two-part comprehensive survey devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Holographic Reduced Representations is an influential HDC/VSA model that is well-known in the machine learning domain and often used to refer to the whole family. However, for the sake of consistency, we use HDC/VSA to refer to the area. Part I of this survey covered foundational aspects of the area, such as historical context leading to the development of HDC/VSA, key elements of any HDC/VSA model, known HDC/VSA models, and transforming input data of various types into high-dimensional vectors suitable for HDC/VSA. This second part surveys existing applications, the role of HDC/VSA in cognitive computing and architectures, as well as directions for future work. Most of the applications lie within the machine learning/artificial intelligence domain, however we also cover other applications to provide a thorough picture. The survey is written to be useful for both newcomers and practitioners.},
	number = {9},
	urldate = {2023-04-17},
	journal = {ACM Computing Surveys},
	author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
	month = sep,
	year = {2023},
	note = {arXiv:2112.15424 [cs]},
	pages = {1--52},
}

@misc{bricken_attention_2022,
	title = {Attention {Approximates} {Sparse} {Distributed} {Memory}},
	url = {http://arxiv.org/abs/2111.05498},
	doi = {10.48550/arXiv.2111.05498},
	abstract = {While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva's Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Bricken, Trenton and Pehlevan, Cengiz},
	month = jan,
	year = {2022},
	note = {arXiv:2111.05498 [cs]},
}

@article{hersche_neuro-vector-symbolic_2023,
	title = {A neuro-vector-symbolic architecture for solving {Raven}’s progressive matrices},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00630-8},
	doi = {10.1038/s42256-023-00630-8},
	abstract = {Neither deep neural networks nor symbolic artificial intelligence (AI) alone has approached the kind of intelligence expressed in humans. This is mainly because neural networks are not able to decompose joint representations to obtain distinct objects (the so-called binding problem), while symbolic AI suffers from exhaustive rule searches, among other problems. These two problems are still pronounced in neuro-symbolic AI, which aims to combine the best of the two paradigms. Here we show that the two problems can be addressed with our proposed neuro-vector-symbolic architecture (NVSA) by exploiting its powerful operators on high-dimensional distributed representations that serve as a common language between neural networks and symbolic AI. The efficacy of NVSA is demonstrated by solving Raven’s progressive matrices datasets. Compared with state-of-the-art deep neural network and neuro-symbolic approaches, end-to-end training of NVSA achieves a new record of 87.7\% average accuracy in RAVEN, and 88.1\% in I-RAVEN datasets. Moreover, compared with the symbolic reasoning within the neuro-symbolic approaches, the probabilistic reasoning of NVSA with less expensive operations on the distributed representations is two orders of magnitude faster.},
	language = {en},
	urldate = {2023-04-17},
	journal = {Nature Machine Intelligence},
	author = {Hersche, Michael and Zeqiri, Mustafa and Benini, Luca and Sebastian, Abu and Rahimi, Abbas},
	month = mar,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	pages = {1--13},
}

@article{kanerva_hyperdimensional_2009,
	title = {Hyperdimensional {Computing}: {An} {Introduction} to {Computing} in {Distributed} {Representation} with {High}-{Dimensional} {Random} {Vectors}},
	volume = {1},
	issn = {1866-9964},
	shorttitle = {Hyperdimensional {Computing}},
	url = {https://doi.org/10.1007/s12559-009-9009-8},
	doi = {10.1007/s12559-009-9009-8},
	abstract = {The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture. They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.},
	language = {en},
	number = {2},
	urldate = {2023-04-17},
	journal = {Cognitive Computation},
	author = {Kanerva, Pentti},
	month = jun,
	year = {2009},
	pages = {139--159},
}

@article{graves_hybrid_2016,
	title = {Hybrid computing using a neural network with dynamic external memory},
	volume = {538},
	copyright = {2016 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature20101},
	doi = {10.1038/nature20101},
	abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.},
	language = {en},
	number = {7626},
	urldate = {2023-04-11},
	journal = {Nature},
	author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
	month = oct,
	year = {2016},
	note = {Number: 7626
Publisher: Nature Publishing Group},
	pages = {471--476},
}

@misc{noauthor_deep_2023,
	title = {Deep {Learning} {Tuning} {Playbook}},
	url = {https://github.com/google-research/tuning_playbook/blob/1760458aa9cb18ff0d12e12e9acb850f047e9f6c/README.md},
	abstract = {A playbook for systematically maximizing the performance of deep learning models.},
	urldate = {2023-04-07},
	publisher = {Google Research},
	month = apr,
	year = {2023},
	note = {original-date: 2023-01-18T23:32:32Z},
}

@article{dehaene_symbols_2022,
	title = {Symbols and mental programs: a hypothesis about human singularity},
	volume = {26},
	issn = {1364-6613},
	shorttitle = {Symbols and mental programs},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661322001413},
	doi = {10.1016/j.tics.2022.06.010},
	abstract = {Natural language is often seen as the single factor that explains the cognitive singularity of the human species. Instead, we propose that humans possess multiple internal languages of thought, akin to computer languages, which encode and compress structures in various domains (mathematics, music, shape…). These languages rely on cortical circuits distinct from classical language areas. Each is characterized by: (i) the discretization of a domain using a small set of symbols, and (ii) their recursive composition into mental programs that encode nested repetitions with variations. In various tasks of elementary shape or sequence perception, minimum description length in the proposed languages captures human behavior and brain activity, whereas non-human primate data are captured by simpler nonsymbolic models. Our research argues in favor of discrete symbolic models of human thought.},
	language = {en},
	number = {9},
	urldate = {2023-03-30},
	journal = {Trends in Cognitive Sciences},
	author = {Dehaene, Stanislas and Al Roumi, Fosca and Lakretz, Yair and Planton, Samuel and Sablé-Meyer, Mathias},
	month = sep,
	year = {2022},
	pages = {751--766},
}

@article{drozdov_unsupervised_2019,
	title = {Unsupervised {Latent} {Tree} {Induction} with {Deep} {Inside}-{Outside} {Recursive} {Auto}-{Encoders}},
	url = {http://aclweb.org/anthology/N19-1116},
	doi = {10.18653/v1/N19-1116},
	abstract = {We introduce the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence. During training we use dynamic programming to consider all possible binary trees over the sentence, and for inference we use the CKY algorithm to extract the highest scoring parse. DIORA outperforms previously reported results for unsupervised binary constituency parsing on the benchmark WSJ dataset.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Proceedings of the 2019 Conference of the North},
	author = {Drozdov, Andrew and Verga, Patrick and Yadav, Mohit and Iyyer, Mohit and McCallum, Andrew},
	year = {2019},
	note = {Conference Name: Proceedings of the 2019 Conference of the North
Place: Minneapolis, Minnesota
Publisher: Association for Computational Linguistics},
	pages = {1129--1141},
}

@article{kim_unsupervised_2019,
	title = {Unsupervised {Recurrent} {Neural} {Network} {Grammars}},
	url = {http://aclweb.org/anthology/N19-1114},
	doi = {10.18653/v1/N19-1114},
	abstract = {Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Proceedings of the 2019 Conference of the North},
	author = {Kim, Yoon and Rush, Alexander and Yu, Lei and Kuncoro, Adhiguna and Dyer, Chris and Melis, Gábor},
	year = {2019},
	note = {Conference Name: Proceedings of the 2019 Conference of the North
Place: Minneapolis, Minnesota
Publisher: Association for Computational Linguistics},
	pages = {1105--1117},
}

@article{kim_compound_2019,
	title = {Compound {Probabilistic} {Context}-{Free} {Grammars} for {Grammar} {Induction}},
	url = {https://www.aclweb.org/anthology/P19-1228},
	doi = {10.18653/v1/P19-1228},
	abstract = {We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	author = {Kim, Yoon and Dyer, Chris and Rush, Alexander},
	year = {2019},
	note = {Conference Name: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
Place: Florence, Italy
Publisher: Association for Computational Linguistics},
	pages = {2369--2385},
}

@inproceedings{shen_neural_2022,
	title = {Neural {Language} {Modeling} by {Jointly} {Learning} {Syntax} and {Lexicon}},
	url = {https://openreview.net/forum?id=rkgOLb-0W},
	abstract = {We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.},
	language = {en},
	urldate = {2023-03-21},
	author = {Shen, Yikang and Lin, Zhouhan and Huang, Chin-wei and Courville, Aaron},
	month = feb,
	year = {2022},
}

@misc{shen_ordered_2019,
	title = {Ordered {Neurons}: {Integrating} {Tree} {Structures} into {Recurrent} {Neural} {Networks}},
	shorttitle = {Ordered {Neurons}},
	url = {http://arxiv.org/abs/1810.09536},
	doi = {10.48550/arXiv.1810.09536},
	abstract = {Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron},
	month = may,
	year = {2019},
	note = {arXiv:1810.09536 [cs]},
}

@inproceedings{zhao_transformers_2023,
	title = {Do {Transformers} {Parse} while {Predicting} the {Masked} {Word}?},
	url = {https://www.semanticscholar.org/paper/Do-Transformers-Parse-while-Predicting-the-Masked-Zhao-Panigrahi/b3c5a8fe044eeac50d7a8ac4538a7da2d1c4eb3a?utm_source=alert_email&utm_content=LibraryFolder&utm_campaign=AlertEmails_WEEKLY&utm_term=LibraryFolder+AuthorPaper&email_index=0-0-0&utm_medium=13386990},
	abstract = {Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling. Some doubts have been raised whether the models actually are doing parsing or only some computation weakly correlated with it. We study questions: (a) Is it possible to explicitly describe transformers with realistic embedding dimension, number of heads, etc. that are capable of doing parsing -- or even approximate parsing? (b) Why do pre-trained models capture parsing structure? This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm for the English PCFG [Marcus et al, 1993]. We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generated data. We also give a construction of transformers with \$50\$ layers, \$15\$ attention heads, and \$1275\$ dimensional embeddings in average such that using its embeddings it is possible to do constituency parsing with \${\textgreater}70{\textbackslash}\%\$ F1 score on PTB dataset. We conduct probing experiments on models pre-trained on PCFG-generated data to show that this not only allows recovery of approximate parse tree, but also recovers marginal span probabilities computed by the Inside-Outside algorithm, which suggests an implicit bias of masked language modeling towards this algorithm.},
	urldate = {2023-03-21},
	author = {Zhao, Haoyu and Panigrahi, A. and Ge, Rong and Arora, Sanjeev},
	month = mar,
	year = {2023},
}

@misc{goyal_neural_2022,
	title = {Neural {Production} {Systems}: {Learning} {Rule}-{Governed} {Visual} {Dynamics}},
	shorttitle = {Neural {Production} {Systems}},
	url = {http://arxiv.org/abs/2103.01937},
	doi = {10.48550/arXiv.2103.01937},
	abstract = {Visual environments are structured, consisting of distinct objects or entities. These entities have properties -- both visible and latent -- that determine the manner in which they interact with one another. To partition images into entities, deep-learning researchers have proposed structural inductive biases such as slot-based architectures. To model interactions among entities, equivariant graph neural nets (GNNs) are used, but these are not particularly well suited to the task for two reasons. First, GNNs do not predispose interactions to be sparse, as relationships among independent entities are likely to be. Second, GNNs do not factorize knowledge about interactions in an entity-conditional manner. As an alternative, we take inspiration from cognitive science and resurrect a classic approach, production systems, which consist of a set of rule templates that are applied by binding placeholder variables in the rules to specific entities. Rules are scored on their match to entities, and the best fitting rules are applied to update entity properties. In a series of experiments, we demonstrate that this architecture achieves a flexible, dynamic flow of control and serves to factorize entity-specific and rule-based information. This disentangling of knowledge achieves robust future-state prediction in rich visual environments, outperforming state-of-the-art methods using GNNs, and allows for the extrapolation from simple (few object) environments to more complex environments.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Goyal, Anirudh and Didolkar, Aniket and Ke, Nan Rosemary and Blundell, Charles and Beaudoin, Philippe and Heess, Nicolas and Mozer, Michael and Bengio, Yoshua},
	month = mar,
	year = {2022},
	note = {arXiv:2103.01937 [cs, stat]},
}

@misc{kusner_grammar_2017,
	title = {Grammar {Variational} {Autoencoder}},
	url = {http://arxiv.org/abs/1703.01925},
	doi = {10.48550/arXiv.1703.01925},
	abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Kusner, Matt J. and Paige, Brooks and Hernández-Lobato, José Miguel},
	month = mar,
	year = {2017},
	note = {arXiv:1703.01925 [stat]},
}

@misc{saxe_exact_2014,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	url = {http://arxiv.org/abs/1312.6120},
	doi = {10.48550/arXiv.1312.6120},
	abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
	month = feb,
	year = {2014},
	note = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
}

@inproceedings{haley_invertible_2020,
	address = {Barcelona, Spain (Online)},
	title = {Invertible {Tree} {Embeddings} using a {Cryptographic} {Role} {Embedding} {Scheme}},
	url = {https://aclanthology.org/2020.coling-main.328},
	doi = {10.18653/v1/2020.coling-main.328},
	abstract = {We present a novel method for embedding trees in a vector space based on Tensor-Product Representations (TPRs) which allows for inversion: the retrieval of the original tree structure and nodes from the vectorial embedding. Unlike previous attempts, this does not come at the cost of intractable representation size; we utilize a method for non-exact inversion, showing that it works well when there is sufficient randomness in the representation scheme for simple data and providing an upper bound on its error. To handle the huge number of possible tree positions without memoizing position representation vectors, we present a method (Cryptographic Role Embedding) using cryptographic hashing algorithms that allows for the representation of unboundedly many positions. Through experiments on parse tree data, we show a 30,000-dimensional Cryptographic Role Embedding of trees can provide invertibility with error {\textbackslash}textless 1\% that previous methods would require 8.6 {\textbackslash}mbox\${\textbackslash}times\$ 1057 dimensions to represent.},
	urldate = {2023-03-13},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Haley, Coleman and Smolensky, Paul},
	month = dec,
	year = {2020},
	pages = {3671--3683},
}

@inproceedings{bowman_fast_2016,
	address = {Berlin, Germany},
	title = {A {Fast} {Unified} {Model} for {Parsing} and {Sentence} {Understanding}},
	url = {https://aclanthology.org/P16-1139},
	doi = {10.18653/v1/P16-1139},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bowman, Samuel R. and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D. and Potts, Christopher},
	month = aug,
	year = {2016},
	pages = {1466--1477},
}

@misc{elhage_toy_2022,
	title = {Toy {Models} of {Superposition}},
	url = {http://arxiv.org/abs/2209.10652},
	doi = {10.48550/arXiv.2209.10652},
	abstract = {Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in "superposition." We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
	month = sep,
	year = {2022},
	note = {arXiv:2209.10652 [cs]},
}

@misc{kurach_neural_2016,
	title = {Neural {Random}-{Access} {Machines}},
	url = {http://arxiv.org/abs/1511.06392},
	doi = {10.48550/arXiv.1511.06392},
	abstract = {In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya},
	month = feb,
	year = {2016},
	note = {arXiv:1511.06392 [cs]},
}

@inproceedings{bosnjak_programming_2017,
	title = {Programming with a {Differentiable} {Forth} {Interpreter}},
	url = {https://proceedings.mlr.press/v70/bosnjak17a.html},
	abstract = {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.},
	language = {en},
	urldate = {2023-02-24},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bošnjak, Matko and Rocktäschel, Tim and Naradowsky, Jason and Riedel, Sebastian},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {547--556},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	doi = {10.48550/arXiv.2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
}

@misc{evans_learning_2018,
	title = {Learning {Explanatory} {Rules} from {Noisy} {Data}},
	url = {http://arxiv.org/abs/1711.04574},
	doi = {10.48550/arXiv.1711.04574},
	abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data---which is not necessarily easily obtained---that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Evans, Richard and Grefenstette, Edward},
	month = jan,
	year = {2018},
	note = {arXiv:1711.04574 [cs, math]},
}

@misc{wang_hierarchical_2022,
	title = {Hierarchical {Phrase}-based {Sequence}-to-{Sequence} {Learning}},
	url = {http://arxiv.org/abs/2211.07906},
	doi = {10.48550/arXiv.2211.07906},
	abstract = {We describe a neural transducer that maintains the flexibility of standard sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases as a source of inductive bias during training and as explicit constraints during inference. Our approach trains two models: a discriminative parser based on a bracketing transduction grammar whose derivation tree hierarchically aligns source and target phrases, and a neural seq2seq model that learns to translate the aligned phrases one-by-one. We use the same seq2seq model to translate at all phrase scales, which results in two inference modes: one mode in which the parser is discarded and only the seq2seq component is used at the sequence-level, and another in which the parser is combined with the seq2seq model. Decoding in the latter mode is done with the cube-pruned CKY algorithm, which is more involved but can make use of new translation rules during inference. We formalize our model as a source-conditioned synchronous grammar and develop an efficient variational inference algorithm for training. When applied on top of both randomly initialized and pretrained seq2seq models, we find that both inference modes performs well compared to baselines on small scale machine translation benchmarks.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Wang, Bailin and Titov, Ivan and Andreas, Jacob and Kim, Yoon},
	month = nov,
	year = {2022},
	note = {arXiv:2211.07906 [cs]},
}

@book{szabo_case_2012,
	title = {The case for compositionality},
	url = {https://academic.oup.com/edited-volume/41264/chapter/350861452},
	language = {en},
	urldate = {2023-02-16},
	publisher = {Oxford University Press},
	author = {Szabó, Zoltán Gendler},
	month = feb,
	year = {2012},
	doi = {10.1093/oxfordhb/9780199541072.013.0003},
}

@incollection{janssen_montague_2021,
	edition = {Summer 2021},
	title = {Montague {Semantics}},
	url = {https://plato.stanford.edu/archives/sum2021/entries/montague-semantics/},
	abstract = {Montague semantics is a theory of natural language semantics and ofits relation with syntax. It was originally developed by the logicianRichard Montague (1930–1971) and subsequently modified andextended by linguists, philosophers, and logicians. The most importantfeatures of the theory are its use of model theoretic semantics whichis nowadays commonly used for the semantics of logical languages andits adherence to the principle of compositionality—that is, themeaning of the whole is a function of the meanings of its parts andtheir mode of syntactic combination. This entry presents the originsof Montague Semantics, summarizes important aspects of the classicaltheory, and sketches more recent developments. We conclude with asmall example, which illustrates some modern features.},
	urldate = {2023-02-14},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Janssen, Theo M. V. and Zimmermann, Thomas Ede},
	editor = {Zalta, Edward N.},
	year = {2021},
}

@incollection{szabo_compositionality_2022,
	edition = {Fall 2022},
	title = {Compositionality},
	url = {https://plato.stanford.edu/archives/fall2022/entries/compositionality/},
	abstract = {Anything that deserves to be called a language must contain meaningfulexpressions built up from other meaningful expressions. How are theircomplexity and meaning related? The traditional view is that therelationship is fairly tight: the meaning of a complex expression isfully determined by its structure and the meanings of itsconstituents—once we fix what the parts mean and how they areput together we have no more leeway regarding the meaning of thewhole. This is the principle of compositionality, a fundamentalpresupposition of most contemporary work in semantics., Proponents of compositionality typically emphasize the productivityand systematicity of our linguistic understanding. We can understand alarge—perhaps infinitely large—collection of complexexpressions the first time we encounter them, and if we understandsome complex expressions we tend to understand others that can beobtained by recombining their constituents. Compositionality issupposed to feature in the best explanation of these phenomena.Opponents of compositionality typically point to cases when meaningsof larger expressions seem to depend on the intentions of the speaker,on the linguistic environment, or on the setting in which theutterance takes place without their parts displaying a similardependence. They try to respond to the arguments from productivity andsystematicity by insisting that the phenomena are limited, and bysuggesting alternative explanations.},
	urldate = {2023-02-14},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Szabó, Zoltán Gendler},
	editor = {Zalta, Edward N. and Nodelman, Uri},
	year = {2022},
}

@misc{murty_characterizing_2022,
	title = {Characterizing {Intrinsic} {Compositionality} in {Transformers} with {Tree} {Projections}},
	url = {http://arxiv.org/abs/2211.01288},
	doi = {10.48550/arXiv.2211.01288},
	abstract = {When trained on language data, do transformers learn some arbitrary computation that utilizes the full capacity of the architecture or do they learn a simpler, tree-like computation, hypothesized to underlie compositional meaning systems like human languages? There is an apparent tension between compositional accounts of human language understanding, which are based on a restricted bottom-up computational process, and the enormous success of neural models like transformers, which can route information arbitrarily between different parts of their input. One possibility is that these models, while extremely flexible in principle, in practice learn to interpret language hierarchically, ultimately building sentence representations close to those predictable by a bottom-up, tree-structured model. To evaluate this possibility, we describe an unsupervised and parameter-free method to {\textbackslash}emph\{functionally project\} the behavior of any transformer into the space of tree-structured networks. Given an input sentence, we produce a binary tree that approximates the transformer's representation-building process and a score that captures how "tree-like" the transformer's behavior is on the input. While calculation of this score does not require training any additional models, it provably upper-bounds the fit between a transformer and any tree-structured approximation. Using this method, we show that transformers for three different tasks become more tree-like over the course of training, in some cases unsupervisedly recovering the same trees as supervised parsers. These trees, in turn, are predictive of model behavior, with more tree-like models generalizing better on tests of compositional generalization.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Murty, Shikhar and Sharma, Pratyusha and Andreas, Jacob and Manning, Christopher D.},
	month = nov,
	year = {2022},
	note = {arXiv:2211.01288 [cs]},
}

@misc{hupkes_state---art_2023,
	title = {State-of-the-art generalisation research in {NLP}: {A} taxonomy and review},
	shorttitle = {State-of-the-art generalisation research in {NLP}},
	url = {http://arxiv.org/abs/2210.03050},
	doi = {10.48550/arXiv.2210.03050},
	abstract = {The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what 'good generalisation' entails and how it should be evaluated is not well understood, nor are there any evaluation standards for generalisation. In this paper, we lay the groundwork to address both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they investigate, the type of data shift they consider, the source of this data shift, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis that maps out the current state of generalisation research in NLP, and we make recommendations for which areas might deserve attention in the future. Along with this paper, we release a webpage where the results of our review can be dynamically explored, and which we intend to update as new NLP generalisation studies are published. With this work, we aim to take steps towards making state-of-the-art generalisation testing the new status quo in NLP.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Hupkes, Dieuwke and Giulianelli, Mario and Dankers, Verna and Artetxe, Mikel and Elazar, Yanai and Pimentel, Tiago and Christodoulopoulos, Christos and Lasri, Karim and Saphra, Naomi and Sinclair, Arabella and Ulmer, Dennis and Schottmann, Florian and Batsuren, Khuyagbaatar and Sun, Kaiser and Sinha, Koustuv and Khalatbari, Leila and Ryskina, Maria and Frieske, Rita and Cotterell, Ryan and Jin, Zhijing},
	month = jan,
	year = {2023},
	note = {arXiv:2210.03050 [cs]},
}

@misc{noauthor_state---art_nodate,
	title = {State-of-the-art generalisation research in {NLP}: {A} taxonomy and review - {Google} {Search}},
	url = {https://www.google.com/search?client=firefox-b-1-d&q=State-of-the-art+generalisation+research+in+NLP%3A+A+taxonomy+and+revie},
	urldate = {2023-02-10},
}

@inproceedings{gordon_permutation_2020,
	title = {Permutation {Equivariant} {Models} for {Compositional} {Generalization} in {Language}},
	url = {https://openreview.net/forum?id=SylVNerFvr},
	abstract = {Humans understand novel sentences by composing meanings and roles of core language components. In contrast, neural network models for natural language modeling fail when such compositional generalization is required. The main contribution of this paper is to hypothesize that language compositionality is a form of group-equivariance. Based on this hypothesis, we propose a set of tools for constructing equivariant sequence-to-sequence models. Throughout a variety of experiments on the SCAN tasks, we analyze the behavior of existing models under the lens of equivariance, and demonstrate that our equivariant architecture is able to achieve the type compositional generalization required in human language understanding.},
	language = {en},
	urldate = {2023-02-10},
	author = {Gordon, Jonathan and Lopez-Paz, David and Baroni, Marco and Bouchacourt, Diane},
	month = mar,
	year = {2020},
}

@article{sartran_transformer_2022,
	title = {Transformer {Grammars}: {Augmenting} {Transformer} {Language} {Models} with {Syntactic} {Inductive} {Biases} at {Scale}},
	volume = {10},
	issn = {2307-387X},
	shorttitle = {Transformer {Grammars}},
	url = {https://doi.org/10.1162/tacl_a_00526},
	doi = {10.1162/tacl_a_00526},
	abstract = {We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism—one that is independent of composed syntactic representations—plays an important role in current successful models of long text.},
	urldate = {2023-02-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Sartran, Laurent and Barrett, Samuel and Kuncoro, Adhiguna and Stanojević, Miloš and Blunsom, Phil and Dyer, Chris},
	month = dec,
	year = {2022},
	pages = {1423--1439},
}

@article{smolensky_tensor_1990,
	title = {Tensor product variable binding and the representation of symbolic structures in connectionist systems},
	volume = {46},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/000437029090007M},
	doi = {10.1016/0004-3702(90)90007-M},
	abstract = {Semantic Scholar extracted view of "Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems" by P. Smolensky},
	language = {en},
	number = {1-2},
	urldate = {2023-02-10},
	journal = {Artificial Intelligence},
	author = {Smolensky, Paul},
	month = nov,
	year = {1990},
	pages = {159--216},
}

@misc{mou_convolutional_2015,
	title = {Convolutional {Neural} {Networks} over {Tree} {Structures} for {Programming} {Language} {Processing}},
	url = {http://arxiv.org/abs/1409.5718},
	doi = {10.48550/arXiv.1409.5718},
	abstract = {Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
	month = dec,
	year = {2015},
	note = {arXiv:1409.5718 [cs]},
}

@inproceedings{socher_recursive_2013,
	address = {Seattle, Washington, USA},
	title = {Recursive {Deep} {Models} for {Semantic} {Compositionality} {Over} a {Sentiment} {Treebank}},
	url = {https://aclanthology.org/D13-1170},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
	month = oct,
	year = {2013},
	pages = {1631--1642},
}

@inproceedings{wang_tree_2019,
	address = {Hong Kong, China},
	title = {Tree {Transformer}: {Integrating} {Tree} {Structures} into {Self}-{Attention}},
	shorttitle = {Tree {Transformer}},
	url = {https://aclanthology.org/D19-1098},
	doi = {10.18653/v1/D19-1098},
	abstract = {Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed “Constituent Attention” module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yaushian and Lee, Hung-Yi and Chen, Yun-Nung},
	month = nov,
	year = {2019},
	pages = {1061--1070},
}

@inproceedings{ganesan_learning_2021,
	title = {Learning with {Holographic} {Reduced} {Representations}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/d71dd235287466052f1630f31bde7932-Abstract.html},
	urldate = {2023-02-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ganesan, Ashwinkumar and Gao, Hang and Gandhi, Sunil and Raff, Edward and Oates, Tim and Holt, James and McLean, Mark},
	year = {2021},
	pages = {25606--25620},
}

@article{keshav_how_2013,
	title = {How to {Read} a {Paper}},
	issn = {01464833},
	doi = {10.1145/1273445.1273458},
	abstract = {Researchers spend a great deal of time reading research pa- pers. However, this skill is rarely taught, leading to much wasted effort. This article outlines a practical and efficient three-pass method for reading research papers. I also de- scribe how to use this method to do a literature survey.},
	journal = {Work},
	author = {Keshav, S},
	year = {2013},
	pmid = {15735874},
	note = {arXiv: 1003.3921v1
ISBN: 0727915789},
	pages = {2--3},
}

@misc{everett_technical_2022,
	title = {A technical guide to setting up {Notero} ({Zotero} + {Notion} plugin)},
	url = {https://medium.com/@anna-everett/a-technical-guide-to-setting-up-notero-zotero-notion-plugin-d467d675039b},
	abstract = {For those who want more detailed instructions on how to set up Notero (a plugin that connects your Zotero reference manager to your Notion…},
	language = {en},
	urldate = {2023-02-05},
	journal = {Medium},
	author = {Everett, Anna},
	month = oct,
	year = {2022},
}

@incollection{szabo_compositionality_2020,
	edition = {Fall 2020},
	title = {Compositionality},
	url = {https://plato.stanford.edu/archives/fall2020/entries/compositionality/},
	abstract = {Anything that deserves to be called a language must contain meaningfulexpressions built up from other meaningful expressions. How are theircomplexity and meaning related? The traditional view is that therelationship is fairly tight: the meaning of a complex expression isfully determined by its structure and the meanings of itsconstituents—once we fix what the parts mean and how they areput together we have no more leeway regarding the meaning of thewhole. This is the principle of compositionality, a fundamentalpresupposition of most contemporary work in semantics., Proponents of compositionality typically emphasize the productivityand systematicity of our linguistic understanding. We can understand alarge—perhaps infinitely large—collection of complexexpressions the first time we encounter them, and if we understandsome complex expressions we tend to understand others that can beobtained by recombining their constituents. Compositionality issupposed to feature in the best explanation of these phenomena.Opponents of compositionality typically point to cases when meaningsof larger expressions seem to depend on the intentions of the speaker,on the linguistic environment, or on the setting in which theutterance takes place without their parts displaying a similardependence. They try to respond to the arguments from productivity andsystematicity by insisting that the phenomena are limited, and bysuggesting alternative explanations.},
	urldate = {2022-02-03},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Szabó, Zoltán Gendler},
	editor = {Zalta, Edward N.},
	year = {2020},
}

@article{liu_learning_2021,
	title = {Learning {Algebraic} {Recombination} for {Compositional} {Generalization}},
	url = {http://arxiv.org/abs/2107.06516},
	abstract = {Neural sequence models exhibit limited compositional generalization ability in semantic parsing tasks. Compositional generalization requires algebraic recombination, i.e., dynamically recombining structured expressions in a recursive manner. However, most previous studies mainly concentrate on recombining lexical units, which is an important but not sufficient part of algebraic recombination. In this paper, we propose LeAR, an end-to-end neural model to learn algebraic recombination for compositional generalization. The key insight is to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic recombination. Specifically, we learn two modules jointly: a Composer for producing latent syntax, and an Interpreter for assigning semantic operations. Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model. The source code is publicly available at https://github.com/microsoft/ContextualSP.},
	urldate = {2021-08-02},
	journal = {arXiv:2107.06516 [cs]},
	author = {Liu, Chenyao and An, Shengnan and Lin, Zeqi and Liu, Qian and Chen, Bei and Lou, Jian-Guang and Wen, Lijie and Zheng, Nanning and Zhang, Dongmei},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.06516
version: 1},
}

@inproceedings{yin_compositional_2021,
	address = {Online},
	title = {Compositional {Generalization} for {Neural} {Semantic} {Parsing} via {Span}-level {Supervised} {Attention}},
	url = {https://aclanthology.org/2021.naacl-main.225},
	doi = {10.18653/v1/2021.naacl-main.225},
	abstract = {We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.},
	urldate = {2021-08-02},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Pengcheng and Fang, Hao and Neubig, Graham and Pauls, Adam and Platanios, Emmanouil Antonios and Su, Yu and Thomson, Sam and Andreas, Jacob},
	month = jun,
	year = {2021},
	pages = {2810--2823},
}

@article{baggio_compositionality_2021,
	title = {Compositionality in a {Parallel} {Architecture} for {Language} {Processing}},
	volume = {45},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12949},
	doi = {10.1111/cogs.12949},
	abstract = {Compositionality has been a central concept in linguistics and philosophy for decades, and it is increasingly prominent in many other areas of cognitive science. Its status, however, remains contentious. Here, I reassess the nature and scope of the principle of compositionality (Partee, 1995) from the perspective of psycholinguistics and cognitive neuroscience. First, I review classic arguments for compositionality and conclude that they fail to establish compositionality as a property of human language. Next, I state a new competence argument, acknowledging the fact that any competent user of a language L can assign to most expressions in L at least one meaning which is a function only of the meanings of the expression's parts and of its syntactic structure. I then discuss selected results from cognitive neuroscience, indicating that the human brain possesses the processing capacities presupposed by the competence argument. Finally, I outline a language processing architecture consistent with the neuroscience results, where semantic representations may be generated by a syntax-driven stream and by an “asyntactic” processing stream, jointly or independently. Compositionality is viewed as a constraint on computation in the former stream only.},
	language = {en},
	number = {5},
	urldate = {2021-08-02},
	journal = {Cognitive Science},
	author = {Baggio, Giosuè},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12949},
	pages = {e12949},
}

@article{chen_infogan_2016,
	title = {Infogan: {Interpretable} representation learning by information maximizing generative adversarial nets},
	volume = {29},
	journal = {Advances in neural information processing systems},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	year = {2016},
}

@article{chen_isolating_2018,
	title = {Isolating sources of disentanglement in variational autoencoders},
	volume = {31},
	journal = {Advances in neural information processing systems},
	author = {Chen, Ricky TQ and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
	year = {2018},
}

@inproceedings{kulkarni_deep_2015,
	title = {Deep {Convolutional} {Inverse} {Graphics} {Network}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/file/ced556cd9f9c0c8315cfbe0744a3baf0-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kulkarni, Tejas D and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Josh},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@article{bengio_representation_2013,
	title = {Representation learning: {A} review and new perspectives},
	volume = {35},
	number = {8},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	year = {2013},
	note = {Publisher: IEEE},
	pages = {1798--1828},
}

@inproceedings{locatello_challenging_2019,
	title = {Challenging common assumptions in the unsupervised learning of disentangled representations},
	booktitle = {international conference on machine learning},
	publisher = {PMLR},
	author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
	year = {2019},
	pages = {4114--4124},
}

@incollection{smolensky_hamonic_nodate,
	title = {The {Hamonic} {Mind} {Ch} 23},
	author = {Smolensky, Paul},
}

@misc{piantadosi_meaning_2022,
	title = {Meaning without reference in large language models},
	url = {http://arxiv.org/abs/2208.02957},
	doi = {10.48550/arXiv.2208.02957},
	abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Piantadosi, Steven T. and Hill, Felix},
	month = aug,
	year = {2022},
	note = {arXiv:2208.02957 [cs]},
}

@inproceedings{soulos_disentangled_2022,
	address = {San Francisco},
	title = {Disentangled {Face} {Representations} in {Deep} {Generative} {Models} and the {Human} {Brain}},
	url = {https://2022.ccneuro.org/view_paper.php?PaperNum=1082},
	doi = {10.32470/CCN.2022.1082-0},
	abstract = {Despite decades of research, much is still unknown about the computations carried out in the human face processing network. Recently deep networks have been proposed as a computational account of human visual processing. While they provide a good match to neural data throughout visual cortex, they lack interpretability. Here we use a new class of deep generative models, disentangled representation learning models, which learn a low-dimensional latent space that “disentangles” different interpretable dimensions of faces, such as rotation, lighting, or hairstyle. We show that these disentangled networks are a good encoding model for human fMRI data and allow us to investigate how semantically meaningful face features are represented in the brain. We find that several interpretable dimensions, including both identity-specific and identity-invariant dimensions, are distributed widely across the face processing system. The remaining “entangled” representations may be the basis of identity recognition in the brain. These disentangled encoding models provide an exciting alternative to standard “black box” deep learning models and have the potential to change the way we understand face processing in the human brain.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {2022 {Conference} on {Cognitive} {Computational} {Neuroscience}},
	publisher = {Cognitive Computational Neuroscience},
	author = {Soulos, Paul and Isik, Leyla},
	year = {2022},
}

@misc{baker_philosophical_2021,
	title = {A {Philosophical} {Understanding} of {Representation} for {Neuroscience}},
	url = {http://arxiv.org/abs/2102.06592},
	doi = {10.48550/arXiv.2102.06592},
	abstract = {Neuroscientists often describe neural activity as a representation of something, or claim to have found evidence for a neural representation. But what do these statements mean? The reasons to call some neural activity a representation and the assumptions that come with this term are not generally made clear from its common uses in neuroscience. Representation is a central concept in philosophy of mind, with a rich history going back to the ancient period. In order to clarify its usage in neuroscience, here we advance a link between the connotations of this term across these disciplines. We draw on a broad range of discourse in philosophy to distinguish three key aspects of representation: correspondence, functional role, and teleology. We argue that each of these aspects are implied by the explanatory role the term plays in neuroscience. However, evidence related to all three aspects is rarely presented or discussed in the course of individual studies that aim to identify representations. Overlooking the significance of all three aspects hinders communication in neuroscience, as it obscures the limitations of experimental paradigms and conceals gaps in our understanding of the phenomena of primary interest. Working from this three-part view, we discuss how to move toward clearer communication about representations in the brain.},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Baker, Ben and Lansdell, Benjamin and Kording, Konrad},
	month = apr,
	year = {2021},
	note = {arXiv:2102.06592 [q-bio]},
}

@inproceedings{soulos_structural_2021,
	address = {Virtual},
	title = {Structural {Biases} for {Improving} {Transformers} on {Translation} into {Morphologically} {Rich} {Languages}},
	url = {https://aclanthology.org/2021.mtsummit-loresmt.6},
	abstract = {Machine translation has seen rapid progress with the advent of Transformer-based models. These models have no explicit linguistic structure built into them, yet they may still implicitly learn structured relationships by attending to relevant tokens. We hypothesize that this structural learning could be made more robust by explicitly endowing Transformers with a structural bias, and we investigate two methods for building in such a bias. One method, the TP-Transformer, augments the traditional Transformer architecture to include an additional component to represent structure. The second method imbues structure at the data level by segmenting the data with morphological tokenization. We test these methods on translating from English into morphologically rich languages, Turkish and Inuktitut, and consider both automatic metrics and human evaluations. We find that each of these two approaches allows the network to achieve better performance, but this improvement is dependent on the size of the dataset. In sum, structural encoding methods make Transformers more sample-efficient, enabling them to perform better from smaller amounts of data.},
	urldate = {2022-09-04},
	booktitle = {Proceedings of the 4th {Workshop} on {Technologies} for {MT} of {Low} {Resource} {Languages} ({LoResMT2021})},
	publisher = {Association for Machine Translation in the Americas},
	author = {Soulos, Paul and Rao, Sudha and Smith, Caitlin and Rosen, Eric and Celikyilmaz, Asli and McCoy, R. Thomas and Jiang, Yichen and Haley, Coleman and Fernandez, Roland and Palangi, Hamid and Gao, Jianfeng and Smolensky, Paul},
	month = aug,
	year = {2021},
	pages = {52--67},
}

@inproceedings{soulos_discovering_2020,
	address = {Online},
	title = {Discovering the {Compositional} {Structure} of {Vector} {Representations} with {Role} {Learning} {Networks}},
	url = {https://aclanthology.org/2020.blackboxnlp-1.23},
	doi = {10.18653/v1/2020.blackboxnlp-1.23},
	abstract = {How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model's output is changed in the way predicted by our analysis.},
	urldate = {2022-09-04},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Soulos, Paul and McCoy, R. Thomas and Linzen, Tal and Smolensky, Paul},
	month = nov,
	year = {2020},
	pages = {238--254},
}

@misc{patel_revisiting_2022,
	title = {Revisiting the {Compositional} {Generalization} {Abilities} of {Neural} {Sequence} {Models}},
	url = {http://arxiv.org/abs/2203.07402},
	doi = {10.48550/arXiv.2203.07402},
	abstract = {Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future.},
	urldate = {2022-08-22},
	publisher = {arXiv},
	author = {Patel, Arkil and Bhattamishra, Satwik and Blunsom, Phil and Goyal, Navin},
	month = mar,
	year = {2022},
	note = {arXiv:2203.07402 [cs]},
}

@misc{higgins_towards_2018,
	title = {Towards a {Definition} of {Disentangled} {Representations}},
	url = {http://arxiv.org/abs/1812.02230},
	doi = {10.48550/arXiv.1812.02230},
	abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
	urldate = {2022-08-22},
	publisher = {arXiv},
	author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
	month = dec,
	year = {2018},
	note = {arXiv:1812.02230 [cs, stat]},
}

@misc{santoro_symbolic_2022,
	title = {Symbolic {Behaviour} in {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2102.03406},
	doi = {10.48550/arXiv.2102.03406},
	abstract = {The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and Lillicrap, Timothy and Raposo, David},
	month = jan,
	year = {2022},
	note = {arXiv:2102.03406 [cs]},
}

@article{fedorenko_similarity_2021,
	title = {Similarity of {Computations} {Across} {Domains} {Does} {Not} {Imply} {Shared} {Implementation}: {The} {Case} of {Language} {Comprehension}},
	volume = {30},
	url = {https://doi.org/10.1177/09637214211046955},
	doi = {10.1177/09637214211046955},
	abstract = {Understanding language requires applying cognitive operations (e.g., memory retrieval, prediction, structure building) that are relevant across many cognitive domains to specialized knowledge structures (e.g., a particular language’s lexicon and syntax). Are these computations carried out by domain-general circuits or by circuits that store domain-specific representations? Recent work has characterized the roles in language comprehension of the language network, which is selective for high-level language processing, and the multiple-demand (MD) network, which has been implicated in executive functions and linked to fluid intelligence and thus is a prime candidate for implementing computations that support information processing across domains. The language network responds robustly to diverse aspects of comprehension, but the MD network shows no sensitivity to linguistic variables. We therefore argue that the MD network does not play a core role in language comprehension and that past findings suggesting the contrary are likely due to methodological artifacts. Although future studies may reveal some aspects of language comprehension that require the MD network, evidence to date suggests that those will not be related to core linguistic processes such as lexical access or composition. The finding that the circuits that store linguistic knowledge carry out computations on those representations aligns with general arguments against the separation of memory and computation in the mind and brain.},
	number = {6},
	journal = {Current Directions in Psychological Science},
	author = {Fedorenko, Evelina and Shain, Cory},
	year = {2021},
	pmid = {35295820},
	note = {\_eprint: https://doi.org/10.1177/09637214211046955},
	pages = {526--534},
}

@article{shamir_learning_2010,
	series = {Algorithmic {Learning} {Theory} ({ALT} 2008)},
	title = {Learning and generalization with the information bottleneck},
	volume = {411},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S030439751000201X},
	doi = {10.1016/j.tcs.2010.04.006},
	abstract = {The Information Bottleneck is an information theoretic framework that finds concise representations for an ‘input’ random variable that are as relevant as possible for an ‘output’ random variable. This framework has been used successfully in various supervised and unsupervised applications. However, its learning theoretic properties and justification remained unclear as it differs from standard learning models in several crucial aspects, primarily its explicit reliance on the joint input–output distribution. In practice, an empirical plug-in estimate of the underlying distribution has been used, so far without any finite sample performance guarantees. In this paper we present several formal results that address these difficulties. We prove several finite sample bounds, which show that the information bottleneck can provide concise representations with good generalization, based on smaller sample sizes than needed to estimate the underlying distribution. The bounds are non-uniform and adaptive to the complexity of the specific model chosen. Based on these results, we also present a preliminary analysis on the possibility of analyzing the information bottleneck method as a learning algorithm in the familiar performance-complexity tradeoff framework. In addition, we formally describe the connection between the information bottleneck and minimal sufficient statistics.},
	language = {en},
	number = {29},
	urldate = {2022-08-12},
	journal = {Theoretical Computer Science},
	author = {Shamir, Ohad and Sabato, Sivan and Tishby, Naftali},
	month = jun,
	year = {2010},
	pages = {2696--2711},
}

@article{hao_formal_2022,
	title = {Formal {Language} {Recognition} by {Hard} {Attention} {Transformers}: {Perspectives} from {Circuit} {Complexity}},
	volume = {10},
	shorttitle = {Formal {Language} {Recognition} by {Hard} {Attention} {Transformers}},
	url = {https://aclanthology.org/2022.tacl-1.46},
	doi = {10.1162/tacl_a_00490},
	abstract = {This paper analyzes three formal models of Transformer encoders that differ in the form of their self-attention mechanism: unique hard attention (UHAT); generalized unique hard attention (GUHAT), which generalizes UHAT; and averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers, viewed as string acceptors, can only recognize formal languages in the complexity class AC0, the class of languages recognizable by families of Boolean circuits of constant depth and polynomial size. This upper bound subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages or the PARITY language, since those languages are outside AC0 (Furst et al., 1984). In contrast, the non-AC0 languages MAJORITY and DYCK-1 are recognizable by AHAT networks, implying that AHAT can recognize languages that UHAT and GUHAT cannot.},
	urldate = {2022-08-12},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Hao, Yiding and Angluin, Dana and Frank, Robert},
	year = {2022},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {800--810},
}

@misc{shaw_compositional_2021,
	title = {Compositional {Generalization} and {Natural} {Language} {Variation}: {Can} a {Semantic} {Parsing} {Approach} {Handle} {Both}?},
	shorttitle = {Compositional {Generalization} and {Natural} {Language} {Variation}},
	url = {http://arxiv.org/abs/2010.12725},
	doi = {10.48550/arXiv.2010.12725},
	abstract = {Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization? To better assess this capability, we propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model. It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations. While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Shaw, Peter and Chang, Ming-Wei and Pasupat, Panupong and Toutanova, Kristina},
	month = jun,
	year = {2021},
	note = {arXiv:2010.12725 [cs]},
}

@inproceedings{schlag_learning_2018,
	title = {Learning to {Reason} with {Third} {Order} {Tensor} {Products}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/a274315e1abede44d63005826249d1df-Abstract.html},
	abstract = {We combine Recurrent Neural Networks with Tensor Product Representations to
learn combinatorial representations of sequential data. This improves symbolic
interpretation and systematic generalisation. Our architecture is trained end-to-end
through gradient descent on a variety of simple natural language reasoning tasks,
significantly outperforming the latest state-of-the-art models in single-task and
all-tasks settings. We also augment a subset of the data such that training and test
data exhibit large systematic differences and show that our approach generalises
better than the previous state-of-the-art.},
	urldate = {2022-08-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Schlag, Imanol and Schmidhuber, Jürgen},
	year = {2018},
}

@inproceedings{dankers_paradox_2022,
	address = {Dublin, Ireland},
	title = {The {Paradox} of the {Compositionality} of {Natural} {Language}: {A} {Neural} {Machine} {Translation} {Case} {Study}},
	shorttitle = {The {Paradox} of the {Compositionality} of {Natural} {Language}},
	url = {https://aclanthology.org/2022.acl-long.286},
	doi = {10.18653/v1/2022.acl-long.286},
	abstract = {Obtaining human-like performance in NLP is often argued to require compositional generalisation. Whether neural networks exhibit this ability is usually studied by training models on highly compositional synthetic data. However, compositionality in natural language is much more complex than the rigid, arithmetic-like version such data adheres to, and artificial compositionality tests thus do not allow us to determine how neural models deal with more realistic forms of compositionality. In this work, we re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT).Our results highlight that: i) unfavourably, models trained on more data are more compositional; ii) models are sometimes less compositional than expected, but sometimes more, exemplifying that different levels of compositionality are required, and models are not always able to modulate between them correctly; iii) some of the non-compositional behaviours are mistakes, whereas others reflect the natural variation in data. Apart from an empirical study, our work is a call to action: we should rethink the evaluation of compositionality in neural networks and develop benchmarks using real data to evaluate compositionality on natural language, where composing meaning is not as straightforward as doing the math.},
	urldate = {2022-08-12},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dankers, Verna and Bruni, Elia and Hupkes, Dieuwke},
	month = may,
	year = {2022},
	pages = {4154--4175},
}

@inproceedings{dankers_can_2022,
	address = {Dublin, Ireland},
	title = {Can {Transformer} be {Too} {Compositional}? {Analysing} {Idiom} {Processing} in {Neural} {Machine} {Translation}},
	shorttitle = {Can {Transformer} be {Too} {Compositional}?},
	url = {https://aclanthology.org/2022.acl-long.252},
	doi = {10.18653/v1/2022.acl-long.252},
	abstract = {Unlike literal expressions, idioms' meanings do not directly follow from their parts, posing a challenge for neural machine translation (NMT). NMT models are often unable to translate idioms accurately and over-generate compositional, literal translations. In this work, we investigate whether the non-compositionality of idioms is reflected in the mechanics of the dominant NMT model, Transformer, by analysing the hidden states and attention patterns for models with English as source language and one of seven European languages as target language.When Transformer emits a non-literal translation - i.e. identifies the expression as idiomatic - the encoder processes idioms more strongly as single lexical units compared to literal expressions. This manifests in idioms' parts being grouped through attention and in reduced interaction between idioms and their context.In the decoder's cross-attention, figurative inputs result in reduced attention on source-side tokens. These results suggest that Transformer's tendency to process idioms as compositional expressions contributes to literal translations of idioms.},
	urldate = {2022-08-12},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dankers, Verna and Lucas, Christopher and Titov, Ivan},
	month = may,
	year = {2022},
	pages = {3608--3626},
}

@article{elhage_softmax_2022,
	title = {Softmax {Linear} {Units}},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Nanda, Neel and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Jones and and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and Hatfield-Dodds, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and Tran-Johnson, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
	year = {2022},
}

@inproceedings{higgins_beta-vae_2022,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	abstract = {We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.},
	language = {en},
	urldate = {2022-08-11},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	month = jul,
	year = {2022},
}

@techreport{smolensky_neurocompositional_2022,
	title = {Neurocompositional computing: {From} the {Central} {Paradox} of {Cognition} to a new generation of {AI} systems},
	shorttitle = {Neurocompositional computing},
	url = {http://arxiv.org/abs/2205.01128},
	abstract = {What explains the dramatic progress from 20th-century to 21st-century AI, and how can the remaining limitations of current AI be overcome? The widely accepted narrative attributes this progress to massive increases in the quantity of computational and data resources available to support statistical learning in deep artificial neural networks. We show that an additional crucial factor is the development of a new type of computation. Neurocompositional computing adopts two principles that must be simultaneously respected to enable human-level cognition: the principles of Compositionality and Continuity. These have seemed irreconcilable until the recent mathematical discovery that compositionality can be realized not only through discrete methods of symbolic computing, but also through novel forms of continuous neural computing. The revolutionary recent progress in AI has resulted from the use of limited forms of neurocompositional computing. New, deeper forms of neurocompositional computing create AI systems that are more robust, accurate, and comprehensible.},
	urldate = {2022-05-31},
	institution = {arXiv},
	author = {Smolensky, Paul and McCoy, R. Thomas and Fernandez, Roland and Goldrick, Matthew and Gao, Jianfeng},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.01128},
	note = {arXiv:2205.01128 [cs]
type: article},
}

@article{hauser_faculty_2002,
	title = {The {Faculty} of {Language}: {What} {Is} {It}, {Who} {Has} {It}, and {How} {Did} {It} {Evolve}?},
	volume = {298},
	shorttitle = {The {Faculty} of {Language}},
	url = {https://www.science.org/doi/10.1126/science.298.5598.1569},
	doi = {10.1126/science.298.5598.1569},
	number = {5598},
	urldate = {2022-08-10},
	journal = {Science},
	author = {Hauser, Marc D. and Chomsky, Noam and Fitch, W. Tecumseh},
	month = nov,
	year = {2002},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1569--1579},
}

@inproceedings{grefenstette_learning_2015,
	title = {Learning to {Transduce} with {Unbounded} {Memory}},
	booktitle = {{NIPS}},
	author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
	year = {2015},
}

@article{krizhevsky_imagenet_2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	pages = {84 -- 90},
}

@inproceedings{papineni_bleu_2002,
	address = {USA},
	series = {{ACL} '02},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle = {{BLEU}},
	url = {https://doi.org/10.3115/1073083.1073135},
	doi = {10.3115/1073083.1073135},
	abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
	urldate = {2022-08-09},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	month = jul,
	year = {2002},
	pages = {311--318},
}

@article{schlag_enhancing_2019,
	title = {Enhancing the {Transformer} with {Explicit} {Relational} {Encoding} for {Math} {Problem} {Solving}},
	volume = {abs/1910.06611},
	journal = {ArXiv},
	author = {Schlag, Imanol and Smolensky, Paul and Fernandez, Roland and Jojic, Nebojsa and Schmidhuber, Jürgen and Gao, Jianfeng},
	year = {2019},
}

@inproceedings{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
	booktitle = {{EMNLP}},
	author = {Cho, Kyunghyun and Merrienboer, Bart van and Gülçehre, Çaglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	year = {2014},
}

@article{noauthor_pdf_nodate,
	title = {[{PDF}] {Learning} {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer/0b544dfe355a5070b60986319a3f51fb45d1348e},
	abstract = {Qualitatively, the proposed RNN Encoder‐Decoder model learns a semantically and syntactically meaningful representation of linguistic phrases. In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2022-08-09},
}

@misc{vito_asymmetric_2022,
	title = {An {Asymmetric} {Contrastive} {Loss} for {Handling} {Imbalanced} {Datasets}},
	url = {http://arxiv.org/abs/2207.07080},
	doi = {10.48550/arXiv.2207.07080},
	abstract = {Contrastive learning is a representation learning method performed by contrasting a sample to other similar samples so that they are brought closely together, forming clusters in the feature space. The learning process is typically conducted using a two-stage training architecture, and it utilizes the contrastive loss (CL) for its feature learning. Contrastive learning has been shown to be quite successful in handling imbalanced datasets, in which some classes are overrepresented while some others are underrepresented. However, previous studies have not specifically modified CL for imbalanced datasets. In this work, we introduce an asymmetric version of CL, referred to as ACL, in order to directly address the problem of class imbalance. In addition, we propose the asymmetric focal contrastive loss (AFCL) as a further generalization of both ACL and focal contrastive loss (FCL). Results on the FMNIST and ISIC 2018 imbalanced datasets show that AFCL is capable of outperforming CL and FCL in terms of both weighted and unweighted classification accuracies. In the appendix, we provide a full axiomatic treatment on entropy, along with complete proofs.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Vito, Valentino and Stefanus, Lim Yohanes},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07080 [cs, math, stat]},
}

@misc{tam_parameter-efficient_2022,
	title = {Parameter-{Efficient} {Prompt} {Tuning} {Makes} {Generalized} and {Calibrated} {Neural} {Text} {Retrievers}},
	url = {http://arxiv.org/abs/2207.07087},
	doi = {10.48550/arXiv.2207.07087},
	abstract = {Prompt tuning attempts to update few task-specific parameters in pre-trained models. It has achieved comparable performance to fine-tuning of the full parameter set on both language understanding and generation tasks. In this work, we study the problem of prompt tuning for neural text retrievers. We introduce parameter-efficient prompt tuning for text retrieval across in-domain, cross-domain, and cross-topic settings. Through an extensive analysis, we show that the strategy can mitigate the two issues -- parameter-inefficiency and weak generalizability -- faced by fine-tuning based retrieval methods. Notably, it can significantly improve the out-of-domain zero-shot generalization of the retrieval models. By updating only 0.1\% of the model parameters, the prompt tuning strategy can help retrieval models achieve better generalization performance than traditional methods in which all parameters are updated. Finally, to facilitate research on retrievers' cross-topic generalizability, we curate and release an academic retrieval dataset with 18K query-results pairs in 87 topics, making it the largest topic-specific one to date.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Tam, Weng Lam and Liu, Xiao and Ji, Kaixuan and Xue, Lilong and Zhang, Xingjian and Dong, Yuxiao and Liu, Jiahua and Hu, Maodi and Tang, Jie},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07087 [cs]},
}

@misc{yamac_personalized_2022,
	title = {A {Personalized} {Zero}-{Shot} {ECG} {Arrhythmia} {Monitoring} {System}: {From} {Sparse} {Representation} {Based} {Domain} {Adaption} to {Energy} {Efficient} {Abnormal} {Beat} {Detection} for {Practical} {ECG} {Surveillance}},
	shorttitle = {A {Personalized} {Zero}-{Shot} {ECG} {Arrhythmia} {Monitoring} {System}},
	url = {http://arxiv.org/abs/2207.07089},
	doi = {10.48550/arXiv.2207.07089},
	abstract = {This paper proposes a low-cost and highly accurate ECG-monitoring system intended for personalized early arrhythmia detection for wearable mobile sensors. Earlier supervised approaches for personalized ECG monitoring require both abnormal and normal heartbeats for the training of the dedicated classifier. However, in a real-world scenario where the personalized algorithm is embedded in a wearable device, such training data is not available for healthy people with no cardiac disorder history. In this study, (i) we propose a null space analysis on the healthy signal space obtained via sparse dictionary learning, and investigate how a simple null space projection or alternatively regularized least squares-based classification methods can reduce the computational complexity, without sacrificing the detection accuracy, when compared to sparse representation-based classification. (ii) Then we introduce a sparse representation-based domain adaptation technique in order to project other existing users' abnormal and normal signals onto the new user's signal space, enabling us to train the dedicated classifier without having any abnormal heartbeat of the new user. Therefore, zero-shot learning can be achieved without the need for synthetic abnormal heartbeat generation. An extensive set of experiments performed on the benchmark MIT-BIH ECG dataset shows that when this domain adaptation-based training data generator is used with a simple 1-D CNN classifier, the method outperforms the prior work by a significant margin. (iii) Then, by combining (i) and (ii), we propose an ensemble classifier that further improves the performance. This approach for zero-shot arrhythmia detection achieves an average accuracy level of 98.2\% and an F1-Score of 92.8\%. Finally, a personalized energy-efficient ECG monitoring scheme is proposed using the above-mentioned innovations.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Yamaç, Mehmet and Duman, Mert and Adalıoğlu, İlke and Kiranyaz, Serkan and Gabbouj, Moncef},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07089 [cs]},
}

@misc{cotogni_explaining_2022,
	title = {Explaining {Image} {Enhancement} {Black}-{Box} {Methods} through a {Path} {Planning} {Based} {Algorithm}},
	url = {http://arxiv.org/abs/2207.07092},
	doi = {10.48550/arXiv.2207.07092},
	abstract = {Nowadays, image-to-image translation methods, are the state of the art for the enhancement of natural images. Even if they usually show high performance in terms of accuracy, they often suffer from several limitations such as the generation of artifacts and the scalability to high resolutions. Moreover, their main drawback is the completely black-box approach that does not allow to provide the final user with any insight about the enhancement processes applied. In this paper we present a path planning algorithm which provides a step-by-step explanation of the output produced by state of the art enhancement methods, overcoming black-box limitation. This algorithm, called eXIE, uses a variant of the A* algorithm to emulate the enhancement process of another method through the application of an equivalent sequence of enhancing operators. We applied eXIE to explain the output of several state-of-the-art models trained on the Five-K dataset, obtaining sequences of enhancing operators able to produce very similar results in terms of performance and overcoming the huge limitation of poor interpretability of the best performing algorithms.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Cotogni, Marco and Cusano, Claudio},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07092 [cs]},
}

@misc{shi_react_2022,
	title = {{ReAct}: {Temporal} {Action} {Detection} with {Relational} {Queries}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2207.07097},
	doi = {10.48550/arXiv.2207.07097},
	abstract = {This work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with action queries, similar to DETR, which has shown great success in object detection. However, the framework suffers from several problems if directly applied to TAD: the insufficient exploration of inter-query relation in the decoder, the inadequate classification training due to a limited number of training samples, and the unreliable classification scores at inference. To this end, we first propose a relational attention mechanism in the decoder, which guides the attention among queries based on their relations. Moreover, we propose two losses to facilitate and stabilize the training of action classification. Lastly, we propose to predict the localization quality of each action query at inference in order to distinguish high-quality queries. The proposed method, named ReAct, achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Besides, extensive ablation studies are conducted to verify the effectiveness of each proposed component. The code is available at https://github.com/sssste/React.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Shi, Dingfeng and Zhong, Yujie and Cao, Qiong and Zhang, Jing and Ma, Lin and Li, Jia and Tao, Dacheng},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07097 [cs]},
}

@misc{chen_likelihood_2022,
	title = {Likelihood {Training} of {Schr}{\textbackslash}"odinger {Bridge} using {Forward}-{Backward} {SDEs} {Theory}},
	url = {http://arxiv.org/abs/2110.11291},
	doi = {10.48550/arXiv.2110.11291},
	abstract = {Schr{\textbackslash}"odinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory - a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimization principle that inherits the same SB optimality yet without losing applications of modern generative training techniques, and we show that the resulting training algorithm achieves comparable results on generating realistic images on MNIST, CelebA, and CIFAR10. Our code is available at https://github.com/ghliu/SB-FBSDE.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Chen, Tianrong and Liu, Guan-Horng and Theodorou, Evangelos A.},
	month = jul,
	year = {2022},
	note = {arXiv:2110.11291 [cs, math, stat]},
}

@misc{dong_bootstrapped_2022,
	title = {Bootstrapped {Masked} {Autoencoders} for {Vision} {BERT} {Pretraining}},
	url = {http://arxiv.org/abs/2207.07116},
	doi = {10.48550/arXiv.2207.07116},
	abstract = {We propose bootstrapped masked autoencoders (BootMAE), a new approach for vision BERT pretraining. BootMAE improves the original masked autoencoders (MAE) with two core designs: 1) momentum encoder that provides online feature as extra BERT prediction targets; 2) target-aware decoder that tries to reduce the pressure on the encoder to memorize target-specific information in BERT pretraining. The first design is motivated by the observation that using a pretrained MAE to extract the features as the BERT prediction target for masked tokens can achieve better pretraining performance. Therefore, we add a momentum encoder in parallel with the original MAE encoder, which bootstraps the pretraining performance by using its own representation as the BERT prediction target. In the second design, we introduce target-specific information (e.g., pixel values of unmasked patches) from the encoder directly to the decoder to reduce the pressure on the encoder of memorizing the target-specific information. Thus, the encoder focuses on semantic modeling, which is the goal of BERT pretraining, and does not need to waste its capacity in memorizing the information of unmasked tokens related to the prediction target. Through extensive experiments, our BootMAE achieves \$84.2{\textbackslash}\%\$ Top-1 accuracy on ImageNet-1K with ViT-B backbone, outperforming MAE by \$+0.8{\textbackslash}\%\$ under the same pre-training epochs. BootMAE also gets \$+1.0\$ mIoU improvements on semantic segmentation on ADE20K and \$+1.3\$ box AP, \$+1.4\$ mask AP improvement on object detection and segmentation on COCO dataset. Code is released at https://github.com/LightDXY/BootMAE.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Dong, Xiaoyi and Bao, Jianmin and Zhang, Ting and Chen, Dongdong and Zhang, Weiming and Yuan, Lu and Chen, Dong and Wen, Fang and Yu, Nenghai},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07116 [cs]},
}

@article{higgins_unsupervised_2021,
	title = {Unsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26751-5},
	doi = {10.1038/s41467-021-26751-5},
	abstract = {In order to better understand how the brain perceives faces, it is important to know what objective drives learning in the ventral visual stream. To answer this question, we model neural responses to faces in the macaque inferotemporal (IT) cortex with a deep self-supervised generative model, β-VAE, which disentangles sensory data into interpretable latent factors, such as gender or age. Our results demonstrate a strong correspondence between the generative factors discovered by β-VAE and those coded by single IT neurons, beyond that found for the baselines, including the handcrafted state-of-the-art model of face perception, the Active Appearance Model, and deep classifiers. Moreover, β-VAE is able to reconstruct novel face images using signals from just a handful of cells. Together our results imply that optimising the disentangling objective leads to representations that closely resemble those in the IT at the single unit level. This points at disentangling as a plausible learning objective for the visual brain.},
	language = {en},
	number = {1},
	urldate = {2022-05-27},
	journal = {Nature Communications},
	author = {Higgins, Irina and Chang, Le and Langston, Victoria and Hassabis, Demis and Summerfield, Christopher and Tsao, Doris and Botvinick, Matthew},
	month = nov,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {6456},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2022-06-13},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ramón Risco and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.04615
arXiv:2206.04615 [cs, stat]},
}

@article{smolensky_neurocompositional_2022-1,
	title = {Neurocompositional computing in human and machine intelligence: {A} tutorial},
	shorttitle = {Neurocompositional computing in human and machine intelligence},
	url = {https://www.microsoft.com/en-us/research/publication/neurocompositional-computing-in-human-and-machine-intelligence-a-tutorial/},
	abstract = {We argue that the unprecedented progress of 21st-century AI has resulted from the use of limited—first-generation—forms of what we call neurocompositional computing. We show that the new techniques now being deployed in second-generation neurocompositional computing create AI systems that are not only more robust and accurate than current systems, but also more comprehensible—making it possible to diagnose errors in, and exert human control over, artificial neural networks through interpretation of their internal states and direct intervention upon those states.},
	language = {en-US},
	urldate = {2022-05-31},
	author = {Smolensky, Paul and McCoy, R. Thomas and Fernandez, Roland and Goldrick, Matthew and Gao, Jianfeng},
	month = may,
	year = {2022},
}

@inproceedings{kim_disentangling_2018,
	title = {Disentangling by {Factorising}},
	url = {https://proceedings.mlr.press/v80/kim18b.html},
	abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
	language = {en},
	urldate = {2022-05-27},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Hyunjik and Mnih, Andriy},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2649--2658},
}

@techreport{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	number = {arXiv:1312.6114},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	doi = {10.48550/arXiv.1312.6114},
	note = {arXiv:1312.6114 [cs, stat]
type: article},
}

@article{kriegeskorte_individual_2007,
	title = {Individual faces elicit distinct response patterns in human anterior temporal cortex},
	volume = {104},
	issn = {1091-6490},
	doi = {10.1073/pnas.0705654104},
	abstract = {Visual face identification requires distinguishing between thousands of faces we know. This computational feat involves a network of brain regions including the fusiform face area (FFA) and anterior inferotemporal cortex (aIT), whose roles in the process are not well understood. Here, we provide the first demonstration that it is possible to discriminate cortical response patterns elicited by individual face images with high-resolution functional magnetic resonance imaging (fMRI). Response patterns elicited by the face images were distinct in aIT but not in the FFA. Individual-level face information is likely to be present in both regions, but our data suggest that it is more pronounced in aIT. One interpretation is that the FFA detects faces and engages aIT for identification.},
	language = {eng},
	number = {51},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kriegeskorte, Nikolaus and Formisano, Elia and Sorger, Bettina and Goebel, Rainer},
	month = dec,
	year = {2007},
	pmid = {18077383},
	pmcid = {PMC2154477},
	pages = {20600--20605},
}

@misc{noauthor_generative_2022,
	title = {Generative {Flow} {Networks}},
	url = {https://yoshuabengio.org/2022/03/05/generative-flow-networks/},
	abstract = {I have rarely been as enthusiastic about a new research direction. We call them GFlowNets, for Generative Flow Networks. They live somewhere at the intersection…},
	language = {en-CA},
	urldate = {2022-04-04},
	journal = {Yoshua Bengio},
	month = mar,
	year = {2022},
}

@article{ruis_improving_2022,
	title = {Improving {Systematic} {Generalization} {Through} {Modularity} and {Augmentation}},
	url = {http://arxiv.org/abs/2202.10745},
	abstract = {Systematic generalization is the ability to combine known parts into novel meaning; an important aspect of efficient human learning, but a weakness of neural network learning. In this work, we investigate how two well-known modeling principles -- modularity and data augmentation -- affect systematic generalization of neural networks in grounded language learning. We analyze how large the vocabulary needs to be to achieve systematic generalization and how similar the augmented data needs to be to the problem at hand. Our findings show that even in the controlled setting of a synthetic benchmark, achieving systematic generalization remains very difficult. After training on an augmented dataset with almost forty times more adverbs than the original problem, a non-modular baseline is not able to systematically generalize to a novel combination of a known verb and adverb. When separating the task into cognitive processes like perception and navigation, a modular neural network is able to utilize the augmented data and generalize more systematically, achieving 70\% and 40\% exact match increase over state-of-the-art on two gSCAN tests that have not previously been improved. We hope that this work gives insight into the drivers of systematic generalization, and what we still need to improve for neural networks to learn more like humans do.},
	urldate = {2022-03-31},
	journal = {arXiv:2202.10745 [cs]},
	author = {Ruis, Laura and Lake, Brenden},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.10745},
}

@book{steedman_categorial_2014,
	title = {Categorial {Grammar}},
	isbn = {978-0-415-53394-2 978-1-315-79660-4 978-1-317-75104-5},
	url = {https://www.routledgehandbooks.com/doi/10.4324/9781315796604.ch32},
	abstract = {The study of syntax over the last half century has seen a remarkable expansion of the boundaries of human knowledge about the structure of natural language. The Routledge Handbook of Syntax presents a comprehensive survey of the major theoretical and empirical advances in the dynamically evolving field of syntax from a variety of perspectives, both within the dominant generative paradigm and between syntacticians working within generative grammar and those working in functionalist and related approaches. The handbook covers key issues within the field that include: • core areas of syntactic empirical investigation, • contemporary approaches to syntactic theory, • interfaces of syntax with other components of the human language system, • experimental and computational approaches to syntax. Bringing together renowned linguistic scientists and cutting-edge scholars from across the discipline and providing a balanced yet comprehensive overview of the field, the Routledge Handbook of Syntax is essential reading for researchers and postgraduate students working in syntactic theory.},
	language = {en},
	urldate = {2022-03-30},
	publisher = {Routledge Handbooks Online},
	author = {Steedman, Mark},
	month = may,
	year = {2014},
	doi = {10.4324/9781315796604.ch32},
}

@misc{jacobson_direct_2012,
	title = {Direct {Compositionality}},
	url = {https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199541072.001.0001/oxfordhb-9780199541072-e-5},
	abstract = {"Direct Compositionality" published on  by Oxford University Press.},
	language = {en},
	urldate = {2022-03-17},
	journal = {The Oxford Handbook of Compositionality},
	author = {Jacobson, Pauline},
	month = feb,
	year = {2012},
	doi = {10.1093/oxfordhb/9780199541072.013.0005},
	note = {ISBN: 9780199541072},
}

@article{friston_world_2021,
	title = {World model learning and inference},
	volume = {144},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608021003610},
	doi = {10.1016/j.neunet.2021.09.011},
	abstract = {Understanding information processing in the brain—and creating general-purpose artificial intelligence—are long-standing aspirations of scientists and engineers worldwide. The distinctive features of human intelligence are high-level cognition and control in various interactions with the world including the self, which are not defined in advance and are vary over time. The challenge of building human-like intelligent machines, as well as progress in brain science and behavioural analyses, robotics, and their associated theoretical formalisations, speaks to the importance of the world-model learning and inference. In this article, after briefly surveying the history and challenges of internal model learning and probabilistic learning, we introduce the free energy principle, which provides a useful framework within which to consider neuronal computation and probabilistic world models. Next, we showcase examples of human behaviour and cognition explained under that principle. We then describe symbol emergence in the context of probabilistic modelling, as a topic at the frontiers of cognitive robotics. Lastly, we review recent progress in creating human-like intelligence by using novel probabilistic programming languages. The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human-like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world.},
	language = {en},
	urldate = {2022-03-10},
	journal = {Neural Networks},
	author = {Friston, Karl and Moran, Rosalyn J. and Nagai, Yukie and Taniguchi, Tadahiro and Gomi, Hiroaki and Tenenbaum, Josh},
	month = dec,
	year = {2021},
	pages = {573--590},
}

@article{shepard_toward_1987,
	title = {Toward a {Universal} {Law} of {Generalization} for {Psychological} {Science}},
	volume = {237},
	url = {https://www.science.org/doi/10.1126/science.3629243},
	doi = {10.1126/science.3629243},
	number = {4820},
	urldate = {2022-03-08},
	journal = {Science},
	author = {Shepard, Roger N.},
	month = sep,
	year = {1987},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1317--1323},
}

@article{hinton_mapping_1990,
	title = {Mapping {Part}-{Whole} {Hierarchies} into {Connectionist} {Networks}},
	doi = {10.1016/0004-3702(90)90004-J},
	abstract = {Semantic Scholar extracted view of "Mapping Part-Whole Hierarchies into Connectionist Networks" by Geoffrey E. Hinton},
	journal = {Artif. Intell.},
	author = {Hinton, Geoffrey E.},
	year = {1990},
}

@inproceedings{hinton_representing_1988,
	title = {Representing {Part}-{Whole} {Hierarchies} in {Connectionist} {Networks}},
	booktitle = {Proceedings of the 10th {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	publisher = {Hillsdale, NJ: Erlbaum},
	author = {Hinton, Geoffrey E.},
	year = {1988},
}

@article{noauthor_notitle_nodate,
}

@misc{noauthor_representing_nodate,
	title = {Representing {Part}-{Whole} {Hierarchies} in {Connectionist} {Networks} {\textbar} {BibSonomy}},
	url = {https://www.bibsonomy.org/bibtex/154b50bdf646aa57be0824687d8288abe/idsia},
	urldate = {2022-03-07},
}

@article{frankland_concepts_2020,
	title = {Concepts and {Compositionality}: {In} {Search} of the {Brain}'s {Language} of {Thought}},
	volume = {71},
	issn = {1545-2085},
	shorttitle = {Concepts and {Compositionality}},
	doi = {10.1146/annurev-psych-122216-011829},
	abstract = {Imagine Genghis Khan, Aretha Franklin, and the Cleveland Cavaliers performing an opera on Maui. This silly sentence makes a serious point: As humans, we can flexibly generate and comprehend an unbounded number of complex ideas. Little is known, however, about how our brains accomplish this. Here we assemble clues from disparate areas of cognitive neuroscience, integrating recent research on language, memory, episodic simulation, and computational models of high-level cognition. Our review is framed by Fodor's classic language of thought hypothesis, according to which our minds employ an amodal, language-like system for combining and recombining simple concepts to form more complex thoughts. Here, we highlight emerging work on combinatorial processes in the brain and consider this work's relation to the language of thought. We review evidence for distinct, but complementary, contributions of map-like representations in subregions of the default mode network and sentence-like representations of conceptual relations in regions of the temporal and prefrontal cortex.},
	language = {eng},
	journal = {Annual Review of Psychology},
	author = {Frankland, Steven M. and Greene, Joshua D.},
	month = jan,
	year = {2020},
	pmid = {31550985},
	pages = {273--303},
}

@article{kennedy_color_2010,
	title = {Color, context, and compositionality},
	volume = {174},
	issn = {0039-7857},
	url = {https://www.jstor.org/stable/40587017},
	abstract = {Color adjectives have played a central role in work on language typology and variation, but there has been relatively little investigation of their meanings by researchers in formal semantics. This is surprising given the fact that color terms have been at the center of debates in the philosophy of language over foundational questions, in particular whether the idea of a compositional, truth-conditional theory of natural language semantics is even coherent. The challenge presented by color terms is articulated in detail in the work of Charles Travis. Travis argues that structurally isomorphic sentences containing color adjectives can shift truth value from context to context depending on how they are used and in the absence of effects of vagueness or ambiguity/polysemy, and concludes that a deterministic mapping from structures to truth conditions is impossible. The goal of this paper is to provide a linguistic perspective on this issue, which we believe defuses Travis' challenge. We provide empirical arguments that color adjectives are in fact ambiguous between gradable and nongradable interpretations, and that this simple ambiguity, together with independently motivated options concerning scalar dimension within the gradable reading accounts for the Travis facts in a simpler, more constrained, and thus ultimately more successful fashion than recent contextualist analyses such as those in Szabó (Perspectives on semantics, pragmatics and discourse: A festschrift for Ferenc Kiefer, 2001) or Rothschild and Segal (Mind Lang, 2009).},
	number = {1},
	urldate = {2022-03-03},
	journal = {Synthese},
	author = {Kennedy, Christopher and McNally, Louise},
	year = {2010},
	note = {Publisher: Springer},
	pages = {79--98},
}

@article{holliday_perception_2021,
	title = {Perception in {Black} and {White}: {Effects} of {Intonational} {Variables} and {Filtering} {Conditions} on {Sociolinguistic} {Judgments} {With} {Implications} for {ASR}},
	volume = {4},
	issn = {2624-8212},
	shorttitle = {Perception in {Black} and {White}},
	url = {https://www.frontiersin.org/article/10.3389/frai.2021.642783},
	abstract = {This study tests the effects of intonational contours and filtering conditions on listener judgments of ethnicity to arrive at a more comprehensive understanding on how prosody influences these judgments, with implications for austomatic speech recognition systems as well as speech synthesis. In a perceptual experiment, 40 American English listeners heard phrase-long clips which were controlled for pitch accent type and focus marking. Each clip contained either two H* (high) or two L+H* (low high) pitch accents and a L-L\% (falling) boundary tone, and had also previously been labelled for broad or narrow focus. Listeners rated clips in two tasks, one with unmodified stimuli and one with stimuli lowpass filtered at 400 Hz, and were asked to judge whether the speaker was “Black” or “White”. In the filtered condition, tokens with the L+H* pitch accent were more likely to be rated as “Black”, with an interaction such that broad focus enhanced this pattern, supporting earlier findings that listeners may perceive African American Language as having more variation in possible pitch accent meanings. In the unfiltered condition, tokens with the L+H* pitch accent were less likely to be rated as Black, with no effect of focus, likely due to the fact that listeners relied more heavily on available segmental information in this condition. These results enhance our understanding of cues listeners rely on in making social judgments about speakers, especially in ethnic identification and linguistic profiling, by highlighting perceptual differences due to listening environment as well as predicted meaning of specific intonational contours. They also contribute to our understanding of the role of how human listeners interpret meaning within a holistic context, which has implications for the construction of computational systems designed to replicate the properties of natural language. In particular, they have important applicability to speech synthesis and speech recognition programs, which are often limited in their capacities due to the fact that they do not make such holistic sociolinguistic considerations of the meanings of input or output speech.},
	urldate = {2022-02-25},
	journal = {Frontiers in Artificial Intelligence},
	author = {Holliday, Nicole R.},
	year = {2021},
}

@article{hinton_how_2021,
	title = {How to represent part-whole hierarchies in a neural network},
	url = {http://arxiv.org/abs/2102.12627},
	abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
	urldate = {2022-02-24},
	journal = {arXiv:2102.12627 [cs]},
	author = {Hinton, Geoffrey},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.12627},
}

@misc{pelletier_content_2007,
	title = {Content, {Context} and {Composition}},
	url = {https://era.library.ualberta.ca/items/a0ac24c2-e384-4ad6-a72d-ef4b9a8dcb10},
	abstract = {Introduction: It is traditional, at least since Grice, to make a distinction between what is called the literal meaning of an utterance...},
	language = {en},
	urldate = {2022-02-24},
	journal = {ERA},
	author = {Pelletier, Francis J. and Pagin, Peter},
	year = {2007},
	doi = {10.7939/R3BR8MW8J},
}

@article{andreas_task-oriented_2020,
	title = {Task-{Oriented} {Dialogue} as {Dataflow} {Synthesis}},
	volume = {8},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00333},
	doi = {10.1162/tacl_a_00333},
	abstract = {We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, SMCalFlow, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the MultiWOZ dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The SMCalFlow dataset, code for replicating experiments, and a public leaderboard are available at https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines.},
	urldate = {2022-02-11},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Andreas, Jacob and Bufe, John and Burkett, David and Chen, Charles and Clausman, Josh and Crawford, Jean and Crim, Kate and DeLoach, Jordan and Dorner, Leah and Eisner, Jason and Fang, Hao and Guo, Alan and Hall, David and Hayes, Kristin and Hill, Kellie and Ho, Diana and Iwaszuk, Wendy and Jha, Smriti and Klein, Dan and Krishnamurthy, Jayant and Lanman, Theo and Liang, Percy and Lin, Christopher H. and Lintsbakh, Ilya and McGovern, Andy and Nisnevich, Aleksandr and Pauls, Adam and Petters, Dmitrij and Read, Brent and Roth, Dan and Roy, Subhro and Rusak, Jesse and Short, Beth and Slomin, Div and Snyder, Ben and Striplin, Stephon and Su, Yu and Tellman, Zachary and Thomson, Sam and Vorobev, Andrei and Witoszko, Izabela and Wolfe, Jason and Wray, Abby and Zhang, Yuchen and Zotov, Alexander},
	month = sep,
	year = {2020},
	pages = {556--571},
}

@article{albright_rules_2003,
	title = {Rules vs. analogy in {English} past tenses: a computational/experimental study},
	volume = {90},
	issn = {0010-0277},
	shorttitle = {Rules vs. analogy in {English} past tenses},
	url = {https://www.sciencedirect.com/science/article/pii/S001002770300146X},
	doi = {10.1016/S0010-0277(03)00146-X},
	abstract = {Are morphological patterns learned in the form of rules? Some models deny this, attributing all morphology to analogical mechanisms. The dual mechanism model (Pinker, S., \& Prince, A. (1998). On language and connectionism: analysis of a parallel distributed processing model of language acquisition. Cognition, 28, 73–193) posits that speakers do internalize rules, but that these rules are few and cover only regular processes; the remaining patterns are attributed to analogy. This article advocates a third approach, which uses multiple stochastic rules and no analogy. We propose a model that employs inductive learning to discover multiple rules, and assigns them confidence scores based on their performance in the lexicon. Our model is supported over the two alternatives by new “wug test” data on English past tenses, which show that participant ratings of novel pasts depend on the phonological shape of the stem, both for irregulars and, surprisingly, also for regulars. The latter observation cannot be explained under the dual mechanism approach, which derives all regulars with a single rule. To evaluate the alternative hypothesis that all morphology is analogical, we implemented a purely analogical model, which evaluates novel pasts based solely on their similarity to existing verbs. Tested against experimental data, this analogical model also failed in key respects: it could not locate patterns that require abstract structural characterizations, and it favored implausible responses based on single, highly similar exemplars. We conclude that speakers extend morphological patterns based on abstract structural properties, of a kind appropriately described with rules.},
	language = {en},
	number = {2},
	urldate = {2022-02-11},
	journal = {Cognition},
	author = {Albright, Adam and Hayes, Bruce},
	month = dec,
	year = {2003},
	pages = {119--161},
}

@article{piantadosi_computational_2021,
	title = {The {Computational} {Origin} of {Representation}},
	volume = {31},
	issn = {1572-8641},
	url = {https://doi.org/10.1007/s11023-020-09540-9},
	doi = {10.1007/s11023-020-09540-9},
	abstract = {Each of our theories of mental representation provides some insight into how the mind works. However, these insights often seem incompatible, as the debates between symbolic, dynamical, emergentist, sub-symbolic, and grounded approaches to cognition attest. Mental representations—whatever they are—must share many features with each of our theories of representation, and yet there are few hypotheses about how a synthesis could be possible. Here, I develop a theory of the underpinnings of symbolic cognition that shows how sub-symbolic dynamics may give rise to higher-level cognitive representations of structures, systems of knowledge, and algorithmic processes. This theory implements a version of conceptual role semantics by positing an internal universal representation language in which learners may create mental models to capture dynamics they observe in the world. The theory formalizes one account of how truly novel conceptual content may arise, allowing us to explain how even elementary logical and computational operations may be learned from a more primitive basis. I provide an implementation that learns to represent a variety of structures, including logic, number, kinship trees, regular languages, context-free languages, domains of theories like magnetism, dominance hierarchies, list structures, quantification, and computational primitives like repetition, reversal, and recursion. This account is based on simple discrete dynamical processes that could be implemented in a variety of different physical or biological systems. In particular, I describe how the required dynamics can be directly implemented in a connectionist framework. The resulting theory provides an “assembly language” for cognition, where high-level theories of symbolic computation can be implemented in simple dynamics that themselves could be encoded in biologically plausible systems.},
	language = {en},
	number = {1},
	urldate = {2022-02-11},
	journal = {Minds and Machines},
	author = {Piantadosi, Steven T.},
	month = mar,
	year = {2021},
	pages = {1--58},
}

@article{thoppilan_lamda_2022,
	title = {{LaMDA}: {Language} {Models} for {Dialog} {Applications}},
	shorttitle = {{LaMDA}},
	url = {http://arxiv.org/abs/2201.08239},
	abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
	urldate = {2022-02-11},
	journal = {arXiv:2201.08239 [cs]},
	author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
	month = feb,
	year = {2022},
	note = {arXiv: 2201.08239
version: 3},
}

@article{askell_general_2021,
	title = {A {General} {Language} {Assistant} as a {Laboratory} for {Alignment}},
	url = {http://arxiv.org/abs/2112.00861},
	abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
	urldate = {2022-02-11},
	journal = {arXiv:2112.00861 [cs]},
	author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.00861},
}

@article{dyer_transition-based_2015,
	title = {Transition-{Based} {Dependency} {Parsing} with {Stack} {Long} {Short}-{Term} {Memory}},
	url = {http://arxiv.org/abs/1505.08075},
	abstract = {We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.},
	urldate = {2022-02-11},
	journal = {arXiv:1505.08075 [cs]},
	author = {Dyer, Chris and Ballesteros, Miguel and Ling, Wang and Matthews, Austin and Smith, Noah A.},
	month = may,
	year = {2015},
	note = {arXiv: 1505.08075},
}

@article{de_marneffe_universal_2021,
	title = {Universal {Dependencies}},
	volume = {47},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00402},
	doi = {10.1162/coli_a_00402},
	abstract = {Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate–argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.},
	number = {2},
	urldate = {2022-02-11},
	journal = {Computational Linguistics},
	author = {de Marneffe, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
	month = jul,
	year = {2021},
	pages = {255--308},
}

@article{goodwin_compositional_2021,
	title = {Compositional {Generalization} in {Dependency} {Parsing}},
	url = {http://arxiv.org/abs/2110.06843},
	abstract = {Compositionality, or the ability to combine familiar units like words into novel phrases and sentences, has been the focus of intense interest in artificial intelligence in recent years. To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). This dataset maximizes the similarity between the test and train distributions over primitive units, like words, while maximizing the compound divergence: the dissimilarity between test and train distributions over larger structures, like phrases. Dependency parsing, however, lacks a compositional generalization benchmark. In this work, we introduce a gold-standard set of dependency parses for CFQ, and use this to analyze the behavior of a state-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We find that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance. Additionally, we find the performance of the dependency parser does not uniformly degrade relative to compound divergence, and the parser performs differently on different splits with the same compound divergence. We explore a number of hypotheses for what causes the non-uniform degradation in dependency parsing performance, and identify a number of syntactic structures that drive the dependency parser's lower performance on the most challenging splits.},
	urldate = {2022-02-11},
	journal = {arXiv:2110.06843 [cs]},
	author = {Goodwin, Emily and Reddy, Siva and O'Donnell, Timothy J. and Bahdanau, Dzmitry},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.06843},
}

@article{mittal_compositional_2021,
	title = {Compositional {Attention}: {Disentangling} {Search} and {Retrieval}},
	shorttitle = {Compositional {Attention}},
	url = {http://arxiv.org/abs/2110.09419},
	abstract = {Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.},
	urldate = {2022-02-11},
	journal = {arXiv:2110.09419 [cs]},
	author = {Mittal, Sarthak and Raparthy, Sharath Chandra and Rish, Irina and Bengio, Yoshua and Lajoie, Guillaume},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.09419},
}

@article{zadrozny_compositional_1994,
	title = {From {Compositional} to {Systematic} {Semantics}},
	volume = {17},
	issn = {0165-0157},
	url = {https://www.jstor.org/stable/25001552},
	abstract = {We prove a theorem stating that any semantics can be encoded as a compositional semantics, which means that, essentially, the standard definition of compositionality is formally vacuous. We then show that when compositional semantics is required to be "systematic" (that is, the meaning function cannot be arbitrary, but must belong to some class), it is possible to distinguish between compositional and noncompositional semantics. As a result, we believe that the paper clarifies the concept of compositionality and opens the possibility of making systematic formal comparisons of different systems of grammar.},
	number = {4},
	urldate = {2022-02-10},
	journal = {Linguistics and Philosophy},
	author = {Zadrozny, Wlodek},
	year = {1994},
	note = {Publisher: Springer},
	pages = {329--342},
}

@inproceedings{ratnaparkhi_maximum_1994,
	title = {A {Maximum} {Entropy} {Model} for {Prepositional} {Phrase} {Attachment}},
	url = {https://aclanthology.org/H94-1048},
	urldate = {2021-12-21},
	booktitle = {Human {Language} {Technology}: {Proceedings} of a {Workshop} held at {Plainsboro}, {New} {Jersey}, {March} 8-11, 1994},
	author = {Ratnaparkhi, Adwait and Reynar, Jeff and Roukos, Salim},
	year = {1994},
}

@article{higgins_machine_2003,
	title = {A {Machine} {Learning} {Approach} to {Modeling} {Scope} {Preferences}},
	volume = {29},
	url = {https://aclanthology.org/J03-1004},
	doi = {10.1162/089120103321337449},
	number = {1},
	urldate = {2021-12-21},
	journal = {Computational Linguistics},
	author = {Higgins, Derrick and Sadock, Jerrold M.},
	year = {2003},
	pages = {73--96},
}

@article{williams_broad-coverage_2018,
	title = {A {Broad}-{Coverage} {Challenge} {Corpus} for {Sentence} {Understanding} through {Inference}},
	url = {http://arxiv.org/abs/1704.05426},
	abstract = {This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.},
	urldate = {2021-12-21},
	journal = {arXiv:1704.05426 [cs]},
	author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R.},
	month = feb,
	year = {2018},
	note = {arXiv: 1704.05426},
}

@article{wang_superglue_2020,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2021-12-21},
	journal = {arXiv:1905.00537 [cs]},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv: 1905.00537},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2021-12-21},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2021-12-21},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
}

@techreport{jiahui_not_2021,
	title = {Not so fast: {Limited} validity of deep convolutional neural networks as in silico models for human naturalistic face processing},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Not so fast},
	url = {https://www.biorxiv.org/content/10.1101/2021.11.17.469009v1},
	abstract = {Deep convolutional neural networks (DCNNs) trained for face identification can rival and even exceed human-level performance. The relationships between internal representations learned by DCNNs and those of the primate face processing system are not well understood, especially in naturalistic settings. We developed the largest naturalistic dynamic face stimulus set in human neuroimaging research (700+ naturalistic video clips of unfamiliar faces) and used representational similarity analysis to investigate how well the representations learned by high-performing DCNNs match human brain representations across the entire distributed face processing system. DCNN representational geometries were strikingly consistent across diverse architectures and captured meaningful variance among faces. Similarly, representational geometries throughout the human face network were highly consistent across subjects. Nonetheless, correlations between DCNN and neural representations were very weak overall—DCNNs captured 3\% of variance in the neural representational geometries at best. Intermediate DCNN layers better matched visual and face-selective cortices than the final fully-connected layers. Behavioral ratings of face similarity were highly correlated with intermediate layers of DCNNs, but also failed to capture representational geometry in the human brain. Our results suggest that the correspondence between intermediate DCNN layers and neural representations of naturalistic human face processing is weak at best, and diverges even further in the later fully-connected layers. This poor correspondence can be attributed, at least in part, to the dynamic and cognitive information that plays an essential role in human face processing but is not modeled by DCNNs. These mismatches indicate that current DCNNs have limited validity as in silico models of dynamic, naturalistic face processing in humans.},
	language = {en},
	urldate = {2021-12-11},
	author = {Jiahui, Guo and Feilong, Ma and Castello, Matteo Visconti di Oleggio and Nastase, Samuel A. and Haxby, James V. and Gobbini, M. Ida},
	month = nov,
	year = {2021},
	doi = {10.1101/2021.11.17.469009},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2021.11.17.469009},
}

@article{freiwald_functional_2010,
	title = {Functional {Compartmentalization} and {Viewpoint} {Generalization} {Within} the {Macaque} {Face}-{Processing} {System}},
	volume = {330},
	url = {https://www.science.org/doi/10.1126/science.1194908},
	doi = {10.1126/science.1194908},
	number = {6005},
	urldate = {2021-12-09},
	journal = {Science},
	author = {Freiwald, Winrich A. and Tsao, Doris Y.},
	month = nov,
	year = {2010},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {845--851},
}

@article{kleyko_vector_2021,
	title = {Vector {Symbolic} {Architectures} as a {Computing} {Framework} for {Nanoscale} {Hardware}},
	url = {http://arxiv.org/abs/2106.05268},
	abstract = {This article reviews recent progress in the development of the computing framework Vector Symbolic Architectures (also known as Hyperdimensional Computing). This framework is well suited for implementation in stochastic, nanoscale hardware and it naturally expresses the types of cognitive operations required for Artificial Intelligence (AI). We demonstrate in this article that the ring-like algebraic structure of Vector Symbolic Architectures offers simple but powerful operations on high-dimensional vectors that can support all data structures and manipulations relevant in modern computing. In addition, we illustrate the distinguishing feature of Vector Symbolic Architectures, "computing in superposition," which sets it apart from conventional computing. This latter property opens the door to efficient solutions to the difficult combinatorial search problems inherent in AI applications. Vector Symbolic Architectures are Turing complete, as we show, and we see them acting as a framework for computing with distributed representations in myriad AI settings. This paper serves as a reference for computer architects by illustrating techniques and philosophy of VSAs for distributed computing and relevance to emerging computing hardware, such as neuromorphic computing.},
	urldate = {2021-09-16},
	journal = {arXiv:2106.05268 [cs]},
	author = {Kleyko, Denis and Davies, Mike and Frady, E. Paxon and Kanerva, Pentti and Kent, Spencer J. and Olshausen, Bruno A. and Osipov, Evgeny and Rabaey, Jan M. and Rachkovskij, Dmitri A. and Rahimi, Abbas and Sommer, Friedrich T.},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05268},
}

@techreport{wurm_two_2021,
	title = {Two "what" pathways for action and object recognition},
	url = {https://psyarxiv.com/af65s/},
	abstract = {The ventral visual stream is conceived as a pathway for object recognition. However, we also recognize the actions an object can be involved in. Here, we show that action recognition critically depends on a pathway in lateral occipitotemporal cortex, partially overlapping and topographically aligned with object representations that are precursors for action recognition. By contrast, object features that are more relevant for object recognition, such as color and texture, are typically found in ventral occipitotemporal cortex. We argue that occipitotemporal cortex contains similarly organized lateral and ventral "what" pathways for action and object recognition, respectively. This account explains a number of observed phenomena, such as the duplication of object domains and the specific representational profiles in lateral and ventral cortex.},
	urldate = {2021-09-16},
	institution = {PsyArXiv},
	author = {Wurm, Moritz and Caramazza, Alfonso},
	month = apr,
	year = {2021},
	doi = {10.31234/osf.io/af65s},
	note = {type: article},
}

@techreport{chang_explaining_2021,
	title = {Explaining face representation in the primate brain using different computational models},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2020.06.07.111930v2},
	abstract = {Understanding how the brain represents the identity of complex objects is a central challenge of visual neuroscience. The principles governing object processing have been extensively studied in the macaque face patch system, a sub-network of inferotemporal (IT) cortex specialized for face processing. A previous study reported that single face patch neurons encode axes of a generative model called the “active appearance” model, which transforms 50-d feature vectors separately representing facial shape and facial texture into facial images. However, a systematic investigation comparing this model to other computational models, especially convolutional neural network models that have shown success in explaining neural responses in the ventral visual stream, has been lacking. Here, we recorded responses of cells in the most anterior face patch AM to a large set of real face images and compared a large number of models for explaining neural responses. We found that the active appearance model better explained responses than any other model except CORnet-Z, a feedforward deep neural network trained on general object classification to classify non-face images, whose performance it tied on some face image sets and exceeded on others. Surprisingly, deep neural networks trained specifically on facial identification did not explain neural responses well. A major reason is that units in the network, unlike neurons, are less modulated by face-related factors unrelated to facial identification such as illumination.},
	language = {en},
	urldate = {2021-08-24},
	author = {Chang, Le and Egger, Bernhard and Vetter, Thomas and Tsao, Doris Y.},
	month = mar,
	year = {2021},
	doi = {10.1101/2020.06.07.111930},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.06.07.111930},
}

@article{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2021-08-02},
	journal = {arXiv:1711.00937 [cs]},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv: 1711.00937},
}

@article{chaudhari_attentive_2021,
	title = {An {Attentive} {Survey} of {Attention} {Models}},
	url = {http://arxiv.org/abs/1904.02874},
	abstract = {Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy which groups existing techniques into coherent categories. We review salient neural architectures in which attention has been incorporated, and discuss applications in which modeling attention has shown a significant impact. We also describe how attention has been used to improve the interpretability of neural networks. Finally, we discuss some future research directions in attention. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.},
	urldate = {2021-08-02},
	journal = {arXiv:1904.02874 [cs, stat]},
	author = {Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor and Ramanath, Rohan},
	month = jul,
	year = {2021},
	note = {arXiv: 1904.02874},
}

@article{martins_softmax_2016,
	title = {From {Softmax} to {Sparsemax}: {A} {Sparse} {Model} of {Attention} and {Multi}-{Label} {Classification}},
	shorttitle = {From {Softmax} to {Sparsemax}},
	url = {http://arxiv.org/abs/1602.02068},
	abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
	urldate = {2021-08-02},
	journal = {arXiv:1602.02068 [cs, stat]},
	author = {Martins, André F. T. and Astudillo, Ramón Fernandez},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02068},
}

@article{roy_efficient_2020,
	title = {Efficient {Content}-{Based} {Sparse} {Attention} with {Routing} {Transformers}},
	url = {http://arxiv.org/abs/2003.05997},
	abstract = {Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to \$O{\textbackslash}left(n{\textasciicircum}\{1.5\}d{\textbackslash}right)\$ from \$O{\textbackslash}left(n{\textasciicircum}2d{\textbackslash}right)\$ for sequence length \$n\$ and hidden dimension \$d\$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.},
	urldate = {2021-08-02},
	journal = {arXiv:2003.05997 [cs, eess, stat]},
	author = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
	month = oct,
	year = {2020},
	note = {arXiv: 2003.05997},
}

@article{martins_sparse_nodate,
	title = {Sparse and {Continuous} {Attention} {Mechanisms}},
	abstract = {Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have ﬁxed support. In contrast, for ﬁnite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and αentmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: ﬁrst, we extend α-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efﬁcient gradient backpropagation algorithms for α ∈ \{1, 2\}. Experiments on attention-based text classiﬁcation, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.},
	language = {en},
	author = {Martins, André F T and Farinhas, António and Treviso, Marcos},
	pages = {13},
}

@article{julian_algorithmic_2012,
	title = {An algorithmic method for functionally defining regions of interest in the ventral visual pathway},
	volume = {60},
	issn = {1095-9572},
	doi = {10.1016/j.neuroimage.2012.02.055},
	abstract = {In a widely used functional magnetic resonance imaging (fMRI) data analysis method, functional regions of interest (fROIs) are handpicked in each participant using macroanatomic landmarks as guides, and the response of these regions to new conditions is then measured. A key limitation of this standard handpicked fROI method is the subjectivity of decisions about which clusters of activated voxels should be treated as the particular fROI in question in each subject. Here we apply the Group-Constrained Subject-Specific (GSS) method for defining fROIs, recently developed for identifying language fROIs (Fedorenko et al., 2010), to algorithmically identify fourteen well-studied category-selective regions of the ventral visual pathway (Kanwisher, 2010). We show that this method retains the benefit of defining fROIs in individual subjects without the subjectivity inherent in the traditional handpicked fROI approach. The tools necessary for using this method are available on our website (http://web.mit.edu/bcs/nklab/GSS.shtml).},
	language = {eng},
	number = {4},
	journal = {NeuroImage},
	author = {Julian, J. B. and Fedorenko, Evelina and Webster, Jason and Kanwisher, Nancy},
	month = may,
	year = {2012},
	pmid = {22398396},
	pages = {2357--2364},
}

@article{ridgeway_learning_2018,
	title = {Learning {Deep} {Disentangled} {Embeddings} with the {F}-{Statistic} {Loss}},
	url = {http://arxiv.org/abs/1802.05312},
	abstract = {Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the \$F\$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@\$k\$ and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.},
	urldate = {2021-07-09},
	journal = {arXiv:1802.05312 [cs, stat]},
	author = {Ridgeway, Karl and Mozer, Michael C.},
	month = may,
	year = {2018},
	note = {arXiv: 1802.05312},
}

@inproceedings{eastwood_framework_2018,
	title = {A {Framework} for the {Quantitative} {Evaluation} of {Disentangled} {Representations}},
	url = {https://openreview.net/forum?id=By-7dz-AZ},
	abstract = {Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors  behind data. Despite the growing interest in models which can learn such...},
	language = {en},
	urldate = {2021-07-09},
	author = {Eastwood, Cian and Williams, Christopher K. I.},
	month = feb,
	year = {2018},
}

@article{carstensen_graded_2021,
	series = {Same-different conceptualization},
	title = {Do graded representations support abstract thought?},
	volume = {37},
	issn = {2352-1546},
	url = {https://www.sciencedirect.com/science/article/pii/S2352154620301546},
	doi = {10.1016/j.cobeha.2020.10.009},
	abstract = {Relational reasoning requires the reasoner to go beyond her/his specific experience, abstracting from items to make inferences about categories and kinds on the basis of structural or analogical similarities. Reasoning about the relations same and different is one of the best-studied cases of relational reasoning, both across species and over human development, and has become a paradigm case study for abstract representation. However, decades of careful study have nonetheless produced seemingly contradictory findings — with surprising successes and puzzling failures — in both the comparative and developmental literatures. In this article, we review these literatures and suggest first steps toward a reconciliation of these contradictions by suggesting that same-different reasoning is supported by graded representations.},
	language = {en},
	urldate = {2021-06-16},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Carstensen, Alexandra and Frank, Michael C},
	month = feb,
	year = {2021},
	pages = {90--97},
}

@article{al-tahan_reconstructing_2021,
	title = {Reconstructing feedback representations in the ventral visual pathway with a generative adversarial autoencoder},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008775},
	doi = {10.1371/journal.pcbi.1008775},
	abstract = {While vision evokes a dense network of feedforward and feedback neural processes in the brain, visual processes are primarily modeled with feedforward hierarchical neural networks, leaving the computational role of feedback processes poorly understood. Here, we developed a generative autoencoder neural network model and adversarially trained it on a categorically diverse data set of images. We hypothesized that the feedback processes in the ventral visual pathway can be represented by reconstruction of the visual information performed by the generative model. We compared representational similarity of the activity patterns in the proposed model with temporal (magnetoencephalography) and spatial (functional magnetic resonance imaging) visual brain responses. The proposed generative model identified two segregated neural dynamics in the visual brain. A temporal hierarchy of processes transforming low level visual information into high level semantics in the feedforward sweep, and a temporally later dynamics of inverse processes reconstructing low level visual information from a high level latent representation in the feedback sweep. Our results append to previous studies on neural feedback processes by presenting a new insight into the algorithmic function and the information carried by the feedback processes in the ventral visual pathway.},
	language = {en},
	number = {3},
	urldate = {2021-06-10},
	journal = {PLOS Computational Biology},
	author = {Al-Tahan, Haider and Mohsenzadeh, Yalda},
	month = mar,
	year = {2021},
	note = {Publisher: Public Library of Science},
	pages = {e1008775},
}

@incollection{searle_minds_nodate,
	title = {Minds, {Brain}, and {Computers}},
	author = {Searle},
}

@inproceedings{duan_unsupervised_2019,
	title = {Unsupervised {Model} {Selection} for {Variational} {Disentangled} {Representation} {Learning}},
	url = {https://openreview.net/forum?id=SyxL2TNtvr},
	abstract = {We introduce a method for unsupervised disentangled model selection for VAE-based disentangled representation learning approaches.},
	language = {en},
	urldate = {2021-05-12},
	author = {Duan, Sunny and Matthey, Loic and Saraiva, Andre and Watters, Nick and Burgess, Chris and Lerchner, Alexander and Higgins, Irina},
	month = sep,
	year = {2019},
}

@article{anderson_newell_2003,
	title = {The {Newell} {Test} for a theory of cognition},
	volume = {26},
	doi = {10.1017/S0140525X0300013X},
	number = {5},
	journal = {Behavioral and Brain Sciences},
	author = {Anderson, John R. and Lebiere, Christian},
	year = {2003},
	note = {Publisher: Cambridge University Press},
	pages = {587--601},
}

@incollection{rickford_john_creole_nodate,
	title = {The {Creole} {Origins} {Hypothesis}},
	author = {Rickford, John},
}

@article{sandler_emergence_2005,
	title = {The emergence of grammar: {Systematic} structure in a new language},
	volume = {102},
	copyright = {Copyright © 2005, The National Academy of Sciences.                    Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	shorttitle = {The emergence of grammar},
	url = {https://www.pnas.org/content/102/7/2661},
	doi = {10.1073/pnas.0405448102},
	abstract = {This report contains a linguistic description of a language created spontaneously without any apparent external influence in a stable existing community. We describe the syntactic structure of Al-Sayyid Bedouin Sign Language, a language that has arisen in the last 70 years in an isolated endogamous community with a high incidence of nonsyndromic, genetically recessive, profound prelingual neurosensory deafness. In the space of one generation from its inception, systematic grammatical structure has emerged in the language. Going beyond a conventionalized list of words for actions, objects, people, characteristics, and so on, a systematic way of marking the grammatical relations among those elements has appeared in the form of highly regular word order. These systematic structures cannot be attributed to influence from other languages, because the particular word orders that appear in Al-Sayyid Bedouin Sign Language differ from those found both in the ambient spoken languages in the community and in the other sign language found predominantly in the surrounding area. Therefore, the emerging grammatical structures should be regarded as an independent development within the language.},
	language = {en},
	number = {7},
	urldate = {2021-05-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Sandler, Wendy and Meir, Irit and Padden, Carol and Aronoff, Mark},
	month = feb,
	year = {2005},
	pmid = {15699343},
	note = {Publisher: National Academy of Sciences
Section: Social Sciences},
	pages = {2661--2665},
}

@inproceedings{mostafazadeh_lsdsem_2017,
	address = {Valencia, Spain},
	title = {{LSDSem} 2017 {Shared} {Task}: {The} {Story} {Cloze} {Test}},
	shorttitle = {{LSDSem} 2017 {Shared} {Task}},
	url = {https://www.aclweb.org/anthology/W17-0906},
	doi = {10.18653/v1/W17-0906},
	abstract = {The LSDSem'17 shared task is the Story Cloze Test, a new evaluation for story understanding and script learning. This test provides a system with a four-sentence story and two possible endings, and the system must choose the correct ending to the story. Successful narrative understanding (getting closer to human performance of 100\%) requires systems to link various levels of semantics to commonsense knowledge. A total of eight systems participated in the shared task, with a variety of approaches including.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 2nd {Workshop} on {Linking} {Models} of {Lexical}, {Sentential} and {Discourse}-level {Semantics}},
	publisher = {Association for Computational Linguistics},
	author = {Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
	month = apr,
	year = {2017},
	pages = {46--51},
}

@inproceedings{mostafazadeh_corpus_2016,
	address = {San Diego, California},
	title = {A {Corpus} and {Cloze} {Evaluation} for {Deeper} {Understanding} of {Commonsense} {Stories}},
	url = {https://www.aclweb.org/anthology/N16-1098},
	doi = {10.18653/v1/N16-1098},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
	month = jun,
	year = {2016},
	pages = {839--849},
}

@article{p_cognitiveevolutionary_2017,
	title = {Cognitive/{Evolutionary} {Psychology} and the {History} of {Racism}},
	volume = {84},
	doi = {10.1086/690720},
	number = {2},
	journal = {Philosophy of Science},
	author = {P, Jackson Jr John},
	year = {2017},
	note = {Publisher: University of Chicago Press},
	pages = {296--314},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? \&\#x1f99c;},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://doi.org/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2021-04-26},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@article{hamrick_levels_2020,
	title = {Levels of {Analysis} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/2004.05107},
	abstract = {Machine learning is currently involved in some of the most vigorous debates it has ever seen. Such debates often seem to go around in circles, reaching no conclusion or resolution. This is perhaps unsurprising given that researchers in machine learning come to these discussions with very different frames of reference, making it challenging for them to align perspectives and find common ground. As a remedy for this dilemma, we advocate for the adoption of a common conceptual framework which can be used to understand, analyze, and discuss research. We present one such framework which is popular in cognitive science and neuroscience and which we believe has great utility in machine learning as well: Marr's levels of analysis. Through a series of case studies, we demonstrate how the levels facilitate an understanding and dissection of several methods from machine learning. By adopting the levels of analysis in one's own work, we argue that researchers can be better equipped to engage in the debates necessary to drive forward progress in our field.},
	urldate = {2021-04-14},
	journal = {arXiv:2004.05107 [cs, stat]},
	author = {Hamrick, Jessica and Mohamed, Shakir},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.05107},
}

@article{perfors_learnability_2011,
	title = {The learnability of abstract syntactic principles},
	volume = {118},
	issn = {0010-0277},
	url = {https://www.sciencedirect.com/science/article/pii/S0010027710002593},
	doi = {10.1016/j.cognition.2010.11.001},
	abstract = {Children acquiring language infer the correct form of syntactic constructions for which they appear to have little or no direct evidence, avoiding simple but incorrect generalizations that would be consistent with the data they receive. These generalizations must be guided by some inductive bias – some abstract knowledge – that leads them to prefer the correct hypotheses even in the absence of directly supporting evidence. What form do these inductive constraints take? It is often argued or assumed that they reflect innately specified knowledge of language. A classic example of such an argument moves from the phenomenon of auxiliary fronting in English interrogatives to the conclusion that children must innately know that syntactic rules are defined over hierarchical phrase structures rather than linear sequences of words (e.g., Chomsky, 1965, Chomsky, 1971, Chomsky, 1980, Crain and Nakayama, 1987). Here we use a Bayesian framework for grammar induction to address a version of this argument and show that, given typical child-directed speech and certain innate domain-general capacities, an ideal learner could recognize the hierarchical phrase structure of language without having this knowledge innately specified as part of the language faculty. We discuss the implications of this analysis for accounts of human language acquisition.},
	language = {en},
	number = {3},
	urldate = {2021-04-13},
	journal = {Cognition},
	author = {Perfors, Amy and Tenenbaum, Joshua B. and Regier, Terry},
	month = mar,
	year = {2011},
	pages = {306--338},
}

@article{hobbs_toward_2005,
	title = {Toward a {Useful} {Concept} of {Causality} for {Lexical} {Semantics}},
	doi = {10.1093/jos/ffh024},
	abstract = {We do things in the world by exploiting our knowledge of what causes what. But in trying to reason formally about causality, there is a difficulty: to reason with certainty we need complete knowledge of all the relevant events and circumstances, whereas in everyday reasoning tasks we need a more serviceable but looser notion that does not make such demands on our knowledge. In this work the notion of'causal complex' is introduced for a complete set of events and conditions necessary for the causal consequent to occur, and the term 'cause' is used for the makeshift, nonmonotonic notion we require for everyday tasks such as planning and language understanding. Like all interesting concepts, neither of these can be defined with necessary and sufficient conditions, but they can be more or less tightly constrained by necessary conditions or sufficient conditions. The issue of how to distinguish between what is in a causal complex from what is outside it is discussed, and within a causal complex, how to distinguish the eventualities that deserve to be called 'causes' from those that do not, in particular circumstances. One particular modal, the word 'would', is examined from the standpoint of its underlying causal content, as a linguistic motivation for this enterprise.},
	journal = {J. Semant.},
	author = {Hobbs, Jerry R.},
	year = {2005},
}

@inproceedings{thomason_formal_2010,
	title = {Formal {Semantics} for {Causal} {Constructions}},
	doi = {10.1093/acprof:oso/9780199672073.003.0003},
	abstract = {Montague’s framework for semantic interpretation has always been less well adapted to the interpretation of words than of syntactic constructions. In the late 1970s, David Dowty addressed this problem, concentrating on the interpretation of tense, aspect, inchoatives, and causatives in an extension of Montague’s Intensional Logic. In this paper I will try to revive this project, conceiving it as part of a larger task aiming at the interpretation of derivational morphology. I suggest an alternative to Dowty’s approach that, while it does not provide a global interpretation of causality, seems to work well with a wide range of the causal constructions that are important in word formation.},
	author = {Thomason, R.},
	year = {2010},
}

@inproceedings{schank_scripts_1975,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'75},
	title = {Scripts, {Plans}, and {Knowledge}},
	abstract = {We describe a theoretical system intended to facilitate the use of knowledge In an understanding system. The notion of script is introduced to account for knowledge about mundane situations. A program, SAM, is capable of using scripts to understand. The notion of plans is introduced to account for general knowledge about novel situations.},
	booktitle = {Proceedings of the 4th {International} {Joint} {Conference} on {Artificial} {Intelligence} - {Volume} 1},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Schank, Roger C. and Abelson, Robert P.},
	year = {1975},
	note = {event-place: Tblisi, USSR},
	pages = {151--157},
}

@article{gopnik_reconstructing_2012,
	title = {Reconstructing constructivism: causal models, {Bayesian} learning mechanisms, and the theory theory.},
	volume = {138 6},
	journal = {Psychological bulletin},
	author = {Gopnik, A. and Wellman, H.},
	year = {2012},
	pages = {1085--108},
}

@article{pdtb_group_penn_nodate,
	title = {The {Penn} {Discourse} {Treebank} 2.0 {Annotation} {Manual}},
	author = {PDTB Group},
}

@inproceedings{prasad_penn_2008,
	address = {Marrakech, Morocco},
	title = {The {Penn} {Discourse} {TreeBank} 2.0.},
	url = {http://www.lrec-conf.org/proceedings/lrec2008/pdf/754_paper.pdf},
	abstract = {We present the second version of the Penn Discourse Treebank, PDTB-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between PDTB-1.0 and PDTB-2.0. We present representative statistics for several aspects of the annotation in the corpus.},
	urldate = {2021-04-06},
	booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'08)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Prasad, Rashmi and Dinesh, Nikhil and Lee, Alan and Miltsakaki, Eleni and Robaldo, Livio and Joshi, Aravind and Webber, Bonnie},
	month = may,
	year = {2008},
}

@inproceedings{vashishtha_fine-grained_2019,
	address = {Florence, Italy},
	title = {Fine-{Grained} {Temporal} {Relation} {Extraction}},
	url = {https://www.aclweb.org/anthology/P19-1280},
	doi = {10.18653/v1/P19-1280},
	abstract = {We present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to real-valued scales. We use this framework to construct the largest temporal relations dataset to date, covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. We report strong results on our data and show the efficacy of a transfer-learning approach for predicting categorical relations.},
	urldate = {2021-04-06},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Vashishtha, Siddharth and Van Durme, Benjamin and White, Aaron Steven},
	month = jul,
	year = {2019},
	pages = {2906--2919},
}

@article{pustejovsky_james_timebank_nodate,
	title = {The {TIMEBANK} {Corpus}},
	author = {Pustejovsky, James},
}

@article{wolfson_break_2020,
	title = {Break {It} {Down}: {A} {Question} {Understanding} {Benchmark}},
	volume = {8},
	shorttitle = {Break {It} {Down}},
	url = {https://www.aclweb.org/anthology/2020.tacl-1.13},
	doi = {10.1162/tacl_a_00309},
	abstract = {Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines.},
	urldate = {2021-04-03},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Wolfson, Tomer and Geva, Mor and Gupta, Ankit and Gardner, Matt and Goldberg, Yoav and Deutch, Daniel and Berant, Jonathan},
	year = {2020},
	pages = {183--198},
}

@article{michael_crowdsourcing_2017,
	title = {Crowdsourcing {Question}-{Answer} {Meaning} {Representations}},
	url = {http://arxiv.org/abs/1711.05885},
	abstract = {We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We also develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions. A detailed qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including PropBank, NomBank, QA-SRL, and AMR) along with many previously under-resourced ones, including implicit arguments and relations. The QAMR data and annotation code is made publicly available to enable future work on how best to model these complex phenomena.},
	urldate = {2021-04-03},
	journal = {arXiv:1711.05885 [cs]},
	author = {Michael, Julian and Stanovsky, Gabriel and He, Luheng and Dagan, Ido and Zettlemoyer, Luke},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.05885},
}

@article{pustejovsky_temporal_2005,
	title = {Temporal and {Event} {Information} in {Natural} {Language} {Text}},
	volume = {39},
	issn = {1574-020X},
	url = {http://www.jstor.org/stable/30200549},
	abstract = {In this paper, we discuss the role that temporal information plays in natural language text, specifically in the context of question answering systems. We define a descriptive framework with which we can examine the temporally sensitive aspects of natural language queries. We then investigate broadly what properties a general specification language would need, in order to mark up temporal and event information in text. We present a language, TimeML, which attempts to capture the richness of temporal and event related information in language, while demonstrating how it can play an important part in the development of more robust question answering systems.},
	number = {2/3},
	urldate = {2021-03-31},
	journal = {Language Resources and Evaluation},
	author = {Pustejovsky, James and Knippen, Robert and Littman, Jessica and Saurí, Roser},
	year = {2005},
	note = {Publisher: Springer},
	pages = {123--164},
}

@inproceedings{uzzaman_semeval-2013_2013,
	address = {Atlanta, Georgia, USA},
	title = {{SemEval}-2013 {Task} 1: {TempEval}-3: {Evaluating} {Time} {Expressions}, {Events}, and {Temporal} {Relations}},
	shorttitle = {{SemEval}-2013 {Task} 1},
	url = {https://www.aclweb.org/anthology/S13-2001},
	urldate = {2021-03-31},
	booktitle = {Second {Joint} {Conference} on {Lexical} and {Computational} {Semantics} (*{SEM}), {Volume} 2: {Proceedings} of the {Seventh} {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2013)},
	publisher = {Association for Computational Linguistics},
	author = {UzZaman, Naushad and Llorens, Hector and Derczynski, Leon and Allen, James and Verhagen, Marc and Pustejovsky, James},
	month = jun,
	year = {2013},
	pages = {1--9},
}

@inproceedings{wang_emergent_2017,
	address = {Vancouver, Canada},
	title = {Emergent {Predication} {Structure} in {Hidden} {State} {Vectors} of {Neural} {Readers}},
	url = {https://www.aclweb.org/anthology/W17-2604},
	doi = {10.18653/v1/W17-2604},
	abstract = {A significant number of neural architectures for reading comprehension have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of “predication structure” in the hidden state vectors of these readers. More specifically, we provide evidence that the hidden state vectors represent atomic formulas \${\textbackslash}Phi[c]\$ where Φ is a semantic property (predicate) and \$c\$ is a constant symbol entity identifier.},
	urldate = {2021-03-23},
	booktitle = {Proceedings of the 2nd {Workshop} on {Representation} {Learning} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Hai and Onishi, Takeshi and Gimpel, Kevin and McAllester, David},
	month = aug,
	year = {2017},
	pages = {26--36},
}

@article{von_stechow_tenses_nodate,
	title = {{TENSES} {IN} {COMPOSITIONAL} {SEMANTIC}},
	author = {VON STECHOW},
}

@book{parsons_events_nodate,
	title = {Events in the {Semantics} of {English}},
	author = {Parsons},
}

@inproceedings{jiang_you_2019,
	address = {Florence, Italy},
	title = {Do {You} {Know} {That} {Florence} {Is} {Packed} with {Visitors}? {Evaluating} {State}-of-the-art {Models} of {Speaker} {Commitment}},
	shorttitle = {Do {You} {Know} {That} {Florence} {Is} {Packed} with {Visitors}?},
	url = {https://www.aclweb.org/anthology/P19-1412},
	doi = {10.18653/v1/P19-1412},
	abstract = {When a speaker, Mary, asks “Do you know that Florence is packed with visitors?”, we take her to believe that Florence is packed with visitors, but not if she asks “Do you think that Florence is packed with visitors?”. Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The CommitmentBank is annotated with speaker commitment towards the content of the complement (“Florence is packed with visitors” in our example) of clause-embedding verbs (“know”, “think”) under four entailment-canceling environments (negation, modal, question, conditional). A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.},
	urldate = {2021-03-13},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Nanjiang and de Marneffe, Marie-Catherine},
	month = jul,
	year = {2019},
	pages = {4208--4213},
}

@article{sauri_factbank_nodate,
	title = {{FactBank}: a corpus annotated with event factuality},
	author = {Sauri and Pustejovsky},
}

@article{fedorenko_lack_2020,
	title = {Lack of selectivity for syntax relative to word meanings throughout the language network},
	volume = {203},
	issn = {0010-0277},
	url = {https://www.sciencedirect.com/science/article/pii/S0010027720301670},
	doi = {10.1016/j.cognition.2020.104348},
	abstract = {To understand what you are reading now, your mind retrieves the meanings of words and constructions from a linguistic knowledge store (lexico-semantic processing) and identifies the relationships among them to construct a complex meaning (syntactic or combinatorial processing). Do these two sets of processes rely on distinct, specialized mechanisms or, rather, share a common pool of resources? Linguistic theorizing, empirical evidence from language acquisition and processing, and computational modeling have jointly painted a picture whereby lexico-semantic and syntactic processing are deeply inter-connected and perhaps not separable. In contrast, many current proposals of the neural architecture of language continue to endorse a view whereby certain brain regions selectively support syntactic/combinatorial processing, although the locus of such “syntactic hub”, and its nature, vary across proposals. Here, we searched for selectivity for syntactic over lexico-semantic processing using a powerful individual-subjects fMRI approach across three sentence comprehension paradigms that have been used in prior work to argue for such selectivity: responses to lexico-semantic vs. morpho-syntactic violations (Experiment 1); recovery from neural suppression across pairs of sentences differing in only lexical items vs. only syntactic structure (Experiment 2); and same/different meaning judgments on such sentence pairs (Experiment 3). Across experiments, both lexico-semantic and syntactic conditions elicited robust responses throughout the left fronto-temporal language network. Critically, however, no regions were more strongly engaged by syntactic than lexico-semantic processing, although some regions showed the opposite pattern. Thus, contra many current proposals of the neural architecture of language, syntactic/combinatorial processing is not separable from lexico-semantic processing at the level of brain regions—or even voxel subsets—within the language network, in line with strong integration between these two processes that has been consistently observed in behavioral and computational language research. The results further suggest that the language network may be generally more strongly concerned with meaning than syntactic form, in line with the primary function of language—to share meanings across minds.},
	language = {en},
	urldate = {2021-03-09},
	journal = {Cognition},
	author = {Fedorenko, Evelina and Blank, Idan Asher and Siegelman, Matthew and Mineroff, Zachary},
	month = oct,
	year = {2020},
	pages = {104348},
}

@article{kanwisher_fusiform_2006,
	title = {The fusiform face area: a cortical region specialized for the perception of faces},
	volume = {361},
	issn = {0962-8436},
	shorttitle = {The fusiform face area},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1857737/},
	doi = {10.1098/rstb.2006.1934},
	abstract = {Faces are among the most important visual stimuli we perceive, informing us not only about a person's identity, but also about their mood, sex, age and direction of gaze. The ability to extract this information within a fraction of a second of viewing a face is important for normal social interactions and has probably played a critical role in the survival of our primate ancestors. Considerable evidence from behavioural, neuropsychological and neurophysiological investigations supports the hypothesis that humans have specialized cognitive and neural mechanisms dedicated to the perception of faces (the face-specificity hypothesis). Here, we review the literature on a region of the human brain that appears to play a key role in face perception, known as the fusiform face area (FFA).,  outlines the theoretical background for much of this work. The face-specificity hypothesis falls squarely on one side of a longstanding debate in the fields of cognitive science and cognitive neuroscience concerning the extent to which the mind/brain is composed of: (i) special-purpose (‘domain-specific’) mechanisms, each dedicated to processing a specific kind of information (e.g. faces, according to the face-specificity hypothesis), versus (ii) general-purpose (‘domain-general’) mechanisms, each capable of operating on any kind of information. Face perception has long served both as one of the prime candidates of a domain-specific process and as a key target for attack by proponents of domain-general theories of brain and mind.  briefly reviews the prior literature on face perception from behaviour and neurophysiology. This work supports the face-specificity hypothesis and argues against its domain-general alternatives (the individuation hypothesis, the expertise hypothesis and others).,  outlines the more recent evidence on this debate from brain imaging, focusing particularly on the FFA. We review the evidence that the FFA is selectively engaged in face perception, by addressing (and rebutting) five of the most widely discussed alternatives to this hypothesis. In , we consider recent findings that are beginning to provide clues into the computations conducted in the FFA and the nature of the representations the FFA extracts from faces. We argue that the FFA is engaged both in detecting faces and in extracting the necessary perceptual information to recognize them, and that the properties of the FFA mirror previously identified behavioural signatures of face-specific processing (e.g. the face-inversion effect).,  asks how the computations and representations in the FFA differ from those occurring in other nearby regions of cortex that respond strongly to faces and objects. The evidence indicates clear functional dissociations between these regions, demonstrating that the FFA shows not only functional specificity but also area specificity. We end by speculating in  on some of the broader questions raised by current research on the FFA, including the developmental origins of this region and the question of whether faces are unique versus whether similarly specialized mechanisms also exist for other domains of high-level perception and cognition.},
	number = {1476},
	urldate = {2021-03-09},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Kanwisher, Nancy and Yovel, Galit},
	month = dec,
	year = {2006},
	pmid = {17118927},
	pmcid = {PMC1857737},
	pages = {2109--2128},
}

@article{stengel-eskin_elias_joint_nodate,
	title = {Joint {Universal} {Syntactic} and {Semantic} {Parsing}},
	author = {Stengel-Eskin, Elias},
}

@inproceedings{niklaus_survey_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {A {Survey} on {Open} {Information} {Extraction}},
	url = {https://www.aclweb.org/anthology/C18-1326},
	abstract = {We provide a detailed overview of the various approaches that were proposed to date to solve the task of Open Information Extraction. We present the major challenges that such systems face, show the evolution of the suggested approaches over time and depict the specific issues they address. In addition, we provide a critique of the commonly applied evaluation procedures for assessing the performance of Open IE systems and highlight some directions for future work.},
	urldate = {2021-03-01},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Niklaus, Christina and Cetto, Matthias and Freitas, André and Handschuh, Siegfried},
	month = aug,
	year = {2018},
	pages = {3866--3878},
}

@inproceedings{rudinger_is_2014,
	address = {Baltimore, Maryland, USA},
	title = {Is the {Stanford} {Dependency} {Representation} {Semantic}?},
	url = {https://www.aclweb.org/anthology/W14-2908},
	doi = {10.3115/v1/W14-2908},
	urldate = {2021-03-01},
	booktitle = {Proceedings of the {Second} {Workshop} on {EVENTS}: {Definition}, {Detection}, {Coreference}, and {Representation}},
	publisher = {Association for Computational Linguistics},
	author = {Rudinger, Rachel and Van Durme, Benjamin},
	month = jun,
	year = {2014},
	pages = {54--58},
}

@inproceedings{banarescu_abstract_2013,
	address = {Sofia, Bulgaria},
	title = {Abstract {Meaning} {Representation} for {Sembanking}},
	url = {https://www.aclweb.org/anthology/W13-2322},
	urldate = {2021-03-01},
	booktitle = {Proceedings of the 7th {Linguistic} {Annotation} {Workshop} and {Interoperability} with {Discourse}},
	publisher = {Association for Computational Linguistics},
	author = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
	month = aug,
	year = {2013},
	pages = {178--186},
}

@inproceedings{ebner_multi-sentence_2020,
	address = {Online},
	title = {Multi-{Sentence} {Argument} {Linking}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.718},
	doi = {10.18653/v1/2020.acl-main.718},
	abstract = {We present a novel document-level model for finding argument spans that fill an event's roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.},
	urldate = {2021-03-01},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ebner, Seth and Xia, Patrick and Culkin, Ryan and Rawlins, Kyle and Van Durme, Benjamin},
	month = jul,
	year = {2020},
	pages = {8057--8077},
}

@inproceedings{hobbs_ontological_1985,
	address = {Chicago, Illinois, USA},
	title = {Ontological {Promiscuity}},
	url = {https://www.aclweb.org/anthology/P85-1008},
	doi = {10.3115/981210.981218},
	urldate = {2021-02-22},
	booktitle = {23rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hobbs, Jerry R.},
	month = jul,
	year = {1985},
	pages = {60--69},
}

@inproceedings{sundheim_overview_1995,
	title = {Overview of {Results} of the {MUC}-6 {Evaluation}},
	url = {https://www.aclweb.org/anthology/M95-1002},
	urldate = {2021-02-22},
	booktitle = {Sixth {Message} {Understanding} {Conference} ({MUC}-6): {Proceedings} of a {Conference} {Held} in {Columbia}, {Maryland}, {November} 6-8, 1995},
	author = {Sundheim, Beth M.},
	year = {1995},
}

@incollection{schubert_situations_2000,
	address = {Boston, MA},
	series = {The {Springer} {International} {Series} in {Engineering} and {Computer} {Science}},
	title = {The {Situations} {We} {Talk} {About}},
	isbn = {978-1-4615-1567-8},
	url = {https://doi.org/10.1007/978-1-4615-1567-8_18},
	abstract = {I argue in favor of associating situations (events, episodes, eventualities, etc.) with arbitrarily complex sentences, not just atomic predicates, in NL interpretation. In that respect, a Situation Semantics approach to incorporating situations into semantic representations is preferable to a Davidsonian one. However, I will further argue that beyond the notion of truth or falsity of a sentence in a situation, as in Situation Semantics, we also need the notion of a sentence characterizing a situation, in order to deal adequately with causal relations mentioned or implied in NL texts. I propose a way of doing this that essentially reduces complex situations to joins of basic, Davidsonian ones, along with basic situations corresponding to negated predications. The resulting situational logic, called FOL**, captures many of the essential features of both Davidsonian and Situation Semantics approaches to representing the content of sentences describing situations. The proposed semantics supports common intuitions about truth-in-situations, about the existence of situations characterized by sentences, and about persistence of information from parts of situations to the whole. I allow for temporal parts of situations as well as concurrent parts, and distinguish persistence properties of telic and atelic sentences. The development of FOL** is part of a continuing effort to fully formalize Episodic Logic, an implemented knowledge representation designed to support language understanding.},
	language = {en},
	urldate = {2021-02-22},
	booktitle = {Logic-{Based} {Artificial} {Intelligence}},
	publisher = {Springer US},
	author = {Schubert, Lenhart K.},
	editor = {Minker, Jack},
	year = {2000},
	doi = {10.1007/978-1-4615-1567-8_18},
	pages = {407--439},
}

@inproceedings{he_question-answer_2015,
	address = {Lisbon, Portugal},
	title = {Question-{Answer} {Driven} {Semantic} {Role} {Labeling}: {Using} {Natural} {Language} to {Annotate} {Natural} {Language}},
	shorttitle = {Question-{Answer} {Driven} {Semantic} {Role} {Labeling}},
	url = {https://www.aclweb.org/anthology/D15-1076},
	doi = {10.18653/v1/D15-1076},
	urldate = {2021-02-18},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
	month = sep,
	year = {2015},
	pages = {643--653},
}

@inproceedings{abend_state_2017,
	address = {Vancouver, Canada},
	title = {The {State} of the {Art} in {Semantic} {Representation}},
	url = {https://www.aclweb.org/anthology/P17-1008},
	doi = {10.18653/v1/P17-1008},
	abstract = {Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.},
	urldate = {2021-02-15},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Abend, Omri and Rappoport, Ari},
	month = jul,
	year = {2017},
	pages = {77--89},
}

@article{schuler_verbnetabroad-coveragecomprehensiveverblexicon_nodate,
	title = {{VERBNET}:{ABROAD}-{COVERAGE},{COMPREHENSIVEVERBLEXICON}},
	author = {Schuler},
}

@article{henderson_pluractionality_nodate,
	title = {Pluractionality and distributivity},
	author = {Henderson},
}

@article{krifka_four_1990,
	title = {Four thousand ships passed through the lock: object-induced measure functions on events},
	author = {Krifka},
	year = {1990},
}

@article{carlson_thematic_nodate,
	title = {Thematic roles and their role in semantic interpretation},
	author = {Carlson},
}

@article{dowty_semantic_nodate,
	title = {On the semantic content of the notion `thematic role'},
	author = {Dowty},
}

@incollection{parsons_terence_events_nodate,
	title = {Events in the {Semantics} of {English}},
	author = {Parsons, Terence},
}

@article{davidson_donald_logical_nodate,
	title = {The {Logical} {Form} of {Action} {Sentences}},
	author = {Davidson, Donald},
}

@article{champollion_interaction_2015,
	title = {The interaction of compositional semantics and event semantics},
	volume = {38},
	issn = {1573-0549},
	url = {https://doi.org/10.1007/s10988-014-9162-8},
	doi = {10.1007/s10988-014-9162-8},
	abstract = {Davidsonian event semantics is often taken to form an unhappy marriage with compositional semantics. For example, it has been claimed to be problematic for semantic accounts of quantification (Beaver and Condoravdi, in: Aloni et al. (eds.) Proceedings of the 16th Amsterdam Colloquium, 2007), for classical accounts of negation (Krifka, in: Bartsch et al. (eds.) Semantics and contextual expression, 1989), and for intersective accounts of verbal coordination (Lasersohn, in Plurality, conjunction and events, 1995). This paper shows that none of this is the case, once we abandon the idea that the event variable is bound at sentence level, and assume instead that verbs denote existential quantifiers over events. Quantificational arguments can then be given a semantic account, negation can be treated classically, and coordination can be modeled as intersection. The framework presented here is a natural choice for researchers and fieldworkers who wish to sketch a semantic analysis of a language without being forced to make commitments about the hierarchical order of arguments, the argument-adjunct distinction, the default scope of quantifiers, or the nature of negation and coordination.},
	language = {en},
	number = {1},
	urldate = {2021-02-09},
	journal = {Linguistics and Philosophy},
	author = {Champollion, Lucas},
	month = feb,
	year = {2015},
	pages = {31--66},
}

@incollection{kratzer_situations_2020,
	edition = {Fall 2020},
	title = {Situations in {Natural} {Language} {Semantics}},
	url = {https://plato.stanford.edu/archives/fall2020/entries/situations-semantics/},
	abstract = {Situation semantics was developed as an alternative to possible worldssemantics. In situation semantics, linguistic expressions areevaluated with respect to partial, rather than complete, worlds. Thereis no consensus about what situations are, just as there is noconsensus about what possible worlds or events are. According to some,situations are structured entities consisting of relations andindividuals standing in those relations. According to others,situations are particulars. In spite of unresolved foundationalissues, the partiality provided by situation semantics has led to somegenuinely new approaches to a variety of phenomena in natural languagesemantics. In the way of illustration, this article includesrelatively detailed overviews of a few selected areas where situationsemantics has been successful: implicit quantifier domainrestrictions, donkey pronouns, and exhaustive interpretations. Itmoreover addresses the question of how Davidsonian event semantics canbe embedded in a semantics based on situations. Other areas where asituation semantics perspective has led to progress include attitudeascriptions, questions, tense, aspect, nominalizations, implicitarguments, point of view, counterfactual conditionals, and discourserelations.},
	urldate = {2021-02-09},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Kratzer, Angelika},
	editor = {Zalta, Edward N.},
	year = {2020},
}

@article{pustejovsky_syntax_1991,
	title = {The syntax of event structure},
	volume = {41},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/001002779190032Y},
	doi = {10.1016/0010-0277(91)90032-Y},
	abstract = {Pustejovsky, J., 1991. The syntax of event structure. Cognition, 41: 47-81 In this paper we examine the role of events within a theory of lexical semantics. We propose a configurational theory of event structure and examine how it contributes to a lexical semantic theory for natural language. In particular, we argue that an event structure can provide a distinct and useful level of representation for linguistic analysis involving the aspectual properties of verbs, adverbial scope, the role of argument structure, and the mapping from the lexicon to syntax.},
	language = {en},
	number = {1-3},
	urldate = {2021-02-07},
	journal = {Cognition},
	author = {Pustejovsky, James},
	month = dec,
	year = {1991},
	pages = {47--81},
}

@article{bach_algebra_1986,
	title = {The {Algebra} of {Events}},
	volume = {9},
	issn = {0165-0157},
	url = {https://www.jstor.org/stable/25001229},
	number = {1},
	urldate = {2021-02-07},
	journal = {Linguistics and Philosophy},
	author = {Bach, Emmon},
	year = {1986},
	note = {Publisher: Springer},
	pages = {5--16},
}

@article{reisinger_semantic_2015,
	title = {Semantic {Proto}-{Roles}},
	volume = {3},
	url = {https://www.aclweb.org/anthology/Q15-1034},
	doi = {10.1162/tacl_a_00152},
	abstract = {We present the first large-scale, corpus based verification of Dowty's seminal theory of proto-roles. Our results demonstrate both the need for and the feasibility of a property-based annotation scheme of semantic relationships, as opposed to the currently dominant notion of categorical roles.},
	urldate = {2021-02-02},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Reisinger, Drew and Rudinger, Rachel and Ferraro, Francis and Harman, Craig and Rawlins, Kyle and Van Durme, Benjamin},
	year = {2015},
	pages = {475--488},
}

@article{palmer_proposition_2005,
	title = {The {Proposition} {Bank}: {An} {Annotated} {Corpus} of {Semantic} {Roles}},
	volume = {31},
	issn = {0891-2017},
	shorttitle = {The {Proposition} {Bank}},
	url = {https://doi.org/10.1162/0891201053630264},
	doi = {10.1162/0891201053630264},
	abstract = {The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank.},
	number = {1},
	urldate = {2021-01-30},
	journal = {Computational Linguistics},
	author = {Palmer, Martha and Gildea, Daniel and Kingsbury, Paul},
	month = mar,
	year = {2005},
	pages = {71--106},
}

@inproceedings{baker_berkeley_1998,
	address = {USA},
	series = {{ACL} '98/{COLING} '98},
	title = {The {Berkeley} {FrameNet} {Project}},
	url = {https://doi.org/10.3115/980845.980860},
	doi = {10.3115/980845.980860},
	abstract = {FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, "Tools for Lexicon Building"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between "frame elements" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.},
	urldate = {2021-01-30},
	booktitle = {Proceedings of the 36th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and 17th {International} {Conference} on {Computational} {Linguistics} - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Baker, Collin F. and Fillmore, Charles J. and Lowe, John B.},
	month = aug,
	year = {1998},
	pages = {86--90},
}

@inproceedings{white_universal_2020,
	address = {Marseille, France},
	title = {The {Universal} {Decompositional} {Semantics} {Dataset} and {Decomp} {Toolkit}},
	isbn = {979-10-95546-34-4},
	url = {https://www.aclweb.org/anthology/2020.lrec-1.699},
	abstract = {We present the Universal Decompositional Semantics (UDS) dataset (v1.0), which is bundled with the Decomp toolkit (v0.1). UDS1.0 unifies five high-quality, decompositional semantics-aligned annotation sets within a single semantic graph specification—with graph structures defined by the predicative patterns produced by the PredPatt tool and real-valued node and edge attributes constructed using sophisticated normalization procedures. The Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using SPARQL. Both UDS1.0 and Decomp0.1 are publicly available at http://decomp.io.},
	language = {English},
	urldate = {2021-01-30},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {White, Aaron Steven and Stengel-Eskin, Elias and Vashishtha, Siddharth and Govindarajan, Venkata Subrahmanyan and Reisinger, Dee Ann and Vieira, Tim and Sakaguchi, Keisuke and Zhang, Sheng and Ferraro, Francis and Rudinger, Rachel and Rawlins, Kyle and Van Durme, Benjamin},
	month = may,
	year = {2020},
	pages = {5698--5707},
}

@article{chang_code_2017,
	title = {The {Code} for {Facial} {Identity} in the {Primate} {Brain}},
	volume = {169},
	issn = {0092-8674},
	url = {http://www.sciencedirect.com/science/article/pii/S009286741730538X},
	doi = {10.1016/j.cell.2017.05.011},
	abstract = {Primates recognize complex objects such as faces with remarkable speed and reliability. Here, we reveal the brain’s code for facial identity. Experiments in macaques demonstrate an extraordinarily simple transformation between faces and responses of cells in face patches. By formatting faces as points in a high-dimensional linear space, we discovered that each face cell’s firing rate is proportional to the projection of an incoming face stimulus onto a single axis in this space, allowing a face cell ensemble to encode the location of any face in the space. Using this code, we could precisely decode faces from neural population responses and predict neural firing rates to faces. Furthermore, this code disavows the long-standing assumption that face cells encode specific facial identities, confirmed by engineering faces with drastically different appearance that elicited identical responses in single face cells. Our work suggests that other objects could be encoded by analogous metric coordinate systems.
PaperClip},
	language = {en},
	number = {6},
	urldate = {2021-01-26},
	journal = {Cell},
	author = {Chang, Le and Tsao, Doris Y.},
	month = jun,
	year = {2017},
	pages = {1013--1028.e14},
}

@article{duchaine_revised_2015,
	title = {A {Revised} {Neural} {Framework} for {Face} {Processing}},
	volume = {1},
	issn = {2374-4642},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-vision-082114-035518},
	doi = {10.1146/annurev-vision-082114-035518},
	abstract = {Face perception relies on computations carried out in face-selective cortical areas. These areas have been intensively investigated for two decades, and this work has been guided by an influential neural model suggested by Haxby and colleagues in 2000. Here, we review new findings about face-selective areas that suggest the need for modifications and additions to the Haxby model. We suggest a revised framework based on (a) evidence for multiple routes from early visual areas into the face-processing system, (b) information about the temporal characteristics of these areas, (c) indications that the fusiform face area contributes to the perception of changeable aspects of faces, (d) the greatly elevated responses to dynamic compared with static faces in dorsal face-selective brain areas, and (e) the identification of three new anterior face-selective areas. Together, these findings lead us to suggest that face perception depends on two separate pathways: a ventral stream that represents form information and a dorsal stream driven by motion and form information.},
	number = {1},
	urldate = {2021-01-15},
	journal = {Annual Review of Vision Science},
	author = {Duchaine, Brad and Yovel, Galit},
	month = nov,
	year = {2015},
	note = {Publisher: Annual Reviews},
	pages = {393--416},
}

@article{rule_child_2020,
	title = {The {Child} as {Hacker}},
	volume = {24},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661320301741},
	doi = {10.1016/j.tics.2020.07.005},
	abstract = {The scope of human learning and development poses a radical challenge for cognitive science. We propose that developmental theories can address this challenge by adopting perspectives from computer science. Many of our best models treat learning as analogous to computer programming because symbolic programs provide the most compelling account of sophisticated mental representations. We specifically propose that children’s learning is analogous to a particular style of programming called hacking, making code better along many dimensions through an open-ended set of goals and activities. By contrast to existing theories, which depend primarily on local search and simple metrics, this view highlights the many features of good mental representations and the multiple complementary processes children use to create them.},
	language = {en},
	number = {11},
	urldate = {2020-12-29},
	journal = {Trends in Cognitive Sciences},
	author = {Rule, Joshua S. and Tenenbaum, Joshua B. and Piantadosi, Steven T.},
	month = nov,
	year = {2020},
	pages = {900--915},
}

@article{shiffrin_brain_2020,
	title = {The brain produces mind by modeling},
	volume = {117},
	copyright = {© 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/47/29299},
	doi = {10.1073/pnas.1912340117},
	abstract = {The connection of brain and mind has been a source of intense speculation at least since humanity became aware that the brain was the source of our behavior. Brain refers to the neurons, cells, and chemicals that govern activities of the organism. Mind is often considered consciously aware perceptions and thoughts. However, there is a gradient from unconscious to conscious, demonstrated by enormous amounts of research, such as the effects upon behavior of subliminal primes, so that mind is best considered to be the conscious and unconscious processes that act as an intermediate stage between the organism’s biology and its behavior, or a translation from one to the other.

The National Academy of Sciences Colloquium “Brain Produces Mind by Modeling” was held May 1–3, 2019 at the Arnold and Mabel Beckman Center of the National Academy of Sciences in Irvine, CA. It was organized by Richard M. Shiffrin, Danielle S. Bassett, Nikolaus Kriegeskorte, and Joshua B. Tenenbaum. The theme of the colloquium and the foundation for the set of articles in this issue of PNAS is that the “mind” consists of a model formed by the “brain”: This would be a model of the entire environment, including the self, the body, the physical environment, other agents, and the social environment. Furthermore, the model would be a best guess about the most likely state of this environment. It uses this model to learn, decide, attend, remember, perceive, predict, and produce action. This model develops as the brain matures, rapidly during infancy and more slowly later. It has structural components that remain stable over long times. It has labile elements that change at multiple time scales, adapting to the current environment and goals. The mind’s formation through modeling of the world might be likened to the way scientists build models: through a … 

[↵][1]1To whom correspondence may be addressed. Email: shiffrin\{at\}indiana.edu.

 [1]: \#xref-corresp-1-1},
	language = {en},
	number = {47},
	urldate = {2020-12-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Shiffrin, Richard M. and Bassett, Danielle S. and Kriegeskorte, Nikolaus and Tenenbaum, Joshua B.},
	month = nov,
	year = {2020},
	pmid = {33229525},
	note = {Publisher: National Academy of Sciences
Section: Introduction},
	pages = {29299--29301},
}

@article{makin_machine_2020,
	title = {Machine translation of cortical activity to text with an encoder–decoder framework},
	volume = {23},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-020-0608-8},
	doi = {10.1038/s41593-020-0608-8},
	abstract = {A decade after speech was first decoded from human brain signals, accuracy and speed remain far below that of natural speech. Here we show how to decode the electrocorticogram with high accuracy and at natural-speech rates. Taking a cue from recent advances in machine translation, we train a recurrent neural network to encode each sentence-length sequence of neural activity into an abstract representation, and then to decode this representation, word by word, into an English sentence. For each participant, data consist of several spoken repeats of a set of 30–50 sentences, along with the contemporaneous signals from {\textasciitilde}250 electrodes distributed over peri-Sylvian cortices. Average word error rates across a held-out repeat set are as low as 3\%. Finally, we show how decoding with limited data can be improved with transfer learning, by training certain layers of the network under multiple participants’ data.},
	language = {en},
	number = {4},
	urldate = {2020-12-27},
	journal = {Nature Neuroscience},
	author = {Makin, Joseph G. and Moses, David A. and Chang, Edward F.},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Nature Publishing Group},
	pages = {575--582},
}

@article{de_vries_towards_2020,
	title = {Towards {Ecologically} {Valid} {Research} on {Language} {User} {Interfaces}},
	url = {http://arxiv.org/abs/2007.14435},
	abstract = {Language User Interfaces (LUIs) could improve human-machine interaction for a wide variety of tasks, such as playing music, getting insights from databases, or instructing domestic robots. In contrast to traditional hand-crafted approaches, recent work attempts to build LUIs in a data-driven way using modern deep learning methods. To satisfy the data needs of such learning algorithms, researchers have constructed benchmarks that emphasize the quantity of collected data at the cost of its naturalness and relevance to real-world LUI use cases. As a consequence, research findings on such benchmarks might not be relevant for developing practical LUIs. The goal of this paper is to bootstrap the discussion around this issue, which we refer to as the benchmarks' low ecological validity. To this end, we describe what we deem an ideal methodology for machine learning research on LUIs and categorize five common ways in which recent benchmarks deviate from it. We give concrete examples of the five kinds of deviations and their consequences. Lastly, we offer a number of recommendations as to how to increase the ecological validity of machine learning research on LUIs.},
	urldate = {2020-12-27},
	journal = {arXiv:2007.14435 [cs]},
	author = {de Vries, Harm and Bahdanau, Dzmitry and Manning, Christopher},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.14435},
}

@article{garcez_neural-symbolic_nodate,
	title = {Neural-{Symbolic} {Learning} and {Reasoning}: {Contributions} and {Challenges}},
	abstract = {The goal of neural-symbolic computation is to integrate robust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in particular deep neural networks, forms of representation learning have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
	language = {en},
	author = {Garcez},
	pages = {4},
}

@article{besold_neural-symbolic_2017,
	title = {Neural-{Symbolic} {Learning} and {Reasoning}: {A} {Survey} and {Interpretation}},
	shorttitle = {Neural-{Symbolic} {Learning} and {Reasoning}},
	url = {http://arxiv.org/abs/1711.03902},
	abstract = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
	urldate = {2020-12-23},
	journal = {arXiv:1711.03902 [cs]},
	author = {Besold, Tarek R. and Garcez, Artur d'Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.03902},
}

@article{lamb_graph_2020,
	title = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}: {A} {Survey} and {Perspective}},
	shorttitle = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/2003.00330},
	abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNN) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientific domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as its relationship to current developments in neural-symbolic computing.},
	urldate = {2020-12-23},
	journal = {arXiv:2003.00330 [cs]},
	author = {Lamb, Luis C. and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
	month = may,
	year = {2020},
	note = {arXiv: 2003.00330},
}

@article{bahdanau_systematic_2019,
	title = {Systematic {Generalization}: {What} {Is} {Required} and {Can} {It} {Be} {Learned}?},
	shorttitle = {Systematic {Generalization}},
	url = {http://arxiv.org/abs/1811.12889},
	abstract = {Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.},
	urldate = {2020-12-23},
	journal = {arXiv:1811.12889 [cs]},
	author = {Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and de Vries, Harm and Courville, Aaron},
	month = apr,
	year = {2019},
	note = {arXiv: 1811.12889},
}

@article{garcez_neurosymbolic_2020,
	title = {Neurosymbolic {AI}: {The} 3rd {Wave}},
	shorttitle = {Neurosymbolic {AI}},
	url = {http://arxiv.org/abs/2012.05876},
	abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
	urldate = {2020-12-23},
	journal = {arXiv:2012.05876 [cs]},
	author = {Garcez, Artur d'Avila and Lamb, Luis C.},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.05876},
}

@article{ivanova_comprehension_2020,
	title = {Comprehension of computer code relies primarily on domain-general executive brain regions},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.58906},
	doi = {10.7554/eLife.58906},
	abstract = {Computer programming is a novel cognitive tool that has transformed modern society. What cognitive and neural mechanisms support this skill? Here, we used functional magnetic resonance imaging to investigate two candidate brain systems: the multiple demand (MD) system, typically recruited during math, logic, problem solving, and executive tasks, and the language system, typically recruited during linguistic processing. We examined MD and language system responses to code written in Python, a text-based programming language (Experiment 1) and in ScratchJr, a graphical programming language (Experiment 2); for both, we contrasted responses to code problems with responses to content-matched sentence problems. We found that the MD system exhibited strong bilateral responses to code in both experiments, whereas the language system responded strongly to sentence problems, but weakly or not at all to code problems. Thus, the MD system supports the use of novel cognitive tools even when the input is structurally similar to natural language.},
	urldate = {2020-12-23},
	journal = {eLife},
	author = {Ivanova, Anna A and Srikant, Shashank and Sueoka, Yotaro and Kean, Hope H and Dhamala, Riva and O'Reilly, Una-May and Bers, Marina U and Fedorenko, Evelina},
	editor = {Martin, Andrea E and Behrens, Timothy E and Matchin, William and Bornkessel-Schlesewsky, Ina},
	month = dec,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	pages = {e58906},
}

@article{zhuang_unsupervised_2020,
	title = {Unsupervised {Neural} {Network} {Models} of the {Ventral} {Visual} {Stream}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.06.16.155556v1},
	doi = {10.1101/2020.06.16.155556},
	abstract = {{\textless}p{\textgreater}Deep neural networks currently provide the best quantitative models of the response patterns of neurons throughout the primate ventral visual stream. However, such networks have remained implausible as a model of the development of the ventral stream, in part because they are trained with supervised methods requiring many more labels than are accessible to infants during development. Here, we report that recent rapid progress in unsupervised learning has largely closed this gap. We find that neural network models learned with deep unsupervised contrastive embedding methods achieve neural prediction accuracy in multiple ventral visual cortical areas that equals or exceeds that of models derived using today’s best supervised methods, and that the mapping of these neural network models’ hidden layers is neuroanatomically consistent across the ventral stream. Moreover, we find that these methods produce brain-like representations even when trained on noisy and limited data measured from real children’s developmental experience. We also find that semi-supervised deep contrastive embeddings can leverage small numbers of labelled examples to produce representations with substantially improved error-pattern consistency to human behavior. Taken together, these results suggest that deep contrastive embedding objectives may be a biologically-plausible computational theory of primate visual development.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-12-23},
	journal = {bioRxiv},
	author = {Zhuang, Chengxu and Yan, Siming and Nayebi, Aran and Schrimpf, Martin and Frank, Michael C. and DiCarlo, James J. and Yamins, Daniel L. K.},
	month = jun,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.06.16.155556},
}

@article{chen_learning_nodate,
	title = {Learning {Inference} {Rules} with {Neural} {TP}-{Reasoner}},
	abstract = {Most standard deep learning models do not perform logical rule-based reasoning like human and are hard to understand. We present a novel neural architecture, Tensor Product Reasoner (TP-Reasoner), for learning inference rules represented with a structured representation. In TP-Reasoner, we aim to integrate symbolic inference and deep learning: we utilize the ability of Tensor Product Representation in a neural model for learning and reasoning inference rules, which extracts intermediate representations of logical rules from a knowledge base reasoning task. TP-Reasoner achieves comparable results with baseline models. Analysis of learned inference rules in TP-Reasoner shows the interpretability of logical composition via a strong neuro-symbolic representation, a novel model expressivity, and an explicit tensor product expressions.},
	language = {en},
	author = {Chen, Kezhen and Huang, Qiuyuan and Smolensky, Paul and Forbus, Kenneth and Gao, Jianfeng},
	pages = {5},
}

@article{zhang_when_2020,
	title = {When {Do} {You} {Need} {Billions} of {Words} of {Pretraining} {Data}?},
	url = {http://arxiv.org/abs/2011.04946},
	abstract = {NLP is currently dominated by generalpurpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from largescale pretraining that they cannot learn from less data? We adopt four probing methods—classiﬁer probing, information-theoretic probing, unsupervised relative acceptability judgment, and ﬁne-tuning on NLU tasks—and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We ﬁnd that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:2011.04946 [cs]},
	author = {Zhang, Yian and Warstadt, Alex and Li, Haau-Sing and Bowman, Samuel R.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.04946},
}

@misc{noauthor_principles_nodate,
	title = {Principles of {Effective} {Research} {\textbar} {Michael} {Nielsen}},
	url = {https://michaelnielsen.org/blog/principles-of-effective-research/},
	language = {en-US},
	urldate = {2020-12-22},
}

@article{achille_life-long_2018,
	title = {Life-{Long} {Disentangled} {Representation} {Learning} with {Cross}-{Domain} {Latent} {Homologies}},
	url = {http://arxiv.org/abs/1808.06508},
	abstract = {Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.},
	urldate = {2020-12-22},
	journal = {arXiv:1808.06508 [cs, stat]},
	author = {Achille, Alessandro and Eccles, Tom and Matthey, Loic and Burgess, Christopher P. and Watters, Nick and Lerchner, Alexander and Higgins, Irina},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.06508},
}

@article{piantadosi_compositional_2016,
	title = {Compositional {Reasoning} in {Early} {Childhood}},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0147734},
	doi = {10.1371/journal.pone.0147734},
	abstract = {Compositional “language of thought” models have recently been proposed to account for a wide range of children’s conceptual and linguistic learning. The present work aims to evaluate one of the most basic assumptions of these models: children should have an ability to represent and compose functions. We show that 3.5–4.5 year olds are able to predictively compose two novel functions at significantly above chance levels, even without any explicit training or feedback on the composition itself. We take this as evidence that children at this age possess some capacity for compositionality, consistent with models that make this ability explicit, and providing an empirical challenge to those that do not.},
	language = {en},
	number = {9},
	urldate = {2020-12-22},
	journal = {PLOS ONE},
	author = {Piantadosi, Steven and Aslin, Richard},
	month = sep,
	year = {2016},
	note = {Publisher: Public Library of Science},
	pages = {e0147734},
}

@article{rosch_principles_nodate,
	title = {Principles of {Categorization}},
	volume = {13},
	issn = {1548-9213},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/11445135%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/16914980%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/18381770%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/11322980%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2323975&t},
	abstract = {Inferences about how the complex somatosensory systems of anthropoid primates evolved are based on comparative studies of such systems in extant mammals. Experimental studies of members of the major clades of extant mammals suggest that somatosensory cortex of early mammals consisted of only a few areas, including a primary area, S1, bordered by strip-like rostral and caudal somatosensory fields, SR and SC. In addition, the second somatosensory area, S2, and the parietal ventral area, PV, were probably present. S1, S2, and PV were activated independently via parallel projections from the ventroposterior nucleus, VP. Little posterior parietal cortex existed, and it was unlikely that a separate primary motor area, M1, existed until placental mammals evolved. Early primates retained this basic organization and also had a larger posterior parietal region that mediated sensorimotor functions via connections with motor and premotor areas. The frontal cortex included M1, dorsal and ventral premotor areas, supplementary motor area, and cingulate motor fields. Ventroposterior superior and ventroposterior inferior nuclei were distinct from the ventroposterior nucleus in the thalamus. In early anthropoid primates, areas S1, SR, and SC had differentiated into the fields now recognized as areas 3b, 3a, and 1. Areas 3b and 1 contained parallel mirror-image representations of cutaneous receptors and a parallel representation in area 2 was probable. Serial processing became dominant, so that neurons in areas 1, S2, and PV became dependent on area 3b for activation. Posterior parietal cortex expanded into more areas that related to frontal cortex. Less is known about changes that might have occurred with the emergence of apes and humans, but their brains were larger and posed scaling problems most likely solved by increasing the number of cortical areas and reducing the proportion of long connections.},
	number = {1},
	author = {Rosch, Eleanor},
	pmid = {17911210},
	note = {ISBN: 1314362704},
	pages = {279--306},
}

@inproceedings{bommasani_interpreting_2020,
	address = {Online},
	title = {Interpreting {Pretrained} {Contextualized} {Representations} via {Reductions} to {Static} {Embeddings}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.431},
	doi = {10.18653/v1/2020.acl-main.431},
	abstract = {Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bommasani, Rishi and Davis, Kelly and Cardie, Claire},
	month = jul,
	year = {2020},
	pages = {4758--4781},
}

@inproceedings{hewitt_rnns_2020,
	address = {Online},
	title = {{RNNs} can generate bounded hierarchical languages with optimal memory},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.156},
	doi = {10.18653/v1/2020.emnlp-main.156},
	abstract = {Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-\$(k,m)\$, the language of well-nested brackets (of \$k\$ types) and \$m\$-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use \$O(k{\textasciicircum}{\textbackslash}fracm2)\$ memory (hidden units) to generate these languages. We prove that an RNN with \$O(m {\textbackslash}log k)\$ hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with \$o(m {\textbackslash}log k)\$ hidden units.},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Hahn, Michael and Ganguli, Surya and Liang, Percy and Manning, Christopher D.},
	month = nov,
	year = {2020},
	pages = {1978--2010},
}

@article{akyurek_learning_2020,
	title = {Learning to {Recombine} and {Resample} {Data} for {Compositional} {Generalization}},
	url = {http://arxiv.org/abs/2010.03706},
	abstract = {Flexible neural models outperform grammar- and automaton-based counterparts on a variety of sequence modeling tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data -- particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. Here we present a family of learned data augmentation schemes that support a large category of compositional generalizations without appeal to latent symbolic structure. Our approach to data augmentation has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems---instruction following (SCAN) and morphological analysis (Sigmorphon 2018)---where our approach enables learning of new constructions and tenses from as few as eight initial examples.},
	urldate = {2020-12-22},
	journal = {arXiv:2010.03706 [cs]},
	author = {Akyürek, Ekin and Akyürek, Afra Feyza and Andreas, Jacob},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.03706},
}

@article{goldstein_thinking_2020,
	title = {Thinking ahead: prediction in context as a keystone of language in humans and machines},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Thinking ahead},
	url = {https://www.biorxiv.org/content/10.1101/2020.12.02.403477v1},
	doi = {10.1101/2020.12.02.403477},
	abstract = {{\textless}p{\textgreater}Departing from classical rule-based linguistic models, advances in deep learning have led to the development of a new family of self-supervised deep language models (DLMs). These models are trained using a simple self-supervised autoregressive objective, which aims to predict the next word in the context of preceding words in real-life corpora. After training, autoregressive DLMs are able to generate new 9context-aware9 sentences with appropriate syntax and convincing semantics and pragmatics. Here we provide empirical evidence for the deep connection between autoregressive DLMs and the human language faculty using a 30-min spoken narrative and electrocorticographic (ECoG) recordings. Behaviorally, we demonstrate that humans have a remarkable capacity for word prediction in natural contexts, and that, given a sufficient context window, DLMs can attain human-level prediction performance. Next, we leverage DLM embeddings to demonstrate that many electrodes spontaneously predict the meaning of upcoming words, even hundreds of milliseconds before they are perceived. Finally, we demonstrate that contextual embeddings derived from autoregressive DLMs capture neural representations of the unique, context-specific meaning of words in the narrative. Our findings suggest that deep language models provide an important step toward creating a biologically feasible computational framework for generative language.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-12-22},
	journal = {bioRxiv},
	author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Lora, Fanda and Flinker, Adeen and Devore, Sasha and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
	month = dec,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.12.02.403477},
}

@article{tang_effective_2019,
	title = {Effective learning is accompanied by high-dimensional and efficient representations of neural activity},
	volume = {22},
	issn = {15461726},
	url = {http://dx.doi.org/10.1038/s41593-019-0400-9},
	doi = {10.1038/s41593-019-0400-9},
	abstract = {A fundamental cognitive process is to map value and identity onto the objects we learn about. However, what space best embeds this mapping is not completely understood. Here we develop tools to quantify the space and organization of such a mapping in neural responses as reflected in functional MRI, to show that quick learners have a higher dimensional representation than slow learners, and hence more easily distinguishable whole-brain responses to objects of different value. Furthermore, we find that quick learners display more compact embedding of their neural responses, and hence have higher ratios of their stimuli dimension to their embedding dimension, which is consistent with greater efficiency of cognitive coding. Lastly, we investigate the neurophysiological drivers at smaller scales and study the complementary distinguishability of whole-brain responses. Our results demonstrate a spatial organization of neural responses characteristic of learning and offer geometric measures applicable to identifying efficient coding in higher-order cognitive processes.},
	number = {6},
	journal = {Nature Neuroscience},
	author = {Tang, Evelyn and Mattar, Marcelo G. and Giusti, Chad and Lydon-Staley, David M. and Thompson-Schill, Sharon L. and Bassett, Danielle S.},
	year = {2019},
	pmid = {31110323},
	note = {arXiv: 1709.10045
Publisher: Springer US},
	pages = {1000--1009},
}

@article{fusi_why_2016,
	title = {Why neurons mix: {High} dimensionality for higher cognition},
	volume = {37},
	issn = {18736882},
	doi = {10.1016/j.conb.2016.01.010},
	abstract = {Neurons often respond to diverse combinations of task-relevant variables. This form of mixed selectivity plays an important computational role which is related to the dimensionality of the neural representations: high-dimensional representations with mixed selectivity allow a simple linear readout to generate a huge number of different potential responses. In contrast, neural representations based on highly specialized neurons are low dimensional and they preclude a linear readout from generating several responses that depend on multiple task-relevant variables. Here we review the conceptual and theoretical framework that explains the importance of mixed selectivity and the experimental evidence that recorded neural representations are high-dimensional. We end by discussing the implications for the design of future experiments.},
	journal = {Current Opinion in Neurobiology},
	author = {Fusi, Stefano and Miller, Earl K. and Rigotti, Mattia},
	year = {2016},
	pmid = {26851755},
	pages = {66--74},
}

@misc{noauthor_unit_2018,
	title = {Unit 7 {Enterprise} {Archicteture} {Modelling}.pdf},
	year = {2018},
	note = {Pages: 1-10},
}

@article{yamins_using_2016,
	title = {Using goal-driven deep learning models to understand sensory cortex},
	volume = {19},
	issn = {15461726},
	doi = {10.1038/nn.4244},
	abstract = {Fueled by innovation in the computer vision and artificial intelligence communities, recent developments in computational neuroscience have used goal-driven hierarchical convolutional neural networks (HCNNs) to make strides in modeling neural single-unit and population responses in higher visual cortical areas. In this Perspective, we review the recent progress in a broader modeling context and describe some of the key technical innovations that have supported it. We then outline how the goal-driven HCNN approach can be used to delve even more deeply into understanding the development and organization of sensory cortical processing.},
	number = {3},
	journal = {Nature Neuroscience},
	author = {Yamins, Daniel L.K. and DiCarlo, James J.},
	year = {2016},
	pmid = {26906502},
	pages = {356--365},
}

@article{lindsay_convolutional_2020,
	title = {Convolutional {Neural} {Networks} as a {Model} of the {Visual} {System}: {Past}, {Present}, and {Future}},
	issn = {0898-929X},
	doi = {10.1162/jocn_a_01544},
	abstract = {Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNs in vision research beyond basic object recognition.},
	journal = {Journal of Cognitive Neuroscience},
	author = {Lindsay, Grace W.},
	year = {2020},
	note = {arXiv: 2001.07092},
	pages = {1--15},
}

@article{yamins_performance-optimized_2014,
	title = {Performance-optimized hierarchical models predict neural responses in higher visual cortex},
	volume = {111},
	issn = {10916490},
	doi = {10.1073/pnas.1403112111},
	abstract = {The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model's categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model's intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization - applied in a biologically appropriate model class - can be used to build quantitative predictive models of neural processing.},
	number = {23},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Yamins, Daniel L.K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
	year = {2014},
	pmid = {24812127},
	pages = {8619--8624},
}

@article{tarhan_sociality_2020,
	title = {Sociality and interaction envelope organize visual action representations},
	volume = {11},
	issn = {20411723},
	url = {http://dx.doi.org/10.1038/s41467-020-16846-w},
	doi = {10.1038/s41467-020-16846-w},
	abstract = {Humans observe a wide range of actions in their surroundings. How is the visual cortex organized to process this diverse input? Using functional neuroimaging, we measured brain responses while participants viewed short videos of everyday actions, then probed the structure in these responses using voxel-wise encoding modeling. Responses are well fit by feature spaces that capture the body parts involved in an action and the action’s targets (i.e. whether the action was directed at an object, another person, the actor, and space). Clustering analyses reveal five large-scale networks that summarize the voxel tuning: one related to social aspects of an action, and four related to the scale of the interaction envelope, ranging from fine-scale manipulations directed at objects, to large-scale whole-body movements directed at distant locations. We propose that these networks reveal the major representational joints in how actions are processed by visual regions of the brain.},
	number = {1},
	journal = {Nature Communications},
	author = {Tarhan, Leyla and Konkle, Talia},
	year = {2020},
	pmid = {32532982},
	note = {Publisher: Springer US},
	pages = {1--11},
}

@article{laughlin_simple_1981,
	title = {A {Simple} {Coding} {Procedure} {Enhances}},
	abstract = {The contrast-response function o f a class of first order intemeurons in the fly's compound eye approximates to the cumulative probability distribution of contrast levels in natural scenes. Elementary information theory shows that this matching enables the neurons to encode contrast fluctuations most efficiently. efficient means of apportioning the neuron's limited response range; the inputs should be encoded so that all response levels are used with equal fre­ quency. Under this condition the information car­ ried by the responses can be maximised because the information channel achieves its maximum entropy [3,4]. For the simplest case of a neuron representing a single input parameter with a single output param­ eter, this optimum can be attained when the input-output function corresponds to the cumulative probability function for the different input levels (Fig. 1), because equal output excursions corres­ pond to equal probabilities of input. The technique of using a cumulative probability function as a Neurons carry and process information and there ought to be situations in which neural coding follows the dictates of information theory [1], Large monopolar cells (LMC's) are first order interneurons of the insect compound eye. Like the analogous bi­ polar cells of the vertebrate retina, their graded responses are driven by small groups of receptors with the same field of view [2]. The compressive intensity-response function of the receptors, com­ bined with lateral and self-inhibition, adjusts the LMC sensitivity to the background intensity so that their responses code contrast fluctuations rather than absolute intensity [3]. I show here that this intemeuron's contrast-response function matches the range of contrasts encountered in natural scenes so as to increase the efficiency with which information is encoded. A fundamental limitation upon neural coding is the restricted range of responses with which a neuron can represent the states of its inputs. For a graded potential cell like the LMC, the response range is ultimately limited by reversal potentials. How should a neuron weigh its inputs so as to best represent their states? If sensitivities are set too high then inputs will often saturate the response, and in­ formation will be lost through clipping. Con­ versely, when sensitivities are set too low, large parts of the response range are underutilised be­ cause they correspond to exceptionally large ex­ cursions of input. Information theory [4] suggests an Fig. 1. The coding strategy for maximising a neuron's information capacity by ensuring that all response levels are used with equal frequency. Upper curve:-probability density function for stimulus intensities: Lower curve:-the intensity-response function that implements the strat­ egy. In this example the neuron has 10 response states, corresponding to 10 "just noticeable differences" in re­ sponse. The intensity-response function ensures that the interval between each response level encompasses an equal area under the intensity distribution, so that each state is used with equal frequency. In the limit where the states are vanishingly small this intensity-response function corre­ sponds to the cumulative probability function for stimulus intensities. Id-o = 01 2 CL Unauthenticated Download Date {\textbar} 4/2/17 7:53 PM},
	journal = {Z. Naturforsch},
	author = {Laughlin, Simon},
	year = {1981},
}

@article{kriegeskorte_matching_2008,
	title = {Matching {Categorical} {Object} {Representations} in {Inferior} {Temporal} {Cortex} of {Man} and {Monkey}},
	volume = {60},
	issn = {08966273},
	url = {http://dx.doi.org/10.1016/j.neuron.2008.10.043},
	doi = {10.1016/j.neuron.2008.10.043},
	abstract = {Inferior temporal (IT) object representations have been intensively studied in monkeys and humans, but representations of the same particular objects have never been compared between the species. Moreover, IT's role in categorization is not well understood. Here, we presented monkeys and humans with the same images of real-world objects and measured the IT response pattern elicited by each image. In order to relate the representations between the species and to computational models, we compare response-pattern dissimilarity matrices. IT response patterns form category clusters, which match between man and monkey. The clusters correspond to animate and inanimate objects; within the animate objects, faces and bodies form subclusters. Within each category, IT distinguishes individual exemplars, and the within-category exemplar similarities also match between the species. Our findings suggest that primate IT across species may host a common code, which combines a categorical and a continuous representation of objects. © 2008 Elsevier Inc. All rights reserved.},
	number = {6},
	journal = {Neuron},
	author = {Kriegeskorte, Nikolaus and Mur, Marieke and Ruff, Douglas A. and Kiani, Roozbeh and Bodurka, Jerzy and Esteky, Hossein and Tanaka, Keiji and Bandettini, Peter A.},
	year = {2008},
	pmid = {19109916},
	note = {Publisher: Elsevier Ltd},
	pages = {1126--1141},
}

@article{haxby_distributed_2013,
	title = {Distributed and overlapping representations of faces and objects in ventral temporal corten},
	volume = {293},
	doi = {10.4324/9780203496190},
	abstract = {The functional architecture of the object vision pathway in the human brain was investigated using functional magnetic resonance imaging to measure patterns of response in ventral temporal cortex while subjects viewed faces, cats, five categories of man-made objects, and nonsense pictures. A distinct pattern of response was found for each stimulus category. The distinctiveness of the response to a given category was not due simply to the regions that responded maximally to that category, because the category being viewed also could be identified on the basis of the pattern of response when those regions were excluded from the analysis. Patterns of response that discriminated among all categories were found even within cortical regions that responded maximally to only one category. These results indicate that the representations of faces and objects in ventral temporal cortex are widely distributed and overlapping.},
	number = {September},
	journal = {Social Neuroscience: Key Readings},
	author = {Haxby, James V. and Gobbini, M. Ida and Furey, Maura L. and Ishai, Alumit and Schouten, Jennifer L. and Pietrini, Pietro},
	year = {2013},
	note = {ISBN: 9780203496190},
	pages = {87--96},
}

@article{vaziri_channel_2014,
	title = {A channel for {3D} environmental shape in anterior inferotemporal cortex},
	volume = {84},
	issn = {10974199},
	doi = {10.1016/j.neuron.2014.08.043},
	abstract = {Inferotemporal cortex (IT) has long been studied as asingle pathway dedicated to object vision, but connectivity analysis reveals anatomically distinct channels, through ventral superior temporal sulcus (STSv) and dorsal/ventral inferotemporal gyrus (TEd, TEv). Here, we report a major functional distinction between channels. We studied individual IT neurons in monkeys viewing stereoscopic 3D images projected on a large screen. We used adaptive stimuli to explore neural tuning for 3D abstract shapes ranging in scale and topology from small, closed, bounded objects to large, open, unbounded environments (landscape-like surfaces and cave-like interiors). In STSv, most neurons were more responsive to objects, as expected. In TEd, surprisingly, most neurons were more responsive to 3D environmental shape. Previous studies have localized environmental information to posterior cortical modules. Our results show it is also channeled through anterior IT, where extensive cross-connections between STSv and TEd could integrate object and environmental shape information.},
	number = {1},
	journal = {Neuron},
	author = {Vaziri, Siavash and Carlson, Eric T. and Wang, Zhihong and Connor, Charles E.},
	year = {2014},
	pmid = {25242216},
	pages = {55--62},
}

@article{freiwald_face_2009,
	title = {A face feature space in the macaque temporal lobe},
	volume = {12},
	issn = {10976256},
	url = {http://dx.doi.org/10.1038/nn.2363},
	doi = {10.1038/nn.2363},
	abstract = {The ability of primates to effortlessly recognize faces has been attributed to the existence of specialized face areas. One such area, the macaque middle face patch, consists almost entirely of cells that are selective for faces, but the principles by which these cells analyze faces are unknown. We found that middle face patch neurons detect and differentiate faces using a strategy that is both part based and holistic. Cells detected distinct constellations of face parts. Furthermore, cells were tuned to the geometry of facial features. Tuning was most often ramp-shaped, with a one-to-one mapping of feature magnitude to firing rate. Tuning amplitude depended on the presence of a whole, upright face and features were interpreted according to their position in a whole, upright face. Thus, cells in the middle face patch encode axes of a face space specialized for whole, upright faces. © 2009 Nature America, Inc. All rights reserved.},
	number = {9},
	journal = {Nature Neuroscience},
	author = {Freiwald, Winrich A. and Tsao, Doris Y. and Livingstone, Margaret S.},
	year = {2009},
	pmid = {19668199},
	note = {Publisher: Nature Publishing Group},
	pages = {1187--1196},
}

@article{kanwisher_fusiform_2005,
	title = {The {Fusiform} {Face} {Area}: {A} {Module} in {Human} {Extrastriate} {Cortex} {Specialized} for {Face} {Perception}},
	volume = {2005},
	doi = {10.1109/CDC.2005.1583375},
	abstract = {This contribution is devoted to a nonlinear energy based controller design for plants consisting of a mechanical 1-DOF load system coupled to a general hydraulic actuator. A systematic controller design which maintains the Port Hamiltonian structure of the plant is demonstrated for the case of a double-ended piston with only one servovalve. The developed controller shows good robustness properties and is able to inject additional damping into the load system without velocity measurement. Finally, the performance of the control law is demonstrated by application to a special industrial plant. © 2005 IEEE.},
	number = {11},
	author = {{Kanwisher} and {McDermott} and {Chun}},
	year = {2005},
	note = {ISBN: 0780395689},
	pages = {7520--7525},
}

@article{gallistel_where_2020,
	title = {Where meanings arise and how: {Building} on {Shannon}'s foundations},
	volume = {35},
	issn = {14680017},
	doi = {10.1111/mila.12289},
	abstract = {Information theory provides a quantitative conceptual framework for understanding the flow of information from the world into and through brains. It focuses our attention on the sets of possible messages a brain's anatomy and physiology enable it to receive. The meanings of the messages arise from the inferences licensed by the brain's processing of them. Different meanings arise at different levels because different representations of the input license different inferences.},
	number = {3},
	journal = {Mind and Language},
	author = {Gallistel, Charles R.},
	year = {2020},
	pages = {390--401},
}

@article{brette_is_2019,
	title = {Is coding a relevant metaphor for the brain?},
	volume = {75012},
	issn = {14691825},
	doi = {10.1017/S0140525X19000049},
	abstract = {"Neural coding" is a popular metaphor in neuroscience, where objective properties of the world are communicated to the brain in the form of spikes. Here I argue that this metaphor is often inappropriate and misleading. First, when neurons are said to encode experimental parameters, the neural code depends on experimental details that are not carried by the coding variable (e.g. the spike count). Thus, the representational power of neural codes is much more limited than generally implied. Second, neural codes carry information only by reference to things with known meaning. In contrast, perceptual systems must build information from relations between sensory signals and actions, forming an internal model. Neural codes are inadequate for this purpose because they are unstructured and therefore unable to represent relations. Third, coding variables are observables tied to the temporality of experiments, while spikes are timed actions that mediate coupling in a distributed dynamical system. The coding metaphor tries to fit the dynamic, circular and distributed causal structure of the brain into a linear chain of transformations between observables, but the two causal structures are incongruent. I conclude that the neural coding metaphor cannot provide a valid basis for theories of brain function, because it is incompatible with both the causal structure of the brain and the representational requirements of cognition.},
	number = {Shannon 1948},
	journal = {Behavioral and Brain Sciences},
	author = {Brette, Romain},
	year = {2019},
	pmid = {30714889},
}

@article{zador_neural_2000,
	title = {Neural {Representation} and the {Cortical} {Code}},
	journal = {New York},
	author = {Zador, Anthony},
	year = {2000},
	pages = {613--647},
}

@misc{noauthor_marr_nodate,
	title = {Marr - 1982 - {Vision}.pdf},
}

@article{gallistel_animal_1989,
	title = {Animal cognition: the representation of space, time and number.},
	volume = {40},
	issn = {00664308},
	doi = {10.1146/annurev.ps.40.020189.001103},
	journal = {Annual review of psychology},
	author = {Gallistel, C. R.},
	year = {1989},
	pmid = {2648974},
	pages = {155--189},
}

@article{smolensky_necst_nodate,
	title = {The {NECST} generation of {AI} systems},
	author = {Smolensky, Paul},
}

@article{manning_emergent_2020,
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	issn = {0027-8424},
	doi = {10.1073/pnas.1907367117},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	year = {2020},
	pages = {201907367},
}

@article{kobayashi_attention_2020,
	title = {Attention {Module} is {Not} {Only} a {Weight}: {Analyzing} {Transformers} with {Vector} {Norms}},
	url = {http://arxiv.org/abs/2004.10102},
	abstract = {Because attention modules are core components of Transformer-based models that have recently achieved considerable success in natural language processing, the community has a great deal of interest in why attention modules are successful and what kind of linguistic information they capture. In particular, previous studies have mainly analyzed attention weights to see how much information the attention modules gather from each input to produce an output. In this study, we point out that attention weights alone are only one of the two factors determining the output of self-attention modules, and we propose to incorporate the other factor as well, namely, the transformed input vectors into the analysis. That is, we measure the norm of the weighted vectors as the contribution of each input to an output. Our analysis of self-attention modules in BERT and the Transformer-based neural machine translation system shows that the attention modules behave very intuitively, contrary to previous findings. That is, our analysis reveals that (1) BERT's attention modules do not pay so much attention to special tokens, and (2) Transformer's attention modules capture word alignment quite well.},
	author = {Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
	year = {2020},
	note = {arXiv: 2004.10102},
}

@article{linzen_how_2020,
	title = {How {Can} {We} {Accelerate} {Progress} {Towards} {Human}-like {Linguistic} {Generalization}?},
	url = {http://arxiv.org/abs/2005.00955},
	abstract = {This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.},
	author = {Linzen, Tal},
	year = {2020},
	note = {arXiv: 2005.00955},
}

@article{yildirim_efficient_2020,
	title = {Efficient inverse graphics in biological face processing},
	volume = {6},
	issn = {23752548},
	doi = {10.1126/sciadv.aax5979},
	abstract = {Vision not only detects and recognizes objects, but performs rich inferences about the underlying scene structure that causes the patterns of light we see. Inverting generative models, or “analysis-by-synthesis”, presents a possible solution, but its mechanistic implementations have typically been too slow for online perception, and their mapping to neural circuits remains unclear. Here we present a neurally plausible efficient inverse graphics model and test it in the domain of face recognition. The model is based on a deep neural network that learns to invert a three-dimensional face graphics program in a single fast feedforward pass. It explains human behavior qualitatively and quantitatively, including the classic “hollow face” illusion, and it maps directly onto a specialized face-processing circuit in the primate brain. The model fits both behavioral and neural data better than state-of-the-art computer vision models, and suggests an interpretable reverse-engineering account of how the brain transforms images into percepts.},
	number = {10},
	journal = {Science Advances},
	author = {Yildirim, Ilker and Belledonne, Mario and Freiwald, Winrich and Tenenbaum, Josh},
	year = {2020},
}

@article{achille_critical_2017,
	title = {Critical {Learning} {Periods} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.08856},
	abstract = {Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of "Information Plasticity". Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.},
	author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
	year = {2017},
	note = {arXiv: 1711.08856},
	pages = {1--14},
}

@article{radford_language_2018,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2018},
}

@article{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	year = {2020},
	note = {arXiv: 2001.08361},
}

@article{noauthor_tht_nodate,
	title = {tht {IB} -},
}

@article{mit_mit_2007,
	title = {{MIT} 6.438},
	volume = {63},
	doi = {10.1177/019263657906342609},
	number = {426},
	journal = {Methods},
	author = {{MIT}},
	year = {2007},
	pages = {60--66},
}

@article{murphy_undirected_2012,
	title = {Undirected {Graphical} {Models} ({Markov} {Random} {Fields})},
	volume = {2},
	journal = {Machine Learning: A Probabilistic Perspective},
	author = {Murphy, Kevin P},
	year = {2012},
	note = {ISBN: 9780262018029},
	pages = {661--705},
}

@article{simplex_probability_nodate,
	title = {Probability {Simplex}},
	author = {Simplex, Probability},
}

@article{mit_mit_2013,
	title = {{MIT} 6.437},
	abstract = {Welcome to the world of statistic inference and its deep information-theoretic foun-dations. As we will see, it is intellectually rich, with a wealth of applications. In these notes, we assume fluency with basic probabilistic system analysis. Ac-cordingly, we need only to establish some notation to get started.},
	author = {{MIT}},
	year = {2013},
	pages = {1--5},
}

@article{find_filyiset_nodate,
	title = {{FIlyis}.et},
	author = {Find, Goal},
}

@article{estimation_enpy_nodate,
	title = {enpy yizj {Ny}},
	author = {Estimation, Maximum Likelihood},
}

@article{noauthor_step_nodate,
	title = {step},
}

@article{noauthor_baye_nodate,
	title = {Baye ' s},
}

@article{generating_efxj_nodate,
	title = {{EfxJ}},
	author = {Generating, Moment},
}

@article{noauthor_1002_nodate,
	title = {10/02},
}

@article{is_icy_nodate,
	title = {Icy {Xd} {XP} {LF} ay {Iif} ly},
	author = {Is, Cle and He, E H F},
}

@article{noauthor_fxfy_nodate,
	title = {fx*{fY} =},
}

@misc{noauthor_rsa_note_190829pdf_nodate,
	title = {{RSA}\_note\_190829.pdf},
}

@article{estim_pywpxxsf_nodate,
	title = {{PyWPxxsf} ×},
	author = {Estim, R V},
}

@article{cao__2006,
	title = {Организация Обороны Тамани Во {Ii} - Середине {Iii} Вв. Н. Э. : Историческое Моделирование На Основе Гис-Технологий},
	issn = {1992-0431},
	number = {16-1},
	journal = {Проблемы Истории, Филологии, Культуры},
	author = {Cao, P},
	year = {2006},
}

@article{jordan_gm_nodate,
	title = {{GM} {Chapter} 12 - {HMMs}},
	author = {Jordan, Michael I},
}

@article{hi_ily_nodate,
	title = {Ily},
	author = {Hi, E Hj},
}

@article{mackay_introduction_nodate,
	title = {Introduction to {Gaussian} {Processes}},
	author = {Mackay, David},
}

@article{jordan_gm_nodate-1,
	title = {{GM} {Ch2} - {Conditional} {Independence} and {Factorization}},
	author = {Jordan, Michael I.},
}

@article{jordan_gm_nodate-2,
	title = {{GM} {Ch} 10 - {Mixtures} and conditional mixtures},
	author = {Jordan, Michael I.},
}

@article{archana_random_2019,
	title = {Random {Signals} {Analysis} {Syllabus}},
	author = {{Archana}},
	year = {2019},
	note = {ISBN: 4105165704},
}

@article{noauthor_i_nodate,
	title = {i {hQ} i},
	pages = {1},
}

@article{noauthor_yijlxiixj_nodate,
	title = {Yijlxiixj},
}

@misc{noauthor_rsa_note_190830pdf_nodate,
	title = {{RSA}\_note\_190830.pdf},
}

@article{paisley_tutorial_2010,
	title = {A {Tutorial} on the {Dirichlet} {Process} for {Engineers} {Technical} {Report}},
	url = {papers2://publication/uuid/40BFD005-8979-47C2-9AFA-C839631D2D5D},
	abstract = {This document provides a review of the Dirichlet process originally given in the author’s preliminary exam paper and is presented here as a tutorial. No motivation is given (in what I’ve excerpted here), and this document is intended to be a mathematical tutorial that is still accessible to the engineer.},
	number = {3},
	journal = {Unpublished Manuscript},
	author = {Paisley, John},
	year = {2010},
	pages = {1--23},
}

@article{sampling_ep_nodate,
	title = {Ep {Stx} rent {TfCxi}},
	author = {Sampling, Rejection},
}

@article{jordan_gm_nodate-3,
	title = {{GM} {Ch} 9 - {Conjugate} {Priors}},
	author = {Jordan, Michael I.},
}

@article{archana_ply_nodate,
	title = {Ply it},
	author = {{Archana}},
}

@article{archana__nodate,
	title = {/ {Pycgs}-gnj},
	author = {{Archana}},
}

@article{marcus_next_2020,
	title = {The {Next} {Decade} in {AI}: {Four} {Steps} {Towards} {Robust} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2002.06177},
	abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
	number = {February},
	author = {Marcus, Gary},
	year = {2020},
	note = {arXiv: 2002.06177},
}

@article{smolensky_proper_1988,
	title = {On the proper treatment of connectionism},
	volume = {11},
	issn = {14691825},
	doi = {10.1017/S0140525X00052432},
	abstract = {A set of hypotheses is formulated for a connectionist approach to cognitive modeling. These hypotheses are shown to be incompatible with the hypotheses underlying traditional cognitive models. The connectionist models considered are massively parallel numerical computational systems that are a kind of continuous dynamical system. The numerical variables in the system correspond semantically to fine-grained features below the level of the concepts consciously used to describe the task domain. The level of analysis is intermediate between those of symbolic cognitive models and neural models. The explanations of behavior provided are like those traditional in the physical sciences, unlike the explanations provided by symbolic models. Higher-level analyses of these connectionist models reveal subtle relations to symbolic models. Parallel connectionist memory and linguistic processes are hypothesized to give rise to processes that are describable at a higher level as sequential rule application. At the lower level, computation has the character of massively parallel satisfaction of soft numerical constraints; at the higher level, this can lead to competence characterizable by hard rules. Performance will typically deviate from this competence since behavior is achieved not by interpreting hard rules but by satisfying soft constraints. The result is a picture in which traditional and connectionist theoretical constructs collaborate intimately to provide an understanding of cognition. © 1988, Cambridge University Press. All rights reserved.},
	number = {1},
	journal = {Behavioral and Brain Sciences},
	author = {Smolensky, Paul},
	year = {1988},
	pages = {1--23},
}

@article{caramazza_drawing_1986,
	title = {On drawing inferences about the structure of normal cognitive systems from the analysis of patterns of impaired performance: {The} case for single-patient studies},
	volume = {5},
	issn = {10902147},
	doi = {10.1016/0278-2626(86)90061-8},
	abstract = {An analysis of the logic of valid inferences about the structure of normal cognitive processes from the study of impaired cognitive performance in brain-damaged patients is presented. The logic of inferences from group studies and single-case studies is compared. It is shown that given certain assumptions, only the single-case method allows valid inferences about the structure of cognitive systems from the analysis of impaired performance. It is also argued that although the single-case approach is not entirely problem-free, the difficulties encountered are relatively minor. © 1986.},
	number = {1},
	journal = {Brain and Cognition},
	author = {Caramazza, Alfonso},
	year = {1986},
	pmid = {3954906},
	pages = {41--66},
}

@article{newell_theory_1988,
	title = {The {Theory} of {Human} {Problem} {Solving}},
	doi = {1-55860-013-2},
	journal = {Cognitive Science},
	author = {Newell, A and Simon, H},
	year = {1988},
}

@article{smolensky_harmonic_nodate,
	title = {The {Harmonic} {Mind} {Ch} 1},
	volume = {1},
	author = {Smolensky, Paul and Legendre, Geraldine},
}

@article{raganato_fixed_2020,
	title = {Fixed {Encoder} {Self}-{Attention} {Patterns} in {Transformer}-{Based} {Machine} {Translation}},
	url = {http://arxiv.org/abs/2002.10260},
	abstract = {Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that attention heads learn simple positional patterns which are often redundant. In this paper, we propose to replace all but one attention head of each encoder layer with fixed -- non-learnable -- attentive patterns that are solely based on position and do not require any external knowledge. Our experiments show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.},
	author = {Raganato, Alessandro and Scherrer, Yves and Tiedemann, Jörg},
	year = {2020},
	note = {arXiv: 2002.10260},
}

@article{mccarthy_programs_1959,
	title = {Programs with {Common} {Sense} - {John} {McCarthy}.pdf},
	url = {http://www-formal.stanford.edu/jmc/},
	abstract = {Interesting work is being done in programming computers to solve problems which require a high degree of intelligence in humans. However, certain elementary verbal reasoning processes so simple that they can be carried out by any non-feeble minded human have yet to be simulated by machine programs. This paper will discuss programs to manipulate in a suitable formal lan- guage (most likely a part of the predicate calculus) common instrumental statements. The basic program will draw immediate conclusions from a list of premises. These conclusions will be either declarative or imperative sen- tences. When an imperative sentence is deduced the program takes a cor- responding action. These actions may include printing sentences, moving sentences on lists, and reinitiating the basic deduction process on these lists. Facilities will be provided for communication with humans in the system via manual intervention and display devices connected to the computer.},
	author = {Mccarthy, John},
	year = {1959},
	pages = {1--15},
}

@article{sejnowski_parallel_1987,
	title = {Parallel networks that learn to pronounce {English} text},
	volume = {1},
	doi = {10.1016/0004-3702(89)90017-9},
	abstract = {This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (;i) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced prac- tice is more effective for long-term retention than massed practice. Network models can be constructed that have the same perfor- mance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk re- veal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations.},
	journal = {Complex systems},
	author = {Sejnowski, Terrence J. and Rosenberg, Charles R.},
	year = {1987},
	pages = {145--168},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {14764687},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	pmid = {26017442},
	pages = {436--444},
}

@article{mcclelland_appeal_nodate,
	title = {The {Appeal} of {Parallel} {Distributed} {Processing}},
	author = {Mcclelland, James L and Rumelhart, D and Hinton, G},
}

@article{griffiths_probabilistic_2010,
	title = {Probabilistic models of cognition: exploring representations and inductive biases},
	volume = {14},
	issn = {13646613},
	url = {http://dx.doi.org/10.1016/j.tics.2010.05.004},
	doi = {10.1016/j.tics.2010.05.004},
	abstract = {Cognitive science aims to reverse-engineer the mind, and many of the engineering challenges the mind faces involve induction. The probabilistic approach to modeling cognition begins by identifying ideal solutions to these inductive problems. Mental processes are then modeled using algorithms for approximating these solutions, and neural processes are viewed as mechanisms for implementing these algorithms, with the result being a top-down analysis of cognition starting with the function of cognitive processes. Typical connectionist models, by contrast, follow a bottom-up approach, beginning with a characterization of neural mechanisms and exploring what macro-level functional phenomena might emerge. We argue that the top-down approach yields greater flexibility for exploring the representations and inductive biases that underlie human cognition. © 2010 Elsevier Ltd.},
	number = {8},
	journal = {Trends in Cognitive Sciences},
	author = {Griffiths, Thomas L. and Chater, Nick and Kemp, Charles and Perfors, Amy and Tenenbaum, Joshua B.},
	year = {2010},
	note = {Publisher: Elsevier Ltd},
	pages = {357--364},
}

@article{dennett_logical_nodate,
	title = {The {Logical} {Geography} of {Computational} {Approaches}: {A} {View} from the {East} {Pole}},
	author = {Dennett, Daniel},
}

@article{skinner_selections_nodate,
	title = {Selections from {Science} and {Human} {Behavior}},
	author = {Skinner, B.F.},
}

@article{mcclelland_letting_2010,
	title = {Letting structure emerge: {Connectionist} and dynamical systems approaches to cognition},
	volume = {14},
	issn = {13646613},
	doi = {10.1016/j.tics.2010.06.002},
	abstract = {Connectionist and dynamical systems approaches explain human thought, language and behavior in terms of the emergent consequences of a large number of simple noncognitive processes. We view the entities that serve as the basis for structured probabilistic approaches as abstractions that are occasionally useful but often misleading: they have no real basis in the actual processes that give rise to linguistic and cognitive abilities or to the development of these abilities. Although structured probabilistic approaches can be useful in determining what would be optimal under certain assumptions, we propose that connectionist, dynamical systems, and related approaches, which focus on explaining the mechanisms that give rise to cognition, will be essential in achieving a full understanding of cognition and development. © 2010 Elsevier Ltd.},
	number = {8},
	journal = {Trends in Cognitive Sciences},
	author = {McClelland, James L. and Botvinick, Matthew M. and Noelle, David C. and Plaut, David C. and Rogers, Timothy T. and Seidenberg, Mark S. and Smith, Linda B.},
	year = {2010},
	pmid = {20598626},
	pages = {348--356},
}

@article{marcus_innateness_2018,
	title = {Innateness, {AlphaZero}, and {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/1801.05667},
	abstract = {The concept of innateness is rarely discussed in the context of artificial intelligence. When it is discussed, or hinted at, it is often the context of trying to reduce the amount of innate machinery in a given system. In this paper, I consider as a test case a recent series of papers by Silver et al (Silver et al., 2017a) on AlphaGo and its successors that have been presented as an argument that a "even in the most challenging of domains: it is possible to train to superhuman level, without human examples or guidance", "starting tabula rasa." I argue that these claims are overstated, for multiple reasons. I close by arguing that artificial intelligence needs greater attention to innateness, and I point to some proposals about what that innateness might look like.},
	author = {Marcus, Gary},
	year = {2018},
	note = {arXiv: 1801.05667},
	pages = {1--18},
}

@article{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	url = {http://arxiv.org/abs/2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	year = {2020},
	note = {arXiv: 2004.05150},
}

@article{voita_analyzing_2019,
	title = {Analyzing {Multi}-{Head} {Self}-{Attention}: {Specialized} {Heads} {Do} the {Heavy} {Lifting}, the {Rest} {Can} {Be} {Pruned}},
	doi = {10.18653/v1/p19-1580},
	abstract = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.},
	author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
	year = {2019},
	note = {arXiv: 1905.09418},
	pages = {5797--5808},
}

@article{saxe_perception_2006,
	title = {The perception of causality in infancy},
	volume = {123},
	issn = {00016918},
	doi = {10.1016/j.actpsy.2006.05.005},
	abstract = {Michotte proposed a rationalist theory of the origin of the human capacity to represent causal relations among events. He suggested that the input analyzer that underlies the causal perception in launching, entraining, and expulsion events is innate and is the ultimate source of all causal representations. We review the literature on infant causal representations, providing evidence that launching, entraining and expulsion events are interpreted causally by young infants. However, there is as of yet no good evidence that these representations are innate. Furthermore, there is considerable evidence that these representations are not the sole source of the human capacity for causal representation. © 2006 Elsevier B.V. All rights reserved.},
	number = {1-2},
	journal = {Acta Psychologica},
	author = {Saxe, Rebecca and Carey, Susan},
	year = {2006},
	pages = {144--165},
}

@article{putnam_innateness_nodate,
	title = {{THE} ‘{INNATENESS} {HYPOTHESIS}’ {AND} {EXPLANATORY} {MODELS} {IN} {LINGUISTICS}},
	author = {Putnam, Hilary},
}

@article{chomsky_review_nodate,
	title = {A review of {B}.{F}. {Skinner}'s {Verbal} {Behavior}},
	author = {Chomsky, Noam},
}

@misc{russell_history_nodate,
	title = {A {History} of {Western} {Philosophy}},
	author = {Russell, Bertrand},
}

@article{vanrullen_reconstructing_2019,
	title = {Reconstructing faces from {fMRI} patterns using deep generative neural networks},
	volume = {2},
	issn = {23993642},
	doi = {10.1038/s42003-019-0438-y},
	abstract = {Although distinct categories are reliably decoded from fMRI brain responses, it has proved more difficult to distinguish visually similar inputs, such as different faces. Here, we apply a recently developed deep learning system to reconstruct face images from human fMRI. We trained a variational auto-encoder (VAE) neural network using a GAN (Generative Adversarial Network) unsupervised procedure over a large data set of celebrity faces. The auto-encoder latent space provides a meaningful, topologically organized 1024-dimensional description of each image. We then presented several thousand faces to human subjects, and learned a simple linear mapping between the multi-voxel fMRI activation patterns and the 1024 latent dimensions. Finally, we applied this mapping to novel test images, translating fMRI patterns into VAE latent codes, and codes into face reconstructions. The system not only performed robust pairwise decoding ({\textgreater}95\% correct), but also accurate gender classification, and even decoded which face was imagined, rather than seen.},
	number = {1},
	journal = {Communications Biology},
	author = {VanRullen, Rufin and Reddy, Leila},
	year = {2019},
}

@article{vargha-khadem_dissociations_2001,
	title = {Dissociations in cognitive memory: {The} syndrome of developmental amnesia},
	volume = {356},
	issn = {09628436},
	doi = {10.1098/rstb.2001.0951},
	abstract = {The dearth of studies on amnesia in children has led to the assumption that when damage to the medial temporal lobe system occurs early in life, the compensatory capacity of the immature brain rescues memory functions. An alternative view is that such damage so interferes with the development of learning and memory that it results not in selective cognitive impairments but in general mental retardation. Data will be presented to counter both of these arguments. Results obtained from a series of 11 amnesic patients with a history of hypoxic ischaemic damage sustained perinatally or during childhood indicate that regardless of age at onset of hippocampal pathology, there is a pronounced dissociation between episodic memory, which is severely impaired, and semantic memory, which is relatively preserved. A second dissociation is characterized by markedly impaired recall and relatively spared recognition leading to a distinction between recollection-based versus familiarity-based judgements. These findings are discussed in terms of the locus and extent of neuropathology associated with hypoxic ischaemic damage, the neural basis of 'remembering' versus 'knowing', and a hierarchical model of cognitive memory.},
	number = {1413},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Vargha-Khadem, F. and Gadian, D. G. and Mishkin, M.},
	year = {2001},
	pages = {1435--1440},
}

@article{sugita_face_2008,
	title = {Face perception in monkeys reared with no exposure to faces},
	volume = {105},
	issn = {00278424},
	doi = {10.1073/pnas.0706079105},
	abstract = {Infant monkeys were reared with no exposure to any faces for 6-24 months. Before being allowed to see a face, the monkeys showed a preference for human and monkey faces in photographs, and they discriminated human faces as well as monkey faces. After the deprivation period, the monkeys were exposed first to either human or monkey faces for a month. Soon after, the monkeys selectively discriminated the exposed species of face and showed a marked difficulty in regaining the ability to discriminate the other nonexposed species of face. These results indicate the existence of an experience-independent ability for face processing as well as an apparent sensitive period during which a broad but flexible face prototype develops into a concrete one for efficient processing of familiar faces. © 2008 by The National Academy of Sciences of the USA.},
	number = {1},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Sugita, Yoichi},
	year = {2008},
	pages = {394--398},
}

@article{vargha-khadem_differential_1997,
	title = {Differential effects of early hippocampal pathology on episodic and semantic memory},
	volume = {277},
	issn = {00368075},
	doi = {10.1126/science.277.5324.376},
	abstract = {Global anterograde amnesia is described in three patients with brain injuries that occurred in one case at birth, in another by age 4, and in the third at age 9. Magnetic resonance techniques revealed bilateral hippocampal pathology in all three cases. Remarkably, despite their pronounced amnesia for the episodes of everyday life, all three patients attended mainstream schools and attained levels of speech and language competence, literacy, and factual knowledge that are within the low average to average range. The findings provide support for the view that the episodic and semantic components of cognitive memory are partly dissociable, with only the episodic component being fully dependent on the hippocampus.},
	number = {5324},
	journal = {Science},
	author = {Vargha-Khadem, F. and Gadian, D. G. and Watkins, K. E. and Connelly, A. and Van Paesschen, W. and Mishkin, M.},
	year = {1997},
	pmid = {9219696},
	pages = {376--380},
}

@article{berl_regional_2015,
	title = {Regional {Differences} in the {Developmental} {Trajectory} of {Lateralization} of the {Language} {Network}},
	volume = {35},
	doi = {10.1002/hbm.22179.Regional},
	number = {1},
	author = {Berl, Madison M and Mayo, Jessica and Parks, Erin N and Rosenberger, Lisa and VanMeter, John and Ratner, Nan Bernstein and Vaidya, Chandan J and Gaillard, William Davis},
	year = {2015},
	pages = {1--24},
}

@article{gallistel_lessons_nodate,
	title = {Lessons {From} {Animal} {Learning} for the {Study} of {Cognitive} {Development}},
	author = {{Gallistel} and {Brown} and {Carey} and {Gelman} and {Keil}},
}

@article{dehaene_is_2016,
	title = {Is the brain prewired for letters?},
	volume = {19},
	issn = {15461726},
	url = {http://dx.doi.org/10.1038/nn.4369},
	doi = {10.1038/nn.4369},
	number = {9},
	journal = {Nature Neuroscience},
	author = {Dehaene, Stanislas and Dehaene-Lambertz, Ghislaine},
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	pages = {1192--1193},
}

@article{dehaene_illiterate_2015,
	title = {Illiterate to literate: {Behavioural} and cerebral changes induced by reading acquisition},
	volume = {16},
	issn = {14710048},
	url = {http://dx.doi.org/10.1038/nrn3924},
	doi = {10.1038/nrn3924},
	abstract = {The acquisition of literacy transforms the human brain. By reviewing studies of illiterate subjects, we propose specific hypotheses on how the functions of core brain systems are partially reoriented or 'recycled' when learning to read. Literacy acquisition improves early visual processing and reorganizes the ventral occipito-temporal pathway: responses to written characters are increased in the left occipito-temporal sulcus, whereas responses to faces shift towards the right hemisphere. Literacy also modifies phonological coding and strengthens the functional and anatomical link between phonemic and graphemic representations. Literacy acquisition therefore provides a remarkable example of how the brain reorganizes to accommodate a novel cultural skill.},
	number = {4},
	journal = {Nature Reviews Neuroscience},
	author = {Dehaene, Stanislas and Cohen, Laurent and Morais, José and Kolinsky, Régine},
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	pages = {234--244},
}

@article{karmiloff-smith_development_nodate,
	title = {Development itself is the key to understanding developmental disorders},
	issn = {09707077},
	doi = {10.14233/ajchem.2014.16542},
	author = {Karmiloff-Smith, Annette},
}

@article{lakusta_impaired_2010,
	title = {Impaired geometric reorientation caused by genetic defect},
	volume = {107},
	issn = {00278424},
	doi = {10.1073/pnas.0909155107},
	abstract = {The capacity to reorient in one's environment is a fundamental part of the spatial cognitive systems of both humans and nonhuman species. Abundant literature has shown that human adults and toddlers, rats, chicks, and fish accomplish reorientation through the construction and use of geometric representations of surrounding layouts, including the lengths of surfaces and their intersection. Does the development of this reorientation system rely on specific genes and their action in brain development? We tested reorientation in individuals who have Williams syndrome (WS), a genetic disorder that results in abnormalities of hippocampal and parietal areas of the brain known to be involved in reorientation.We found that in a rectangular chamber devoid of surface feature information, WS individuals do not use the geometry of the chamber to reorient, failing to find a hidden object. The failure among people with WS cannot be explained by more general deficits in visual-spatial working memory, as the same individuals performed at ceiling in a similar task in which they were not disoriented. We also found that performance among people with WS improves in a rectangular chamber with one blue wall, suggesting that some individuals with WS can use the blue wall feature to locate the hidden object. These results show that the geometric system used for reorientation in humans can be selectively damaged by specific genetic and neural abnormalities in humans.},
	number = {7},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Lakusta, Laura and Dessalegn, Banchiamlack and Landau, Barbara},
	year = {2010},
	pages = {2813--2817},
}

@article{fisher_tangled_2006,
	title = {Tangled webs: {Tracing} the connections between genes and cognition},
	volume = {101},
	issn = {00100277},
	doi = {10.1016/j.cognition.2006.04.004},
	abstract = {The rise of molecular genetics is having a pervasive influence in a wide variety of fields, including research into neurodevelopmental disorders like dyslexia, speech and language impairments, and autism. There are many studies underway which are attempting to determine the roles of genetic factors in the aetiology of these disorders. Beyond the obvious implications for diagnosis, treatment and understanding, success in these efforts promises to shed light on the links between genes and aspects of cognition and behaviour. However, the deceptive simplicity of finding correlations between genetic and phenotypic variation has led to a common misconception that there exist straightforward linear relationships between specific genes and particular behavioural and/or cognitive outputs. The problem is exacerbated by the adoption of an abstract view of the nature of the gene, without consideration of molecular, developmental or ontogenetic frameworks. To illustrate the limitations of this perspective, I select two cases from recent research into the genetic underpinnings of neurodevelopmental disorders. First, I discuss the proposal that dyslexia can be dissected into distinct components specified by different genes. Second, I review the story of the FOXP2 gene and its role in human speech and language. In both cases, adoption of an abstract concept of the gene can lead to erroneous conclusions, which are incompatible with current knowledge of molecular and developmental systems. Genes do not specify behaviours or cognitive processes; they make regulatory factors, signalling molecules, receptors, enzymes, and so on, that interact in highly complex networks, modulated by environmental influences, in order to build and maintain the brain. I propose that it is necessary for us to fully embrace the complexity of biological systems, if we are ever to untangle the webs that link genes to cognition. © 2006 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Cognition},
	author = {Fisher, Simon E.},
	year = {2006},
	pages = {270--297},
}

@article{dundas_joint_2013,
	title = {The joint development of hemispheric lateralization for words and faces},
	volume = {142},
	issn = {00963445},
	doi = {10.1037/a0029503},
	abstract = {Consistent with long-standing findings from behavioral studies, neuroimaging investigations have identified a region of the inferior temporal cortex that, in adults, shows greater face selectivity in the right than left hemisphere and, conversely, a region that shows greater word selectivity in the left than right hemisphere. What has not been determined is how this pattern of mature hemispheric specialization emerges over the course of development. The present study examines the hemispheric superiority for faces and words in children, young adolescents and adults in a discrimination task in which stimuli are presented briefly in either hemifield. Whereas adults showed the expected left and right visual field superiority for face and word discrimination, respectively, the young adolescents demonstrated only the right-field superiority for words and no field superiority for faces. Although the children's overall accuracy was lower than that of the older groups, like the young adolescents, they exhibited a right visual field superiority for words but no field superiority for faces. Interestingly, the emergence of face lateralization was correlated with reading competence, measured on an independent standardized test, after regressing out age, quantitative reasoning scores, and face discrimination accuracy. Taken together, these findings suggest that the hemispheric organization of face and word recognition do not develop independently and that word lateralization, which emerges earlier, may drive later face lateralization. A theoretical account in which competition for visual representations unfolds over the course of development is proposed to account for the findings. © 2012 American Psychological Association.},
	number = {2},
	journal = {Journal of Experimental Psychology: General},
	author = {Dundas, Eva M. and Plaut, David C. and Behrmann, Marlene},
	year = {2013},
	pages = {348--358},
}

@article{cantlon_cortical_2011,
	title = {Cortical representations of symbols, objects, and faces are pruned back during early childhood},
	volume = {21},
	issn = {10473211},
	doi = {10.1093/cercor/bhq078},
	abstract = {Regions of human ventral extrastriate visual cortex develop specializations for natural categories (e.g., faces) and cultural artifacts (e.g., words). In adults, category-based specializations manifest as greater neural responses in visual regions of the brain (e.g., fusiform gyrus) to some categories over others. However, few studies have examined how these specializations originate in the brains of children. Moreover, it is as yet unknown whether the development of visual specializations hinges on "increases" in the response to the preferred categories, "decreases" in the responses to nonpreferred categories, or "both." This question is relevant to a long-standing debate concerning whether neural development is driven by building up or pruning back representations. To explore these questions, we measured patterns of visual activity in 4-year-old children for 4 categories (faces, letters, numbers, and shoes) using functional magnetic resonance imaging. We report 2 key findings regarding the development of visual categories in the brain: 1) the categories "faces" and "symbols" doubly dissociate in the fusiform gyrus before children can read and 2) the development of category-specific responses in young children depends on cortical responses to nonpreferred categories that decrease as preferred category knowledge is acquired. © The Author 2010. Published by Oxford University Press.},
	number = {1},
	journal = {Cerebral Cortex},
	author = {Cantlon, Jessica F. and Pinel, Philippe and Dehaene, Stanislas and Pelphrey, Kevin A.},
	year = {2011},
	pages = {191--199},
}

@article{ramus_genes_2006,
	title = {Genes, brain, and cognition: {A} roadmap for the cognitive scientist},
	volume = {101},
	issn = {00100277},
	doi = {10.1016/j.cognition.2006.04.003},
	abstract = {This paper reviews current progress in genetics in relation to the understanding of human cognition. It is argued that genetics occupies a prominent place in the future of cognitive science, and that cognitive scientists should play an active role in the process. Recent research in genetics and developmental neuroscience is reviewed and argued to provide a new perspective on the timeless questions of innateness and modularity. The special case of the genetic bases of language is further discussed, with the study of developmental dyslexia as an exemplary entry point. This Special Issue puts together articles providing different empirical examples and theoretical perspectives on how the integration between the different levels of description (gene, brain, and cognition) is to be achieved. © 2006 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Cognition},
	author = {Ramus, Franck},
	year = {2006},
	pages = {247--269},
}

@article{moran_impaired_2011,
	title = {Impaired theory of mind for moral judgment in high-functioning autism},
	volume = {108},
	issn = {00278424},
	doi = {10.1073/pnas.1011734108},
	abstract = {High-functioning autism (ASD) is characterized by real-life difficulties in social interaction; however, these individuals often succeed on laboratory tests that require an understanding of another person's beliefs and intentions. This paradox suggests a theory of mind (ToM) deficit in adults with ASD that has yet to be demonstrated in an experimental task eliciting ToM judgments. We tested whether ASD adults would show atypical moral judgments when they need to consider both the intentions (based on ToM) and outcomes of a person's actions. In experiment 1, ASD and neurotypical (NT) participants performed a ToM task designed to test false belief understanding. In experiment 2, the same ASD participants and a new group of NT participants judged the moral permissibility of actions, in a 2 (intention: neutral/negative) x 2 (outcome: neutral/ negative) design. Though there was no difference between groups on the false belief task, there was a selective difference in the moral judgment task for judgments of accidental harms, but not neutral acts, attempted harms, or intentional harms. Unlike the NT group, which judged accidental harms less morally wrong than attempted harms, the ASD group did not reliably judge accidental and attempted harms as morally different. In judging accidental harms, ASD participants appeared to show an underreliance on information about a person's innocent intention and, as a direct result, an over-reliance on the action's negative outcome. These findings reveal impairments in integrating mental state information (e.g., beliefs, intentions) for moral judgment.},
	number = {7},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Moran, Joseph M. and Young, Liane L. and Saxe, Rebecca and Lee, Su Mei and O'Young, Daniel and Mavros, Penelope L. and Gabrieli, John D.},
	year = {2011},
	pages = {2688--2692},
}

@article{bishop_cerebral_2013,
	title = {Cerebral asymmetry and language development: {Cause}, correlate, or consequence?},
	volume = {340},
	issn = {10959203},
	doi = {10.1126/science.1230531},
	abstract = {In most people, language is processed predominantly by the left hemisphere of the brain, but we do not know how or why. A popular view is that developmental language disorders result from a poorly lateralized brain, but until recently, evidence has been weak and indirect. Modern neuroimaging methods have made it possible to study normal and abnormal development of lateralized function in the developing brain and have confirmed links with language and literacy impairments. However, there is little evidence that weak cerebral lateralization has common genetic origins with language and literacy impairments. Our understanding of the association between atypical language lateralization and developmental disorders may benefit if we reconceptualize the nature of cerebral asymmetry to recognize its multidimensionality and consider variation in lateralization over developmental time. Contrary to popular belief, cerebral lateralization may not be a highly heritable, stable characteristic of individuals; rather, weak lateralization may be a consequence of impaired language learning.},
	number = {6138},
	journal = {Science},
	author = {Bishop, Dorothy V.M.},
	year = {2013},
	pmid = {23766329},
}

@article{feigenson_core_2004,
	title = {Core systems of number},
	volume = {8},
	issn = {13646613},
	doi = {10.1016/j.tics.2004.05.002},
	abstract = {What representations underlie the ability to think and reason about number? Whereas certain numerical concepts, such as the real numbers, are only ever represented by a subset of human adults, other numerical abilities are widespread and can be observed in adults, infants and other animal species. We review recent behavioral and neuropsychological evidence that these ontogenetically and phylogenetically shared abilities rest on two core systems for representing number. Performance signatures common across development and across species implicate one system for representing large, approximate numerical magnitudes, and a second system for the precise representation of small numbers of individual objects. These systems account for our basic numerical intuitions, and serve as the foundation for the more sophisticated numerical concepts that are uniquely human.},
	number = {7},
	journal = {Trends in Cognitive Sciences},
	author = {Feigenson, Lisa and Dehaene, Stanislas and Spelke, Elizabeth},
	year = {2004},
	pmid = {15242690},
	pages = {307--314},
}

@article{hermer_modularity_1996,
	title = {Modularity and development: {The} case of spatial reorientation},
	volume = {61},
	issn = {00100277},
	doi = {10.1016/S0010-0277(96)00714-7},
	abstract = {In a series of experiments, young children who were disoriented in a novel environment reoriented themselves in accord with the large-scale shape of the environment but not in accord with nongeometric properties of the environment such as the color of a wall, the patterning on a box, or the categorical identity of an object. Because children's failure to reorient by nongeometric information cannot be attributed to limits on their ability to detect, remember, or use that information for other purposes, this failure suggests that children's reorientation, at least in relatively novel environments, depends on a mechanism that is informationally encapsulated and task-specific: two hallmarks of modular cognitive processes. Parallel studies with rats suggest that children share this mechanism with at least some adult nonhuman mammals. In contrast, our own studies of human adults, who readily solved our tasks by conjoining nongeometric and geometric information, indicated that the most striking limitations of this mechanism are overcome during human development. These findings support broader proposals concerning the domain specificity of humans' core cognitive abilities, the conservation of cognitive abilities across related species and over the course of human development, and the developmental processes by which core abilities are extended to permit more flexible, uniquely human kinds of problem solving.},
	number = {3},
	journal = {Cognition},
	author = {Hermer, Linda and Spelke, Elizabeth},
	year = {1996},
	pages = {195--232},
}

@article{toga_mapping_2013,
	title = {Mapping brain maturation},
	volume = {29},
	issn = {0166-2236},
	doi = {10.1016/j.tins.2006.01.007.Mapping},
	number = {3},
	journal = {Office},
	author = {Toga, Arthur and Thompson, Paul and Sowell, Elizabeth},
	year = {2013},
	pmid = {16472876},
	note = {ISBN: 0166-2236 (Print)},
	pages = {2013},
}

@article{smith_activity-dependent_1996,
	title = {Activity-{Dependent} {Processes} in {Perceptual} and {Cognitive} {Development}},
	doi = {10.1016/b978-012279660-9/50030-0},
	abstract = {Understanding the quantitative regularities of the world and mastering the cultural tool that mathematics provides is a process that begins in infancy and continues beyond the last graduate statistics course. The continuity of quantitative development makes it a fruitful source for the theories of developmental change. This chapter describes some of the problems with a simple, definitional model of early mathematical reasoning. The chapter deals with the developmental course that leads from the 5-month-old surprised at a basic problem of elementary mathematics to the complex problems of high school mathematics. Children use multiple sources of structure in developing quantitative competence. Infants from near the moment of birth attend to number as a feature of their world, but moving from this early, inchoate sensitivity to number to real mathematical sophistication requires developing a repertoire of concepts and abilities that are quite diverse in nature. Making mathematical development possible is the fact that these multiple sources of structure increase the likelihood that different paths will converge on the same ultimate abilities.},
	number = {1985},
	journal = {Perceptual and Cognitive Development},
	author = {Smith, Linda B. and Katz, Donald B.},
	year = {1996},
	pages = {413--445},
}

@article{vargha-khadem_praxic_1995,
	title = {Praxic and nonverbal cognitive deficits in a large family with a genetically transmitted speech and language disorder},
	volume = {92},
	issn = {00278424},
	doi = {10.1073/pnas.92.3.930},
	abstract = {A pronounced speech and language disorder affecting half of the 30 members of the four-generational KE family has been attributed by some researchers to a specific defect in the generation of morphosyntactic rules. The reported selectivity of the impairment has led to the view that the affected members suffer from a grammar-specific disorder. Our investigations of the same KE family indicate that the inherited disorder has a broad phenotype which transcends impaired generation of syntactical rules and includes a striking articulatory impairment as well as defects in intellectual, linguistic, and orofacial praxic functions generally. Although the evidence from this family thus provides no support for the existence of 'grammar genes,' their linguistic difficulties do constitute a prominent part of their phenotype. Investigations of the neural and genetic correlates of their disorder could therefore uncover important clues to some of the bases of the primary human faculties of speech and language.},
	number = {3},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Vargha-Khadem, Faraneh and Watkins, Kate and Alcock, Katie and Fletcher, Paul and Passingham, Richard},
	year = {1995},
	pages = {930--933},
}

@article{le_grand_impairment_2004,
	title = {Impairment in holistic face processing following early visual deprivation},
	volume = {15},
	issn = {09567976},
	doi = {10.1111/j.0956-7976.2004.00753.x},
	abstract = {Unlike most objects, faces are processed holistically: They are processed as a whole rather than as a collection of independent features. We examined the role of early visual experience in the development of this type of processing of faces by using the composite-face task, a measure of holistic processing, to test patients deprived of visual experience during infancy. Visually normal control subjects showed the expected composite-face effect: They had difficulty perceiving that the top halves of two faces were the same when the top halves were aligned with different bottom halves. Performance improved when holistic processing was disrupted by misaligning the top and bottom halves. Deprived patients, in contrast, showed no evidence of holistic processing, and in fact performed significantly better than control subjects when top and bottom halves were aligned. These findings suggest that early visual experience is necessary to set up or maintain the neural substrate that leads to holistic processing of faces.},
	number = {11},
	journal = {Psychological Science},
	author = {Le Grand, Richard and Mondloch, Catherine J. and Maurer, Daphne and Brent, Henry P.},
	year = {2004},
	pages = {762--768},
}

@article{pascalis_is_2002,
	title = {Is face processing species-specific during the first year of life?},
	volume = {296},
	issn = {00368075},
	doi = {10.1126/science.1070223},
	abstract = {Between 6 and 10 months of age, the infant's ability to discriminate among native speech sounds improves, whereas the same ability to discriminate among foreign speech sounds decreases. Our study aimed to determine whether this perceptual narrowing is unique to language or might also apply to face processing. We tested discrimination of human and monkey faces by 6-month-olds, 9-month-olds, and adults, using the visual paired-comparison procedure. Only the youngest group showed discrimination between individuals of both species; older infants and adults only showed evidence of discrimination of their own species. These results suggest that the "perceptual narrowing" phenomenon may represent a more general change in neural networks involved in early cognition.},
	number = {5571},
	journal = {Science},
	author = {Pascalis, Olivier and De Haan, Michelle and Nelson, Charles A.},
	year = {2002},
	pages = {1321--1323},
}

@article{gweon_theory_2012,
	title = {Theory of {Mind} {Performance} in {Children} {Correlates} {With} {Functional} {Specialization} of a {Brain} {Region} for {Thinking} {About} {Thoughts}},
	volume = {83},
	issn = {00093920},
	doi = {10.1111/j.1467-8624.2012.01829.x},
	abstract = {Thinking about other people's thoughts recruits a specific group of brain regions, including the temporo-parietal junctions (TPJ), precuneus (PC), and medial prefrontal cortex (MPFC). The same brain regions were recruited when children (N=20, 5-11years) and adults (N=8) listened to descriptions of characters' mental states, compared to descriptions of physical events. Between ages 5 and 11years, responses in the bilateral TPJ became increasingly specific to stories describing mental states as opposed to people's appearance and social relationships. Functional activity in the right TPJ was related to children's performance on a high level theory of mind task. These findings provide insights into the origin of neural mechanisms of theory of mind, and how behavioral and neural changes can be related in development. © 2012 The Authors. Child Development © 2012 Society for Research in Child Development, Inc.},
	number = {6},
	journal = {Child Development},
	author = {Gweon, Hyowon and Dodell-Feder, David and Bedny, Marina and Saxe, Rebecca},
	year = {2012},
	pages = {1853--1868},
}

@article{duchaine_family_2007,
	title = {Family resemblance: {Ten} family members with prosopagnosia and within-class object agnosia},
	volume = {24},
	issn = {02643294},
	doi = {10.1080/02643290701380491},
	abstract = {We report on neuropsychological testing done with a family in which many members reported severe face recognition impairments. These 10 individuals were high functioning in everyday life and performed normally on tests of low-level vision and high-level cognition. In contrast, they showed clear deficits with tests requiring face memory and judgements of facial similarity. They did not show deficits with all aspects of higher level visual processing as all tested performed normally on a challenging facial emotion recognition task and on a global-local letter identification task. On object memory tasks requiring recognition of particular cars and guns, they showed significant deficits so their recognition impairments were not restricted to facial identity. These results strongly suggest the existence of a genetic condition leading to a selective deficit of visual recognition.},
	number = {4},
	journal = {Cognitive Neuropsychology},
	author = {Duchaine, Bradley and Germine, Laura and Nakayama, Ken},
	year = {2007},
	pages = {419--430},
}

@article{vallortigara_doing_2009,
	title = {Doing {Socrates} experiment right: controlled rearing studies of geometrical knowledge in animals},
	volume = {19},
	issn = {09594388},
	doi = {10.1016/j.conb.2009.02.002},
	abstract = {The issue of whether encoding of geometric information for navigational purposes crucially depends on environmental experience or whether it is innately predisposed in the brain has been recently addressed in controlled rearing studies. Non-human animals can make use of the geometric shape of an environment for spatial reorientation and in some circumstances reliance on purely geometric information (metric properties and sense) can overcome use of local featural information. Animals reared in home cages of different geometric shapes proved to be equally capable of learning and performing navigational tasks based on geometric information. The findings suggest that effective use of geometric information for spatial reorientation does not require experience in environments with right angles and metrically distinct surfaces. © 2009 Elsevier Ltd. All rights reserved.},
	number = {1},
	journal = {Current Opinion in Neurobiology},
	author = {Vallortigara, Giorgio and Sovrano, Valeria Anna and Chiandetti, Cinzia},
	year = {2009},
	pages = {20--26},
}

@article{emerson_continuity_2016,
	title = {Continuity and change in children’s longitudinal neural response to numbers},
	volume = {18},
	doi = {10.1111/desc.12215.Continuity},
	number = {2},
	author = {Emerson, Robert W and Cantlon, Jessica F},
	year = {2016},
	pages = {314--326},
}

@article{spelke_core_2007,
	title = {Core knowledge},
	volume = {10},
	issn = {1363755X},
	doi = {10.1111/j.1467-7687.2007.00569.x},
	abstract = {Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners. Each system has deep roots in human phylogeny and ontogeny, and it guides and shapes the mental lives of adults. Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits. © 2007 The Authors.},
	number = {1},
	journal = {Developmental Science},
	author = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
	year = {2007},
	pages = {89--96},
}

@article{dehaene-lambertz_nature_2006,
	title = {Nature and nurture in language acquisition: anatomical and functional brain-imaging studies in infants},
	volume = {29},
	issn = {01662236},
	doi = {10.1016/j.tins.2006.05.011},
	abstract = {Speech processing in adults relies on precise and specialized networks, located primarily in the left hemisphere. Behavioral studies in infants indicate that a considerable amount of language learning already takes place in the first year of life in the domains of phonology, prosody and word segmentation. Thanks to neuroimaging, we can move beyond behavioral methods and examine how the infant brain processes verbal stimuli before learning. These studies reveal a structural and functional organization close to what is described in adults and suggest a strong bias for speech processing in these regions that might guide infants as they discover the properties of their native language, although no evidence can be provided as yet for speech specificity of such networks. This review is part of the INMED/TINS special issue Nature and nurture in brain development and neurological disorders, based on presentations at the annual INMED/TINS symposium (http://inmednet.com/). © 2006 Elsevier Ltd. All rights reserved.},
	number = {7},
	journal = {Trends in Neurosciences},
	author = {Dehaene-Lambertz, Ghislaine and Hertz-Pannier, Lucie and Dubois, Jessica},
	year = {2006},
	pages = {367--373},
}

@article{dicarlo_untangling_2007,
	title = {Untangling invariant object recognition},
	volume = {11},
	issn = {13646613},
	doi = {10.1016/j.tics.2007.06.010},
	abstract = {Despite tremendous variation in the appearance of visual objects, primates can recognize a multitude of objects, each in a fraction of a second, with no apparent effort. However, the brain mechanisms that enable this fundamental ability are not understood. Drawing on ideas from neurophysiology and computation, we present a graphical perspective on the key computational challenges of object recognition, and argue that the format of neuronal population representation and a property that we term 'object tangling' are central. We use this perspective to show that the primate ventral visual processing stream achieves a particularly effective solution in which single-neuron invariance is not the goal. Finally, we speculate on the key neuronal mechanisms that could enable this solution, which, if understood, would have far-reaching implications for cognitive neuroscience. © 2007 Elsevier Ltd. All rights reserved.},
	number = {8},
	journal = {Trends in Cognitive Sciences},
	author = {DiCarlo, James J. and Cox, David D.},
	year = {2007},
	pmid = {17631409},
	pages = {333--341},
}

@article{feinman_generating_2020,
	title = {Generating new concepts with hybrid neuro-symbolic models},
	url = {http://arxiv.org/abs/2003.08978},
	abstract = {Human conceptual knowledge supports the ability to generate novel yet highly structured concepts, and the form of this conceptual knowledge is of great interest to cognitive scientists. One tradition has emphasized structured knowledge, viewing concepts as embedded in intuitive theories or organized in complex symbolic knowledge structures. A second tradition has emphasized statistical knowledge, viewing conceptual knowledge as an emerging from the rich correlational structure captured by training neural networks and other statistical models. In this paper, we explore a synthesis of these two traditions through a novel neuro-symbolic model for generating new concepts. Using simple visual concepts as a testbed, we bring together neural networks and symbolic probabilistic programs to learn a generative model of novel handwritten characters. Two alternative models are explored with more generic neural network architectures. We compare each of these three models for their likelihoods on held-out character classes and for the quality of their productions, finding that our hybrid model learns the most convincing representation and generalizes further from the training observations.},
	author = {Feinman, Reuben and Lake, Brenden M.},
	year = {2020},
	note = {arXiv: 2003.08978},
}

@article{rumelhart_learning_nodate,
	title = {On {Learning} the {Past} {Tenses} of {English} {Verbs}},
	journal = {Foundations},
	author = {Rumelhart, D and McClelland, J},
}

@article{mccloskey_networks_nodate,
	title = {{NETWORKS} {AND} {THEORIES}: {The} {Place} of {Connectionism} in {Cognitive} {Science}},
	author = {McCloskey, Michael},
}

@article{lakoff_women_nodate,
	title = {Women, {Fire}, and {Dangerous} {Things} {What} {Categories} {Reveal} about the {Mind}},
	author = {Lakoff, George},
}

@article{smolensky_foundations_1991,
	title = {Foundations of {Cognitive} {Science} {Syllabus}},
	volume = {14},
	issn = {0210-1963},
	doi = {10.5840/teachphil199114464},
	number = {4},
	journal = {Teaching Philosophy},
	author = {Smolensky, Paul},
	year = {1991},
	pages = {436--439},
}

@article{smolensky_connectionism_1991,
	title = {Connectionism, constituency and the language of thought},
	abstract = {(from the chapter) discuss J. Fodor's and Z. Pylyshyn's argumentation concerning connectionism, see PA, Vol 83:00311; 95-230057-002 and the abuse of the term 'implementation' focus on the crux of their argument, which turns on the compositional structure of mental states develop in some detail the argument that, unlike simple associationist models connectionist models using distributed representations can embody compositionality at the same time as providing a new cognitive architecture that is not an implementation of a Classical language of thought argue that the debate surrounding compositionality illustrates the general point that by finding new formal instantiations of basic computational notions in the category of continuous mathematics, connectionism can open up genuinely new and powerful accounts of computation and cognition that go well beyond the limited progress that can be afforded by the kind of implementationalist strategy that Fodor and Pylyshyn advocate (PsycINFO Database Record (c) 2000 APA, all rights reserved).},
	journal = {Thought A Review Of Culture And Idea},
	author = {Smolensky, Paul},
	year = {1991},
	pages = {286--306},
}

@article{fodor_connectionism_nodate,
	title = {Connectionism and cognitive architecture: {A} critical analysis},
	url = {http://www.copyright.gov/},
	abstract = {Characteristics of a community},
	journal = {Handbook of Research on Teaching},
	author = {Fodor, Jerry and Pylyshyn, Zenon},
	pages = {209--225},
}

@article{luong_effective_2015,
	title = {Effective approaches to attention-based neural machine translation},
	doi = {10.18653/v1/d15-1166},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	journal = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
	author = {Luong, Minh Thang and Pham, Hieu and Manning, Christopher D.},
	year = {2015},
	note = {arXiv: 1508.04025
ISBN: 9781941643327},
	pages = {1412--1421},
}

@article{hupkes_compositionality_2019,
	title = {Compositionality decomposed: how do neural networks generalise?},
	volume = {67},
	url = {http://arxiv.org/abs/1908.08351},
	abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
	author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
	year = {2019},
	note = {arXiv: 1908.08351},
}

@article{loula_rearranging_2019,
	title = {Rearranging the {Familiar}: {Testing} {Compositional} {Generalization} in {Recurrent} {Networks}},
	doi = {10.18653/v1/w18-5413},
	abstract = {Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it's seen as key to humans' capacity for generalization in language. Recent work has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool, requiring models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as "around" and "right") in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of "X around right" to "jump around right"), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of "around right" from those of "right" and "around").},
	author = {Loula, João and Baroni, Marco and Lake, Brenden},
	year = {2019},
	note = {arXiv: 1807.07545},
	pages = {108--114},
}

@article{tenenbaum_how_2011,
	title = {How to grow a mind: {Statistics}, structure, and abstraction},
	volume = {331},
	issn = {00368075},
	doi = {10.1126/science.1192788},
	abstract = {In coming to understand the world - in learning concepts, acquiring language, and grasping causal relations - our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?},
	number = {6022},
	journal = {Science},
	author = {Tenenbaum, Joshua B. and Kemp, Charles and Griffiths, Thomas L. and Goodman, Noah D.},
	year = {2011},
	pmid = {21393536},
	pages = {1279--1285},
}

@article{lake_generalization_2018,
	title = {Generalization without systematicity: {On} the compositional skills of sequence-to-sequence recurrent networks},
	volume = {7},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generaliza-tions when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
	journal = {35th International Conference on Machine Learning, ICML 2018},
	author = {Lake, Brenden and Baroni, Marco},
	year = {2018},
	note = {arXiv: 1711.00350
ISBN: 9781510867963},
	pages = {4487--4499},
}

@article{schulman_opinionated_2020,
	title = {An opinionated guide to {ML} {Research}},
	url = {https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/},
	abstract = {In this essay, I provide some advice to up-and-coming researchers in machine learning (ML), based on my experience doing research and advising others. The advice covers how to choose problems and organize your time.},
	author = {Schulman, John},
	year = {2020},
	pages = {1--8},
}

@book{nielsen_principles_2011,
	title = {Principles of effective research},
	isbn = {978-1-931971-87-4},
	abstract = {This essay is intended as a letter to both myself and others, to hold up in the sharpest possible terms an ideal of research I believe is worth working toward. I’ve de- liberately limited the essay to ten pages, hoping that the resulting omissions are compensated by the forced brevity. This is a rather personal essay; it’s not the sort of thing I’d usually make publicly available. I’ve made the essay public in order to heighten my commitment to the project, and in the hope that other people will find it stimulating, and perhaps offer some thoughts of their own.},
	author = {Nielsen, Michael},
	year = {2011},
	note = {Publication Title: Proceedings of the 20th USENIX Security Symposium},
}

@article{hamming_you_2010,
	title = {You and {Your} {Research}},
	doi = {10.4324/9780203450710_chapter_30},
	abstract = {The art of research from horse's mouth. Richard Hamming explaining his gardening life and ways.},
	journal = {The Art of Doing Science and Engineering},
	author = {Hamming, Richard W},
	year = {2010},
	pages = {349--360},
}

@article{hale_finding_2018,
	title = {Finding syntax in human encephalography with beam search},
	volume = {1},
	doi = {10.18653/v1/p18-1254},
	abstract = {Recurrent neural network grammars (RNNGs) are generative models of (tree, string) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophys-iological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.},
	number = {2014},
	journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Hale, John and Dyer, Chris and Kuncoro, Adhiguna and Brennan, Jonathan R.},
	year = {2018},
	note = {arXiv: 1806.04127
ISBN: 9781948087322},
	pages = {2727--2736},
}

@article{yu_neural_2018,
	title = {{NEURAL} {SYMBOLIC} {READER}: {SCALABLE} {INTEGRA}- {TION} {OF} {DISTRIBUTED} {AND} {SYMBOLIC} {REPRESENTA}- {TIONS} {FOR} {READING} {COMPREHENSION}},
	author = {Yu, Adams Wei},
	year = {2018},
	pages = {1--15},
}

@article{lample_deep_2019,
	title = {Deep {Learning} for {Symbolic} {Mathematics}},
	url = {http://arxiv.org/abs/1912.01412},
	abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
	author = {Lample, Guillaume and Charton, François},
	year = {2019},
	note = {arXiv: 1912.01412},
	pages = {1--24},
}

@article{perez_film_2018,
	title = {{FiLM}: {Visual} reasoning with a general conditioning layer},
	abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
	journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
	author = {Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
	year = {2018},
	note = {arXiv: 1709.07871
ISBN: 9781577358008},
	pages = {3942--3951},
}

@article{hamrick_analogues_2019,
	title = {Analogues of mental simulation and imagination in deep learning},
	volume = {29},
	issn = {23521546},
	url = {https://doi.org/10.1016/j.cobeha.2018.12.011},
	doi = {10.1016/j.cobeha.2018.12.011},
	abstract = {Mental simulation — the capacity to imagine what will or what could be — is a salient feature of human cognition, playing a key role in a wide range of cognitive abilities. In artificial intelligence, the last few years have seen the development of methods which are analogous to mental models and mental simulation. This paper outlines recent methods in deep learning for constructing such models from data and learning to use them via reinforcement learning, and compares such approaches to human mental simulation. Model-based methods in deep learning can serve as powerful tools for building and scaling cognitive models. However, a number of challenges remain in matching the capacity of human mental simulation for efficiency, compositionality, generalization, and creativity.},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Hamrick, Jessica B.},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	pages = {8--16},
}

@article{kemp_discovery_2008,
	title = {The discovery of structural form},
	volume = {105},
	issn = {00278424},
	doi = {10.1073/pnas.0802631105},
	abstract = {Algorithms for finding structure in data have become increasingly important both as tools for scientific data analysis and as models of human learning, yet they suffer from a critical limitation. Scientists discover qualitatively new forms of structure in observed data: For instance, Linnaeus recognized the hierarchical organization of biological species, and Mendeleev recognized the periodic structure of the chemical elements. Analogous insights play a pivotal role in cognitive development: Children discover that object category labels can be organized into hierarchies, friendship networks are organized into cliques, and comparative relations (e.g., "bigger than" or "better than") respect a transitive order. Standard algorithms, however, can only learn structures of a single form that must be specified in advance: For instance, algorithms for hierarchical clustering create tree structures, whereas algorithms for dimensionality-reduction create low-dimensional spaces. Here, we present a computational model that learns structures of many different forms and that discovers which form is best for a given dataset. The model makes probabilistic inferences over a space of graph grammars representing trees, linear orders, multidimensional spaces, rings, dominance hierarchies, cliques, and other forms and successfully discovers the underlying structure of a variety of physical, biological, and social domains. Our approach brings structure learning methods closer to human abilities and may lead to a deeper computational understanding of cognitive development. © 2008 by The National Academy of Sciences of the USA.},
	number = {31},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kemp, Charles and Tenenbaum, Joshua B.},
	year = {2008},
	pages = {10687--10692},
}

@article{lake_emergence_2018,
	title = {The {Emergence} of {Organizing} {Structure} in {Conceptual} {Representation}},
	volume = {42},
	issn = {15516709},
	doi = {10.1111/cogs.12580},
	abstract = {Both scientists and children make important structural discoveries, yet their computational underpinnings are not well understood. Structure discovery has previously been formalized as probabilistic inference about the right structural form—where form could be a tree, ring, chain, grid, etc. (Kemp \& Tenenbaum, 2008). Although this approach can learn intuitive organizations, including a tree for animals and a ring for the color circle, it assumes a strong inductive bias that considers only these particular forms, and each form is explicitly provided as initial knowledge. Here we introduce a new computational model of how organizing structure can be discovered, utilizing a broad hypothesis space with a preference for sparse connectivity. Given that the inductive bias is more general, the model's initial knowledge shows little qualitative resemblance to some of the discoveries it supports. As a consequence, the model can also learn complex structures for domains that lack intuitive description, as well as predict human property induction judgments without explicit structural forms. By allowing form to emerge from sparsity, our approach clarifies how both the richness and flexibility of human conceptual organization can coexist.},
	journal = {Cognitive Science},
	author = {Lake, Brenden M. and Lawrence, Neil D. and Tenenbaum, Joshua B.},
	year = {2018},
	note = {arXiv: 1611.09384},
	pages = {809--832},
}

@article{yildirim_efficient_2018,
	title = {Efficient inverse graphics in biological face processing},
	url = {https://www.biorxiv.org/content/10.1101/282798v1},
	doi = {10.1101/282798},
	abstract = {The visual system must not only recognize and localize objects, but perform much richer inferences about the underlying causes in the world that give rise to observed sense data. Analyzing scenes by inverting causal generative models, also known as "analysis-by-synthesis", has a long history in computational vision, and these models have some behavioral support, but they are typically too slow to support online perception and have no known mapping to actual neural circuits. Here we present a neurally plausible model for efficiently inverting generative models of images and test it as a precise account of one aspect of high-level vision, the perception of faces. The model is based on a deep neural network that learns to invert a three-dimensional (3D) face graphics program in a single fast feedforward pass. It successfully explains both human behavioral data and multiple levels of neural processing in non-human primates, as well as a classic illusion, the "hollow face" effect. The model also fits qualitatively better than state-of-the-art computer vision models, and suggests an interpretable reverse-engineering account of how images are transformed into scene percepts in the primate ventral stream.},
	journal = {bioRxiv},
	author = {Yildirim, Ilker and Freiwald, Winrich and Tenenbaum, Joshua},
	year = {2018},
	pages = {282798},
}

@article{wallis_decoding_2018,
	title = {Decoding {Cognitive} {Processes} from {Neural} {Ensembles}},
	volume = {22},
	issn = {1879307X},
	url = {http://dx.doi.org/10.1016/j.tics.2018.09.002},
	doi = {10.1016/j.tics.2018.09.002},
	abstract = {An intrinsic difficulty in studying cognitive processes is that they are unobservable states that exist in between observable responses to the sensory environment. Cognitive states must be inferred from indirect behavioral measures. Neuroscience potentially provides the tools necessary to measure cognitive processes directly, but it is challenged on two fronts. First, neuroscientific measures often lack the spatiotemporal resolution to identify the neural computations that underlie a cognitive process. Second, the activity of a single neuron, which is the fundamental building block of neural computation, is too noisy to provide accurate measurements of a cognitive process. In this paper, I examine recent developments in neurophysiological recording and analysis methods that provide a potential solution to these problems.},
	number = {12},
	journal = {Trends in Cognitive Sciences},
	author = {Wallis, Joni D.},
	year = {2018},
	pmid = {30279136},
	note = {Publisher: Elsevier Ltd},
	pages = {1091--1102},
}

@article{botvinick_reinforcement_2019,
	title = {Reinforcement {Learning}, {Fast} and {Slow}},
	volume = {23},
	issn = {1879307X},
	url = {https://doi.org/10.1016/j.tics.2019.02.006},
	doi = {10.1016/j.tics.2019.02.006},
	abstract = {Deep reinforcement learning (RL)methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.},
	number = {5},
	journal = {Trends in Cognitive Sciences},
	author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	pages = {408--422},
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {14691825},
	doi = {10.1017/S0140525X16001837},
	abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	number = {2012},
	journal = {Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	year = {2017},
	note = {arXiv: 1604.00289},
	pages = {1--58},
}

@article{hofstadter_waking_1985,
	title = {Waking up from the {Boolean} dream, or, subcognition as computation},
	issn = {0036-8733},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Waking+up+from+the+Boolean+dream,+or,+subcognition+as+computation#0},
	doi = {10.1038/scientificamerican0581-15},
	abstract = {THE philosopher John Searle has recently made quite a stir in the cognitive-science and philosophy-of-mind circles with his-celebrated article “Minds, Brains, and Programs”, in which he puts forth his “Chinese room” thought experiment. Its purpose is to reveal as illusory the aims of artificial intelligence, and particularly to discredit what he labels strong AI-the belief that a programmed computer can, in principle, be conscious. Various synonymous phrases could be substituted for “be conscious" here, such as: * think; * have a soul (in a humanistic rather than a religious sense); * have an inner lfe; * have semantics (as distinguished from “mere syntax”); * have content (as distinguished from “mere form”); * have intentionality; * be something it is like something to be (a weird phrase due to T. Nagel); * have personhood; and others. Each of these phrases has its own peculiar set of connotations and imagery attached to it, as well as its own history and proponents. For our purposes, however, we shall consider them all as equivalent, and lump them all tbgether, so that the claim of strong AI now becomes very strong indeed. 63},
	journal = {Metamagical themas: Questing for the essence of mind and pattern},
	author = {Hofstadter, Douglas R.},
	year = {1985},
	note = {ISBN: 9780465045662},
	pages = {631--665},
}

@article{simon_theory_1988,
	title = {The {Theory} of {Human} {Problem} {Solving}},
	doi = {1-55860-013-2},
	journal = {Cognitive Science},
	author = {Simon, A and Newell, H},
	year = {1988},
}

@article{anderson_human_2005,
	title = {Human symbol manipulation within an integrated cognitive architecture},
	volume = {29},
	issn = {03640213},
	doi = {10.1207/s15516709cog0000_22},
	abstract = {This article describes the Adaptive Control of Thought-Rational (ACT-R) cognitive architecture (Anderson et al., 2004; Anderson \& Lebiere, 1998) and its detailed application to the learning of algebraic symbol manipulation. The theory is applied to modeling the data from a study by Qin, Anderson, Silk, Stenger, \& Carter (2004) in which children learn to solve linear equations and perfect their skills over a 6-day period. Functional MRI data show that: (a) a motor region tracks the output of equation solutions, (b) a prefrontal region tracks the retrieval of declarative information, (c) a parietal region tracks the transformation of mental representations of the equation, (d) an anterior cingulate region tracks the setting of goal information to control the information flow, and (e) a caudate region tracks the firing of productions in the ACT-R model. The article concludes with an architectural comparison of the competence children display in this task and the competence that monkeys have shown in tasks that require manipulations of sequences of elements. Copyright © 2005 Cognitive Science Society, Inc. All rights reserved.},
	number = {3},
	journal = {Cognitive Science},
	author = {Anderson, John R.},
	year = {2005},
	pages = {313--341},
}

@article{pylyshyn_return_2003,
	title = {Return of the mental image: {Are} there really pictures in the brain?},
	volume = {7},
	issn = {13646613},
	doi = {10.1016/S1364-6613(03)00003-2},
	abstract = {In the past decade there has been renewed interest in the study of mental imagery. Emboldened by new findings from neuroscience, many people have revived the idea that mental imagery involves a special format of thought, one that is pictorial in nature. But the evidence and the arguments that exposed deep conceptual and empirical problems in the picture theory over the past 300 years have not gone away. I argue that the new evidence from neural imaging and clinical neuropsychology does little to justify this recidivism because it does not address the format of mental images. I also discuss some reasons why the picture theory is so resistant to counterarguments and suggest ways in which non-pictorial theories might account for the apparent spatial nature of images.},
	number = {3},
	journal = {Trends in Cognitive Sciences},
	author = {Pylyshyn, Zenon},
	year = {2003},
	pmid = {12639692},
	pages = {113--118},
}

@article{brain_loss_1926,
	title = {Loss of {Visualization}},
	volume = {49},
	issn = {14602156},
	doi = {10.1093/brain/49.2.247},
	number = {2},
	journal = {Brain},
	author = {Brain, Russell},
	year = {1926},
	pages = {247},
}

@article{zeman_lives_2015,
	title = {Lives without imagery - {Congenital} aphantasia},
	volume = {73},
	issn = {19738102},
	url = {http://dx.doi.org/10.1016/j.cortex.2015.05.019},
	doi = {10.1016/j.cortex.2015.05.019},
	journal = {Cortex},
	author = {Zeman, Adam and Dewar, Michaela and Della Sala, Sergio},
	year = {2015},
	note = {Publisher: Elsevier Srl.},
	pages = {378--380},
}

@article{marr_vision_1982,
	title = {Vision},
	author = {Marr, D},
	year = {1982},
}

@article{richards_deep_2019,
	title = {A deep learning framework for neuroscience},
	volume = {22},
	issn = {15461726},
	url = {http://dx.doi.org/10.1038/s41593-019-0520-2},
	doi = {10.1038/s41593-019-0520-2},
	abstract = {Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In artificial neural networks, the three components specified by design are the objective functions, the learning rules and the architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress.},
	number = {11},
	journal = {Nature Neuroscience},
	author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and de Berker, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, João and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
	year = {2019},
	note = {Publisher: Springer US},
	pages = {1761--1770},
}

@article{siegmund_understanding_2014,
	title = {Understanding understanding source code with functional magnetic resonance imaging},
	issn = {02705257},
	doi = {10.1145/2568225.2568252},
	abstract = {Program comprehension is an important cognitive process that inherently eludes direct measurement. Thus, researchers are struggling with providing suitable programming languages, tools, or coding conventions to support developers in their everyday work. In this paper, we explore whether functional magnetic resonance imaging (fMRI), which is well established in cognitive neuroscience, is feasible to soundly measure program comprehension. In a controlled experiment, we observed 17 participants inside an fMRI scanner while they were comprehending short source-code snippets, which we contrasted with locating syntax errors. We found a clear, distinct activation pattern of five brain regions, which are related to working memory, attention, and language processing - -all processes that fit well to our understanding of program comprehension. Our results encourage us and, hopefully, other researchers to use fMRI in future studies to measure program comprehension and, in the long run, answer questions, such as: Can we predict whether someone will be an excellent programmer? How effective are new languages and tools for program understanding? How should we train programmers?},
	number = {1},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Siegmund, Janet and Kästner, Christian and Apel, Sven and Parnin, Chris and Bethmann, Anja and Leich, Thomas and Saake, Gunter and Brechmann, André},
	year = {2014},
	pages = {378--389},
}

@article{sejnowski_unreasonable_2020,
	title = {The unreasonable effectiveness of deep learning in artificial intelligence},
	issn = {0027-8424},
	doi = {10.1073/pnas.1907373117},
	abstract = {Deep learning networks have been trained to recognize speech, caption photographs, and translate text between languages at high levels of performance. Although applications of deep learning networks to real-world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and nonconvex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures, and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Sejnowski, Terrence J.},
	year = {2020},
	pages = {201907373},
}

@article{gupta_neural_2019,
	title = {Neural {Module} {Networks} for {Reasoning} over {Text}},
	url = {http://arxiv.org/abs/1912.04971},
	abstract = {Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning. Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.},
	author = {Gupta, Nitish and Lin, Kevin and Roth, Dan and Singh, Sameer and Gardner, Matt},
	year = {2019},
	note = {arXiv: 1912.04971},
	pages = {1--17},
}

@article{mccloskey_place_1991,
	title = {The {Place} of {Connectionism} in {Cognitive} {Science}},
	volume = {2},
	number = {6},
	journal = {Psychological Science},
	author = {McCloskey, Michael},
	year = {1991},
	pages = {387--396},
}

@article{lillicrap_what_2019,
	title = {What does it mean to understand a neural network?},
	url = {http://arxiv.org/abs/1907.06374},
	abstract = {We can define a neural network that can learn to recognize objects in less than 100 lines of code. However, after training, it is characterized by millions of weights that contain the knowledge about many object types across visual scenes. Such networks are thus dramatically easier to understand in terms of the code that makes them than the resulting properties, such as tuning or connections. In analogy, we conjecture that rules for development and learning in brains may be far easier to understand than their resulting properties. The analogy suggests that neuroscience would benefit from a focus on learning and development.},
	number = {July},
	author = {Lillicrap, Timothy P. and Kording, Konrad P.},
	year = {2019},
	note = {arXiv: 1907.06374},
	pages = {1--9},
}

@article{chollet_measure_2019,
	title = {On the {Measure} of {Intelligence}},
	url = {http://arxiv.org/abs/1911.01547},
	abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
	author = {Chollet, François},
	year = {2019},
	note = {arXiv: 1911.01547},
	pages = {1--64},
}

@article{arabshahi_memory_2019,
	title = {Memory {Augmented} {Recursive} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1911.01545},
	abstract = {Recursive neural networks have shown an impressive performance for modeling compositional data compared to their recurrent counterparts. Although recursive neural networks are better at capturing long range dependencies, their generalization performance starts to decay as the test data becomes more compositional and potentially deeper than the training data. In this paper, we present memory-augmented recursive neural networks to address this generalization performance loss on deeper data points. We augment Tree-LSTMs with an external memory, namely neural stacks. We define soft push and pop operations for filling and emptying the memory to ensure that the networks remain end-to-end differentiable. In order to assess the effectiveness of the external memory, we evaluate our model on a neural programming task introduced in the literature called equation verification. Our results indicate that augmenting recursive neural networks with external memory consistently improves the generalization performance on deeper data points compared to the state-of-the-art Tree-LSTM by up to 10\%.},
	author = {Arabshahi, Forough and Lu, Zhichu and Singh, Sameer and Anandkumar, Animashree},
	year = {2019},
	note = {arXiv: 1911.01545},
}

@article{baroni_linguistic_2020,
	title = {Linguistic generalization and compositionality in modern artificial neural networks},
	volume = {375},
	issn = {14712970},
	url = {http://arxiv.org/abs/1904.00157%0Ahttp://dx.doi.org/10.1098/rstb.2019.0307},
	doi = {10.1098/rstb.2019.0307},
	abstract = {In the lastdecade, deep artificial neural networks have achievedastoundingperformance in many natural language-processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: Are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language-processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they donot relyonsystematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language.},
	number = {1791},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Baroni, Marco},
	year = {2020},
	note = {arXiv: 1904.00157},
}

@article{danihelka_associative_2016,
	title = {Associative long short-term memory},
	volume = {4},
	abstract = {We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: As they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.},
	journal = {33rd International Conference on Machine Learning, ICML 2016},
	author = {Danihelka, Ivo and Wayne, Greg and Una, Benigno and Kalchbrenner, Nal and Graves, Alex},
	year = {2016},
	note = {arXiv: 1602.03032
ISBN: 9781510829008},
	pages = {2929--2938},
}

@article{sukhbaatar_end--end_2015,
	title = {End-to-end memory networks},
	volume = {2015-Janua},
	issn = {10495258},
	abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	year = {2015},
	note = {arXiv: 1503.08895},
	pages = {2440--2448},
}

@article{weston_memory_2015,
	title = {Memory networks},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	year = {2015},
	note = {arXiv: 1410.3916},
	pages = {1--15},
}

@article{brooks_intelligence_2019,
	title = {Intelligence without reason},
	doi = {10.7591/9781501742767-004},
	abstract = {Computers and Thought are the t o categories that together de?ne Arti?cial In w telligence as a discipline? It is generally accepted that w ork in Arti?cial In telligence o er the last thirt v yy ears has had a strong in?uence on aspects of computer arc hitectures? In this paper w e also mak e the con erse claim? that the state of v computer arc hitecture has been a strong in?uence on our models of though t? The V on Neumann model of computation has lead Arti?cial In telligence in particular directions? In telligence in biological systems is completely di?eren t? Recen tw ork in beha vior?based Arti?cial In telligence has produced new models of in telligence that are m h closer in spirit to biological systems? The non?V uc on Neumann computational models they use share man yc haracteristics with biological computation},
	number = {1293},
	journal = {University and Military Research},
	author = {Brooks, Rodney A.},
	year = {2019},
	pages = {15--24},
}

@article{huang_distilling_2019,
	title = {Distilling {Neural} {Representations} of {Data} {Structure} {Manipulation} using {fMRI} and {fNIRS}},
	volume = {2019-May},
	issn = {02705257},
	doi = {10.1109/ICSE.2019.00053},
	abstract = {Data structures permeate many aspects of software engineering, but their associated human cognitive processes are not thoroughly understood. We leverage medical imaging and insights from the psychological notion of spatial ability to decode the neural representations of several fundamental data structures and their manipulations. In a human study involving 76 participants, we examine list, array, tree, and mental rotation tasks using both functional near-infrared spectroscopy (fNIRS) and functional magnetic resonance imaging (fMRI). We find a nuanced relationship: data structure and spatial operations use the same focal regions of the brain but to different degrees. They are related but distinct neural tasks. In addition, more difficult computer science problems induce higher cognitive load than do problems of pure spatial reasoning. Finally, while fNIRS is less expensive and more permissive, there are some computing-relevant brain regions that only fMRI can reach.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Huang, Yu and Liu, Xinyu and Krueger, Ryan and Santander, Tyler and Hu, Xiaosu and Leach, Kevin and Weimer, Westley},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728108698},
	pages = {396--407},
}

@article{brooks_intelligence_1991,
	title = {Intelligence without representation},
	volume = {47},
	issn = {00043702},
	doi = {10.1016/0004-3702(91)90053-M},
	abstract = {Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporate-everything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments. © 1991.},
	number = {1-3},
	journal = {Artificial Intelligence},
	author = {Brooks, Rodney A.},
	year = {1991},
	pages = {139--159},
}

@article{ganin_synthesizing_2018,
	title = {Synthesizing {Programs} for {Images} using {Reinforced} {Adversarial} {Learning}},
	volume = {4},
	abstract = {Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and Tenderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, OMNIGLOT, CELEBA) and synthetic 3D datasets. A video of the agent can be found at https://youtu.be/iSyvwAwa7vk.},
	journal = {35th International Conference on Machine Learning, ICML 2018},
	author = {Ganin, Yaroslav and Kulkarni, Tejas and Babuschkin, Igor and Eslami, S. M.Ali and Vinyals, Oriol},
	year = {2018},
	note = {arXiv: 1804.01118
ISBN: 9781510867963},
	pages = {2684--2695},
}

@article{zaremba_learning_2014,
	title = {Learning to {Execute}},
	url = {http://arxiv.org/abs/1410.4615},
	abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99\% accuracy.},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	year = {2014},
	note = {arXiv: 1410.4615},
	pages = {1--25},
}

@article{kaiser_neural_2016,
	title = {Neural {GPUs} learn algorithms},
	abstract = {Learning an algorithm from examples is a fundamental problem that has been widely studied. It has been addressed using neural networks too, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with up-to 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.},
	journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
	author = {Kaiser, Łukasz and Sutskever, Ilya},
	year = {2016},
	note = {arXiv: 1511.08228},
	pages = {1--9},
}

@article{sun_program_2020,
	title = {Program {Guided} {Agent}},
	author = {Sun, Shao-hua and Wu, Te-lin and Lim, Joseph J},
	year = {2020},
	pages = {1--24},
}

@article{leahu_interactionist_2008,
	title = {Interactionist {AI} and the promise of ubicomp, or, how to put your box in the world without putting the world in your box},
	issn = {1605581364},
	url = {http://dl.acm.org/citation.cfm?id=1409654%5Cnhttp://portal.acm.org/citation.cfm?doid=1409635.1409654},
	doi = {10.1145/1409635.1409654},
	abstract = {In many ways, the central problem of ubiquitous computing -- how computational systems can make sense of and respond sensibly to a complex, dynamic environment laden with human meaning -- is identical to that of Artificial Intelligence (AI). Indeed, some of the central challenges that ubicomp currently faces in moving from prototypes that work in restricted environments to the complexity of real-world environments -- e.g. difficulties in scalability, integration, and fully formalizing context -- echo some of the major issues that have challenged AI researchers over the history of their field. In this paper, we explore a key moment in AI's history where researchers grappled directly with these issues, resulting in a variety of novel technical solutions within AI. We critically reflect on six strategies from this history to suggest technical solutions for how to approach the challenge of building real-world, usable solutions in ubicomp today.},
	journal = {Proceedings of the 10th international conference on Ubiquitous computing - UbiComp '08},
	author = {Leahu, Lucian and Sengers, Phoebe and Mateas, Michael},
	year = {2008},
	note = {ISBN: 9781605581361},
	pages = {134},
}

@misc{noauthor_han_nodate,
	title = {Han - {Variational} {Autoencoder} {Supports} {Direct} {Visual} {Reconstruction}.pdf},
}

@article{eslami_patterns_2012,
	title = {Patterns for {Research} in {Machine} {Learning}},
	author = {Eslami, Ali},
	year = {2012},
}

@article{ritter_cognitive_2017,
	title = {Cognitive {Psychology} for {Deep} {Neural} {Networks}: {A} {Shape} {Bias} {Case} {Study}},
	issn = {1939-1471},
	url = {http://arxiv.org/abs/1706.08606},
	doi = {10.1037/a0037840},
	abstract = {Deep neural networks (DNNs) have achieved unprecedented performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. This has caused a recent surge of interest in methods for rendering modern neural systems more interpretable. In this work, we propose to address the interpretability problem in modern DNNs using the rich history of problem descriptions, theories and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	year = {2017},
	pmid = {25330329},
	note = {arXiv: 1706.08606
ISBN: 1939-1471(Electronic);0033-295X(Print)},
}

@article{workshop_how_2011,
	title = {How to send and reply to email},
	author = {Workshop, Scheme and Rand, Accepting Submissions},
	year = {2011},
	pages = {1--7},
}

@article{bengio_consciousness_2017,
	title = {The {Consciousness} {Prior}},
	url = {http://arxiv.org/abs/1709.08568},
	abstract = {A new prior is proposed for representation learning, which can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by the phenomenon of consciousness seen as the formation of a low-dimensional combination of a few concepts constituting a conscious thought, i.e., consciousness as awareness at a particular time instant. This provides a powerful constraint on the representation in that such low-dimensional thought vectors can correspond to statements about reality which are true, highly probable, or very useful for taking decisions. The fact that a few elements of the current state can be combined into such a predictive or useful statement is a strong constraint and deviates considerably from the maximum likelihood approaches to modelling data and how states unfold in the future based on an agent's actions. Instead of making predictions in the sensory (e.g. pixel) space, the consciousness prior allows the agent to make predictions in the abstract space, with only a few dimensions of that space being involved in each of these predictions. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in the form of facts and rules, although the conscious states may be richer than what can be expressed easily in the form of a sentence, a fact or a rule.},
	number = {1},
	author = {Bengio, Yoshua},
	year = {2017},
	note = {arXiv: 1709.08568},
	pages = {1--4},
}

@article{lampinen_improving_2017,
	title = {Improving image generative models with human interactions},
	url = {http://arxiv.org/abs/1709.10459},
	abstract = {GANs provide a framework for training generative models which mimic a data distribution. However, in many cases we wish to train these generative models to optimize some auxiliary objective function within the data it generates, such as making more aesthetically pleasing images. In some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction. Here, we develop a system for efficiently improving a GAN to target an objective involving human interaction, specifically generating images that increase rates of positive user interactions. To improve the generative model, we build a model of human behavior in the targeted domain from a relatively small set of interactions, and then use this behavioral model as an auxiliary loss function to improve the generative model. We show that this system is successful at improving positive interaction rates, at least on simulated data, and characterize some of the factors that affect its performance.},
	author = {Lampinen, Andrew Kyle and So, David and Eck, Douglas and Bertsch, Fred},
	year = {2017},
	note = {arXiv: 1709.10459},
}

@article{kim_ibcm_2015,
	title = {{iBCM} : {Interactive} {Bayesian} {Case} {Model} {Empowering} {Humans} via {Intuitive} {Interaction}},
	abstract = {Clustering methods optimize the partitioning of data points with respect to an internal metric, such as likelihood, in order to approximate the goodness of clustering. However, this internal metric does not necessarily translate into effective clustering from the user’s perspective. This work presents the interactive Bayesian Case Model (iBCM), a model that opens a communication channel between the clustering model and the user. Users can provide direct input to iBCM in order to achieve effective clustering results, and iBCM optimizes the clustering by creating a balance between what the data indicate and what makes the most sense to the user. This model provides feedback for users and does not assume any prior knowledge of machine learning on their part. We provide quantitative evidence that users are able to obtain more satisfactory clustering results through iBCM than without an interactive model. We also demonstrate the use of this method in a real-world setting where computer language class teachers utilize iBCM to cluster students’ coding assignments for grading.},
	author = {Kim, Been and Glassman, Elena and Johnson, Brittney and Kim, Been and Glassman, Elena and Johnson, Brittney and Shah, Julie},
	year = {2015},
}

@article{burda_importance_2015,
	title = {Importance {Weighted} {Autoencoders}},
	issn = {1312.6114v10},
	url = {http://arxiv.org/abs/1509.00519},
	abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
	author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
	year = {2015},
	note = {arXiv: 1509.00519
ISBN: 1509.00519},
	pages = {1--14},
}

@article{zhang_understanding_2016,
	title = {Understanding deep learning requires rethinking generalization},
	issn = {10414347},
	url = {http://arxiv.org/abs/1611.03530},
	doi = {10.1109/TKDE.2015.2507132},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2016},
	pmid = {88045},
	note = {arXiv: 1611.03530
ISBN: 1506.02142},
}

@article{wu_quantitative_2016,
	title = {On the {Quantitative} {Analysis} of {Decoder}-{Based} {Generative} {Models}},
	url = {http://arxiv.org/abs/1611.04273},
	abstract = {The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at https://github.com/tonywu95/eval\_gen. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.},
	author = {Wu, Yuhuai and Burda, Yuri and Salakhutdinov, Ruslan and Grosse, Roger},
	year = {2016},
	note = {arXiv: 1611.04273},
	pages = {1--17},
}

@article{li_inferring_2017,
	title = {Inferring {The} {Latent} {Structure} of {Human} {Decision}-{Making} from {Raw} {Visual} {Inputs}},
	url = {http://arxiv.org/abs/1703.08840},
	abstract = {The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.},
	author = {Li, Yunzhu and Song, Jiaming and Ermon, Stefano},
	year = {2017},
	note = {arXiv: 1703.08840},
}

@article{hu_toward_2017,
	title = {Toward {Controlled} {Generation} of {Text}},
	url = {http://arxiv.org/abs/1703.00955},
	abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	year = {2017},
	note = {arXiv: 1703.00955},
}

@article{mathieu_disentangling_2016,
	title = {Disentangling factors of variation in deep representations using adversarial training},
	issn = {10495258},
	url = {http://arxiv.org/abs/1611.03383},
	abstract = {We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentanglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.},
	number = {Nips},
	author = {Mathieu, Michael and Zhao, Junbo and Sprechmann, Pablo and Ramesh, Aditya and LeCun, Yann},
	year = {2016},
	note = {arXiv: 1611.03383},
}

@article{siddharth_learning_2017,
	title = {Learning {Disentangled} {Representations} with {Semi}-{Supervised} {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1706.00400},
	abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalize from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
	author = {Siddharth, N. and Paige, Brooks and Van de Meent, Jan-Willem and Desmaison, Alban and Wood, Frank and Goodman, Noah D. and Kohli, Pushmeet and Torr, Philip H. S.},
	year = {2017},
	note = {arXiv: 1706.00400},
}

@article{denton_unsupervised_2017,
	title = {Unsupervised {Learning} of {Disentangled} {Representations} from {Video}},
	url = {http://arxiv.org/abs/1705.10915},
	abstract = {We present a new model DrNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluate our approach on a range of synthetic and real videos, demonstrating the ability to coherently generate hundreds of steps into the future.},
	author = {Denton, Emily and Birodkar, Vighnesh},
	year = {2017},
	note = {arXiv: 1705.10915},
}

@article{bouchacourt_multi-level_2017,
	title = {Multi-{Level} {Variational} {Autoencoder}: {Learning} {Disentangled} {Representations} from {Grouped} {Observations}},
	url = {http://arxiv.org/abs/1705.08841},
	abstract = {We would like to learn a representation of the data which decomposes an observation into factors of variation which we can independently control. Specifically, we want to use minimal supervision to learn a latent representation that reflects the semantics behind a specific grouping of the data, where within a group the samples share a common factor of variation. For example, consider a collection of face images grouped by identity. We wish to anchor the semantics of the grouping into a relevant and disentangled representation that we can easily exploit. However, existing deep probabilistic models often assume that the observations are independent and identically distributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new deep probabilistic model for learning a disentangled representation of a set of grouped observations. The ML-VAE separates the latent representation into semantically meaningful parts by working both at the group level and the observation level, while retaining efficient test-time inference. Quantitative and qualitative evaluations show that the ML-VAE model (i) learns a semantically meaningful disentanglement of grouped data, (ii) enables manipulation of the latent representation, and (iii) generalises to unseen groups.},
	author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
	year = {2017},
	note = {arXiv: 1705.08841},
}

@article{clark_whatever_2013,
	title = {Whatever next? {Predictive} brains, situated agents, and the future of cognitive science},
	volume = {36},
	issn = {0140-525X},
	url = {http://www.journals.cambridge.org/abstract_S0140525X12000477},
	doi = {10.1017/S0140525X12000477},
	abstract = {{\textless}p{\textgreater}Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this “hierarchical prediction machine” approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.{\textless}/p{\textgreater}},
	number = {03},
	journal = {Behavioral and Brain Sciences},
	author = {Clark, Andy},
	year = {2013},
	pmid = {23663408},
	note = {arXiv: 0140-525X
ISBN: 1469-1825 (Electronic){\textbackslash}r0140-525X (Linking)},
	pages = {181--204},
}

@article{boehner_how_2007,
	title = {How emotion is made and measured},
	volume = {65},
	issn = {10715819},
	doi = {10.1016/j.ijhcs.2006.11.016},
	abstract = {How we design and evaluate for emotions depends crucially on what we take emotions to be. In affective computing, affect is often taken to be another kind of information-discrete units or states internal to an individual that can be transmitted in a loss-free manner from people to computational systems and back. While affective computing explicitly challenges the primacy of rationality in cognitivist accounts of human activity, at a deeper level it often relies on and reproduces the same information-processing model of cognition. Drawing on cultural, social, and interactional critiques of cognition which have arisen in human-computer interaction (HCI), as well as anthropological and historical accounts of emotion, we explore an alternative perspective on emotion as interaction: dynamic, culturally mediated, and socially constructed and experienced. We demonstrate how this model leads to new goals for affective systems-instead of sensing and transmitting emotion, systems should support human users in understanding, interpreting, and experiencing emotion in its full complexity and ambiguity. In developing from emotion as objective, externally measurable unit to emotion as experience, evaluation, too, alters focus from externally tracking the circulation of emotional information to co-interpreting emotions as they are made in interaction. © 2006 Elsevier Ltd. All rights reserved.},
	number = {4},
	journal = {International Journal of Human Computer Studies},
	author = {Boehner, Kirsten and DePaula, Rogério and Dourish, Paul and Sengers, Phoebe},
	year = {2007},
	note = {ISBN: 1071-5819},
	pages = {275--291},
}

@misc{rumelhart1986c,
  title={C.(eds.): Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
  author={Rumelhart, DE and McClelland, JL and PDP Research Group and others},
  year={1986},
  publisher={MIT Press, Cambridge, MA, USA}
}

@misc{Sutton2019BitterLesson,
  author       = {Sutton, Rich},
  title        = {The Bitter Lesson},
  howpublished = {\url{http://www.incompleteideas.net/IncIdeas/BitterLesson.html}},
  month        = mar,
  year         = {2019},
  note         = {Accessed: 2025-05-05}
}

@inproceedings{murty2023grokking,
  title={Grokking of Hierarchical Structure in Vanilla Transformers},
  author={Murty, Shikhar and Sharma, Pratyusha and Andreas, Jacob and Manning, Christopher D},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={439--448},
  year={2023}
}

@inproceedings{kim-schuster-2023-entity,
    title = "Entity Tracking in Language Models",
    author = "Kim, Najoung  and
      Schuster, Sebastian",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.213/",
    doi = "10.18653/v1/2023.acl-long.213",
    pages = "3835--3855",
    abstract = "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface."
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}