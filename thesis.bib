%%%% this sample bibliography has been taken from Overleaf

@article{einstein,
  author =       "Albert Einstein",
  title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
                 [{On} the electrodynamics of moving bodies]",
  journal =      "Annalen der Physik",
  volume =       "322",
  number =       "10",
  pages =        "891--921",
  year =         "1905",
  DOI =          "http://dx.doi.org/10.1002/andp.19053221004",
  keywords =     "physics"
}

@inproceedings{soulos-etal-2020-discovering,
    title = "Discovering the Compositional Structure of Vector Representations with Role Learning Networks",
    author = "Soulos, Paul  and
      McCoy, R. Thomas  and
      Linzen, Tal  and
      Smolensky, Paul",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa{\l}a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.23/",
    doi = "10.18653/v1/2020.blackboxnlp-1.23",
    pages = "238--254",
    abstract = "How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model`s output is changed in the way predicted by our analysis."
}

@InProceedings{pmlr-v202-soulos23a,
  title = 	 {Differentiable Tree Operations Promote Compositional Generalization},
  author =       {Soulos, Paul and Hu, Edward J and Mccurdy, Kate and Chen, Yunmo and Fernandez, Roland and Smolensky, Paul and Gao, Jianfeng},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32499--32520},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/soulos23a/soulos23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/soulos23a.html},
  abstract = 	 {In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine (DTM) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, DTM achieves 100\% while existing baselines such as Transformer, Tree Transformer, LSTM, and Tree2Tree LSTM achieve less than 30\%. DTM remains highly interpretable in addition to its perfect performance.}
}


@book{dirac,
  title={The Principles of Quantum Mechanics},
  author={Paul Adrien Maurice Dirac},
  isbn={9780198520115},
  series={International series of monographs on physics},
  year={1981},
  publisher={Clarendon Press},
  keywords = {physics}
}

@book{latexcompanion,
    author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
    title     = "The \LaTeX\ Companion",
    year      = "1993",
    publisher = "Addison-Wesley",
    address   = "Reading, Massachusetts",
    keywords  = "latex"
}
 
@online{knuthwebsite,
    author    = "Donald Knuth",
    title     = "Knuth: Computers and Typesetting",
    url       = "http://www-cs-faculty.stanford.edu/~uno/abcde.html",
    addendum = "(accessed: 01.09.2016)",
    keywords  = "latex,knuth"
}

@inbook{knuth-fa,
   author = "Donald E. Knuth",
   title = "Fundamental Algorithms",
   publisher = "Addison-Wesley",
   year = "1973",
   chapter = "1.2",
   keywords  = "knuth,programming"
}

@book{knuth-acp,
   author = "Donald E. Knuth",
   publisher = "Addison-Wesley",
   title = "The Art of Computer Programming",
   series = "Four volumes",
   year = "1968",
   note = "Seven volumes planned",
   keywords  = "knuth,programming"
}

@article{ctan,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351",
    keywords  = "latex"
}



@book{pearl2000causality,
  title={Causality},
  author={Judea Pearl},
  publisher={MIT Press},
  address={Cambridge, MA},
  year={2000}
}


@article{fodor1990connectionism,
  title={Connectionism and the problem of systematicity: Why {Smolensky's} solution doesn't work},
  author={Fodor, Jerry and McLaughlin, Brian P},
  journal={Cognition},
  volume={35},
  number={2},
  pages={183--204},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{giulianelli2018hood,
    title = "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information",
    author = "Giulianelli, Mario  and
      Harding, Jack  and
      Mohnert, Florian  and
      Hupkes, Dieuwke  and
      Zuidema, Willem",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5426",
    doi = "10.18653/v1/W18-5426",
    pages = "240--248",
}

@article{fodor1997connectionism,
  title={Connectionism and the problem of systematicity (continued): Why {Smolensky's} solution still doesn't work},
  author={Fodor, Jerry},
  journal={Cognition},
  volume={62},
  number={1},
  pages={109--119},
  year={1997},
  publisher={Elsevier}
}

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@article{smolensky1987constituent,
  title={The constituent structure of connectionist mental states: A reply to {Fodor and Pylyshyn}},
  author={Smolensky, Paul},
  journal={Southern Journal of Philosophy},
  volume={26},
  number={Supplement},
  pages={137--161},
  year={1987}
}

@incollection{smolensky1991connectionism,
  title={Connectionism, constituency, and the language of thought},
  author={Smolensky, Paul},
  booktitle={Meaning in Mind: Fodor and his Critics},
  editor={Barry Loewer and Georges Rey},
  pages={201--227},
  publisher={Basil Blackwell},
  address={Oxford},
  year={1991}
}

@inproceedings{mccoy,
title={{RNN}s implicitly implement tensor-product representations},
author={R. Thomas McCoy and Tal Linzen and Ewan Dunbar and Paul Smolensky},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJx0sjC5FX},
}

@inproceedings{
andreas2019measuring,
title={Measuring Compositionality in Representation Learning},
author={Jacob Andreas},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJz05o0qK7},
}

@inproceedings{abnar2019blackbox,
    title = "Blackbox Meets Blackbox: Representational Similarity {\&} Stability Analysis of Neural Language Models and Brains",
    author = "Abnar, Samira  and
      Beinborn, Lisa  and
      Choenni, Rochelle  and
      Zuidema, Willem",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4820",
    doi = "10.18653/v1/W19-4820",
    pages = "191--203",
}

@inproceedings{chrupala2019correlating,
    title = "Correlating Neural and Symbolic Representations of Language",
    author = "Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1283",
    doi = "10.18653/v1/P19-1283",
    pages = "2952--2962",
}

@article{hupkes2018visualisation,
  title={Visualisation and `diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
  author={Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={907--926},
  year={2018}
}


@article{hupkes2020compositionality,
  title={Compositionality Decomposed: How do Neural Networks Generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@inproceedings{lake2018generalization,
  title={Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author={Brenden M. Lake and Marco Baroni},
  booktitle={International Conference on Machine Learning},
  url={https://arxiv.org/pdf/1711.00350.pdf},
  year={2018}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3104--3112},
  year={2014},
  url={https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}


@article{syntaxattention,
  title={Compositional generalization in a deep seq2seq model by separating syntax and semantics},
  author={Russin, Jake and Jo, Jason and O'Reilly, Randall C and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1904.09708},
  year={2019},
  url={https://arxiv.org/abs/1904.09708}
}

@inproceedings{mikolov2016roadmap,
  title={A roadmap towards machine intelligence},
  author={Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
  booktitle={International Conference on Intelligent Text Processing and Computational Linguistics},
  pages={29--61},
  year={2016},
  organization={Springer}
}

@book{Smolensky:2006:HMN:1205244,
 author = {Smolensky, Paul and Legendre, G{\'e}raldine},
 title = {The Harmonic Mind: From Neural Computation to Optimality-Theoretic GrammarVolume I: Cognitive Architecture (Bradford Books)},
 year = {2006},
 isbn = {0262195267},
 publisher = {The MIT Press},
}


@article{structure,
author = {McClelland, James and Botvinick, Matthew and Noelle, David and Plaut, David and Rogers, Timothy and Seidenberg, Mark and Smith, Linda},
year = {2010},
month = {08},
pages = {348-56},
title = {Letting Structure Emerge: Connectionist and Dynamical Systems Approaches to Cognition},
volume = {14},
journal = {Trends in cognitive sciences},
doi = {10.1016/j.tics.2010.06.002}
}

@inproceedings{palangi,
  title={Question-Answering with Grammatically-Interpretable Representations},
  author={Hamid Palangi and Paul Smolensky and Xiaodong He and Li Deng},
  booktitle={Proceedings of the Association for the Advancement of Artificial Intelligence},
  url={https://arxiv.org/pdf/1705.08432.pdf},
  year={2017}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{googlenmt,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016},
  url={https://arxiv.org/abs/1609.08144}
}


@inproceedings{kiros2015skip,
  title={Skip-thought vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3294--3302},
  year={2015},
  url={https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf}
}

@InProceedings{conneau2017supervised,
  author = 	"Conneau, Alexis
		and Kiela, Douwe
		and Schwenk, Holger
		and Barrault, Lo{\"i}c
		and Bordes, Antoine",
  title = 	"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"670--680",
  location = 	"Copenhagen, Denmark",
  url = 	"http://aclweb.org/anthology/D17-1070"
}

@inproceedings{socher2013recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}


@inproceedings{bowman2015large,
	Address = {Lisbon, Portugal},
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	Booktitle = {{Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}},
	Date-Added = {2015-11-06 18:15:56 +0000},
	Date-Modified = {2015-11-06 18:16:03 +0000},
	Month = {September},
	Pages = {632--642},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Url = {http://aclweb.org/anthology/D15-1075},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QaVBhcGVycy9TZW1hbnRpY3MvQm93bWFuIGV0IGFsIDIwMTUgLSBBIGxhcmdlIGFubm90YXRlZCBjb3JwdXMgZm9yIGxlYXJuaW5nIG5hdHVyYWwgbGFuZ3VhZ2UgaW5mZXJlbmNlLnBkZtIXCxgZV05TLmRhdGFPEQKaAAAAAAKaAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADVVNe4SCsAAAAN1SkfQm93bWFuIGV0IGFsIDIwMTUgLSAjMTg0MEU2LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABhA5tJiWEYAAAAAAAAAAAABAAQAAAkgAAAAAAAAAAAAAAAAAAAACVNlbWFudGljcwAAEAAIAADVVQ/4AAAAEQAIAADSYp6WAAAAAQAUAA3VKQANvDIADbqKAA2SfQAGadwAAgBZTWFjaW50b3NoIEhEOlVzZXJzOgB0bGluemVuMToARHJvcGJveDoAUGFwZXJzOgBTZW1hbnRpY3M6AEJvd21hbiBldCBhbCAyMDE1IC0gIzE4NDBFNi5wZGYAAA4AsgBYAEIAbwB3AG0AYQBuACAAZQB0ACAAYQBsACAAMgAwADEANQAgAC0AIABBACAAbABhAHIAZwBlACAAYQBuAG4AbwB0AGEAdABlAGQAIABjAG8AcgBwAHUAcwAgAGYAbwByACAAbABlAGEAcgBuAGkAbgBnACAAbgBhAHQAdQByAGEAbAAgAGwAYQBuAGcAdQBhAGcAZQAgAGkAbgBmAGUAcgBlAG4AYwBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgCAVXNlcnMvdGxpbnplbjEvRHJvcGJveC9QYXBlcnMvU2VtYW50aWNzL0Jvd21hbiBldCBhbCAyMDE1IC0gQSBsYXJnZSBhbm5vdGF0ZWQgY29ycHVzIGZvciBsZWFybmluZyBuYXR1cmFsIGxhbmd1YWdlIGluZmVyZW5jZS5wZGYAEwABLwAAFQACAA///wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgD6AP8BBwOlA6cDrAO3A8ADzgPSA9kD4gPnA/QD9wQJBAwEEQAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAQT}}
	
@inproceedings{mccoy2019right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, R. Thomas  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
}

@inproceedings{jawahar-etal-2019-bert,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1356",
    doi = "10.18653/v1/P19-1356",
    pages = "3651--3657",
    abstract = "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",
}


@article{linzen2016assessing,
  title={Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the ACL},
  year={2016},
  url={https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00115}
}

@inproceedings{poliak2018collecting,
    title = "Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation",
    author = "Poliak, Adam  and
      Haldar, Aparajita  and
      Rudinger, Rachel  and
      Hu, J. Edward  and
      Pavlick, Ellie  and
      White, Aaron Steven  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1007",
    doi = "10.18653/v1/D18-1007",
    pages = "67--81",
}


@article{dasgupta2019analyzing,
  title={Analyzing machine-learned representations: A natural language case study},
  author={Dasgupta, Ishita and Guo, Demi and Gershman, Samuel J and Goodman, Noah D},
  journal={arXiv preprint arXiv:1909.05885},
  year={2019},
  url={https://arxiv.org/abs/1909.05885}
}

@inproceedings{kingma2015adam,
	Author = {Diederik Kingma and Jimmy Ba},
	Booktitle = {International Conference for Learning Representations},
	Title = {Adam: A Method for Stochastic Optimization},
	url={https://arxiv.org/pdf/1412.6980.pdf},
	Year = {2015}}


@inproceedings{wang2018glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
}


@InProceedings{manning-EtAl:2014:P14-5,
  author    = {Manning, Christopher D. and  Surdeanu, Mihai  and  Bauer, John  and  Finkel, Jenny  and  Bethard, Steven J. and  McClosky, David},
  title     = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
  booktitle = {Association for Computational Linguistics (ACL) System Demonstrations},
  year      = {2014},
  pages     = {55--60},
  url       = {http://www.aclweb.org/anthology/P/P14/P14-5010}
}

@incollection{mu2020compositional,
  title={Compositional Explanations of Neurons},
  author={Mu, Jesse and Andreas, Jacob},
  booktitle={Advances in Neural Information Processing Systems 33},
  year={2020},
  url={https://arxiv.org/pdf/2006.14032.pdf}
}

@inproceedings{
tenney2018what,
title={What do you learn from context? Probing for sentence structure in contextualized word representations},
author={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R. Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJzSgnRcKX},
}


@inproceedings{peters2018dissecting,
    title = "Dissecting Contextual Word Embeddings: Architecture and Representation",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Zettlemoyer, Luke  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1179",
    doi = "10.18653/v1/D18-1179",
    pages = "1499--1509",
}



@inproceedings{blevins2018hierarchical,
    title = "Deep {RNN}s Encode Soft Hierarchical Syntax",
    author = "Blevins, Terra  and
      Levy, Omer  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2003",
    doi = "10.18653/v1/P18-2003",
    pages = "14--19",
}


@inproceedings{belinkov2017evaluating,
    title = "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    author = "Belinkov, Yonatan  and
      M{\`a}rquez, Llu{\'\i}s  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Glass, James",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/I17-1001",
    pages = "1--10",
}


@inproceedings{conneau2018senteval,
    title = "{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations",
    author = "Conneau, Alexis  and
      Kiela, Douwe",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1269",
}

@inproceedings{conneau2018cram,
    title = "What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties",
    author = {Conneau, Alexis  and
      Kruszewski, German  and
      Lample, Guillaume  and
      Barrault, Lo{\"\i}c  and
      Baroni, Marco},
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1198",
    doi = "10.18653/v1/P18-1198",
    pages = "2126--2136",
}

@inproceedings{adi2016fine,
  title={Fine-grained analysis of sentence embeddings using auxiliary prediction tasks},
  author={Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
  booktitle = {International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/pdf?id=BJh6Ztuxl}
}

@article{wickelgren1969context,
  title={Context-sensitive coding, associative memory, and serial order in (speech) behavior.},
  author={Wickelgren, Wayne A.},
  journal={Psychological Review},
  volume={76},
  number={1},
  pages={1--15},
  year={1969},
  publisher={American Psychological Association}
}

@article{vanmassenhove2017investigating,
  title={Investigating `aspect' in {NMT} and {SMT}: Translating the {English} simple past and present perfect},
  author={Vanmassenhove, Eva and Du, Jinhua and Way, Andy},
  journal={Computational Linguistics in the Netherlands Journal},
  volume={7},
  pages={109--128},
  year={2017}
}

@inproceedings{bowman2016fast,
	Author = {Bowman, Samuel R. and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D. and Potts, Christopher},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Pages = {1466--1477},
	Publisher = {Association for Computational Linguistics},
	Title = {A Fast Unified Model for Parsing and Sentence Understanding},
	Url = {http://aclweb.org/anthology/P16-1139},
	Year = {2016}}

@inproceedings{vMeasure,
  added-at = {2011-09-18T23:01:04.000+0200},
  author = {Rosenberg, Andrew and Hirschberg, Julia},
  biburl = {https://www.bibsonomy.org/bibtex/238531ca6d31f767da8aef74bd6dadcf7/jil},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP-CoNLL)},
  description = {ESOM},
  interhash = {07bb9cbf44b156e34d57c40b9d2fd314},
  intrahash = {38531ca6d31f767da8aef74bd6dadcf7},
  keywords = {cluster clustering measure v-measure validation},
  pages = {410--420},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {{V}-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure},
  year = 2007
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019},
  url={https://www.aclweb.org/anthology/N19-1419/}
}

@article{Hochreiter:1997:LSM:1246443.1246450,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Computation},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@inproceedings{softattention,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  crossref  = {DBLP:conf/iclr/2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Smolensky:1990:TPV:102418.102425,
 author = {Smolensky, Paul},
 title = {Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
 journal = {Artif. Intell.},
 issue_date = {Nov. 1990},
 volume = {46},
 number = {1-2},
 month = nov,
 year = {1990},
 issn = {0004-3702},
 pages = {159--216},
 numpages = {58},
 url = {http://dx.doi.org/10.1016/0004-3702(90)90007-M},
 doi = {10.1016/0004-3702(90)90007-M},
 acmid = {102425},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
} 

@article{omlin1996extraction,
  title={Extraction of rules from discrete-time recurrent neural networks},
  author={Omlin, Christian W and Giles, C Lee},
  journal={Neural networks},
  volume={9},
  number={1},
  pages={41--52},
  year={1996},
  publisher={Elsevier}
}

@incollection{NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}


@inproceedings{weiss2018extracting,
  author    = {Gail Weiss and
               Yoav Goldberg and
               Eran Yahav},
  title     = {Extracting Automata from Recurrent Neural Networks Using Queries and
               Counterexamples},
  booktitle = {{International Conference on Machine Learning}},
  pages     = {5244--5253},
  year      = {2018},
  url={http://proceedings.mlr.press/v80/weiss18a.html}
}

@article{belinkov2019analysis,
  title={Analysis methods in neural language processing: A survey},
  author={Belinkov, Yonatan and Glass, James},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={49--72},
  year={2019},
  publisher={MIT Press},
  url={https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00254}
}

@inproceedings{mikolov2013linguistic,
    title = "Linguistic Regularities in Continuous Space Word Representations",
    author = "Mikolov, Tomas  and
      Yih, Wen-tau  and
      Zweig, Geoffrey",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1090",
    pages = "746--751",
}

@inproceedings{parikh2016decomposable,
    title = "A Decomposable Attention Model for Natural Language Inference",
    author = {Parikh, Ankur  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Das, Dipanjan  and
      Uszkoreit, Jakob},
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1244",
    doi = "10.18653/v1/D16-1244",
    pages = "2249--2255",
}

@inproceedings{klein2003accurate,
  title={Accurate unlexicalized parsing},
  author={Klein, Dan and Manning, Christopher D},
  booktitle={Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1},
  pages={423--430},
  year={2003},
  url={https://www.aclweb.org/anthology/P03-1054.pdf},
  organization={Association for Computational Linguistics}
}


@inproceedings{lakretz2019emergence,
    title = "The emergence of number and syntax units in {LSTM} language models",
    author = "Lakretz, Yair  and
      Kruszewski, German  and
      Desbordes, Theo  and
      Hupkes, Dieuwke  and
      Dehaene, Stanislas  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1002",
    doi = "10.18653/v1/N19-1002",
    pages = "11--20",
}


@inproceedings{
shen2018ordered,
title={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},
author={Yikang Shen and Shawn Tan and Alessandro Sordoni and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1l6qiR5F7},
}

@article{Voita2020InformationTheoreticPW,
  title={Information-Theoretic Probing with Minimum Description Length},
  author={Elena Voita and Ivan Titov},
  journal={arXiv preprint arXiv:2003.12298},
  year={2020},
  url={https://arxiv.org/abs/2003.12298}
}

@inproceedings{li-etal-2019-compositional,
    title = "Compositional Generalization for Primitive Substitutions",
    author = "Li, Yuanpeng  and
      Zhao, Liang  and
      Wang, Jianyu  and
      Hestness, Joel",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1438",
    doi = "10.18653/v1/D19-1438",
    pages = "4293--4302",
    abstract = "Compositional generalization is a basic mechanism in human language learning, but current neural networks lack such ability. In this paper, we conduct fundamental research for encoding compositionality in neural networks. Conventional methods use a single representation for the input sentence, making it hard to apply prior knowledge of compositionality. In contrast, our approach leverages such knowledge with two representations, one generating attention maps, and the other mapping attended input words to output symbols. We reduce the entropy in each representation to improve generalization. Our experiments demonstrate significant improvements over the conventional methods in five NLP tasks including instruction learning and machine translation. In the SCAN domain, it boosts accuracies from 14.0{\%} to 98.8{\%} in Jump task, and from 92.0{\%} to 99.7{\%} in TurnLeft task. It also beats human performance on a few-shot learning task. We hope the proposed approach can help ease future research towards human-level compositional language learning.",
}

@article{warstadt2019blimp,
  title={{BLiMP}: A Benchmark of Linguistic Minimal Pairs for English},
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  journal={Proceedings of the
Society for Computation in Linguistics.},
  year={2020},
  url={https://scholarworks.umass.edu/scil/vol3/iss1/43/}
}


@inproceedings{hu2020systematic,
    title = "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
    author = "Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url="https://www.aclweb.org/anthology/2020.acl-main.158.pdf",
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020},
  url={https://arxiv.org/abs/2005.14165}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017},
  url={https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@inproceedings{goodwin2020probing,
    title = "Probing Linguistic Systematicity",
    author = "Goodwin, Emily  and
      Sinha, Koustuv  and
      O{'}Donnell, Timothy J.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.177",
    doi = "10.18653/v1/2020.acl-main.177",
    pages = "1958--1969",
    abstract = "Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.",
}

@article{ravichander2020probing,
  title={Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?},
  author={Ravichander, Abhilasha and Belinkov, Yonatan and Hovy, Eduard},
  journal={arXiv preprint arXiv:2005.00719},
  year={2020}
}

@misc{najoung,
  doi = {10.48550/ARXIV.2212.10769},
  url = {https://arxiv.org/abs/2212.10769},
  author = {Kim, Najoung and Linzen, Tal and Smolensky, Paul},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@inproceedings{csordas2021devil,
  title={The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, Juergen},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={619--634},
  year={2021}
}


@article{Smolensky1990TensorPV,
  title={Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
  author={Paul Smolensky},
  journal={Artif. Intell.},
  year={1990},
  volume={46},
  pages={159-216}
}

@article{chen2018tree,
  title={Tree-to-tree neural networks for program translation},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{Graves2014NeuralTM,
  title={Neural Turing Machines},
  author={Alex Graves and Greg Wayne and Ivo Danihelka},
  journal={ArXiv},
  year={2014},
  volume={abs/1410.5401}
}

@article{Newell1980PhysicalSS,
  title={Physical Symbol Systems},
  author={Allen Newell},
  journal={Cogn. Sci.},
  year={1980},
  volume={4},
  pages={135-183}
}

@article{Weston2014MemoryN,
  title={Memory Networks},
  author={Jason Weston and Sumit Chopra and Antoine Bordes},
  journal={CoRR},
  year={2014},
  volume={abs/1410.3916}
}

@article{Graves2016HybridCU,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Alex Graves and Greg Wayne and Malcolm Reynolds and Tim Harley and Ivo Danihelka and Agnieszka Grabska-Barwinska and Sergio Gomez Colmenarejo and Edward Grefenstette and Tiago Ramalho and John P. Agapiou and Adri{\`a} Puigdom{\`e}nech Badia and Karl Moritz Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and Christopher Summerfield and Phil Blunsom and Koray Kavukcuoglu and Demis Hassabis},
  journal={Nature},
  year={2016},
  volume={538},
  pages={471-476}
}

@inproceedings{palangi2018question,
  title={Question-answering with grammatically-interpretable representations},
  author={Palangi, Hamid and Smolensky, Paul and He, Xiaodong and Deng, Li},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{jiang2021enriching,
  title={Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization},
  author={Jiang, Yichen and Celikyilmaz, Asli and Smolensky, Paul and Soulos, Paul and Rao, Sudha and Palangi, Hamid and Fernandez, Roland and Smith, Caitlin and Bansal, Mohit and Gao, Jianfeng},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4780--4793},
  year={2021}
}

@article{soulos2021structural,
  title={Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages},
  author={Soulos, Paul and Rao, Sudha and Smith, Caitlin and Rosen, Eric and Celikyilmaz, Asli and McCoy, R Thomas and Jiang, Yichen and Haley, Coleman and Fernandez, Roland and Palangi, Hamid and others},
  journal={Proceedings of Machine Translation Summit XVIII},
  year={2021}
}

@article{Reed2015NeuralP,
  title={Neural Programmer-Interpreters},
  author={Scott E. Reed and Nando de Freitas},
  journal={CoRR},
  year={2015},
  volume={abs/1511.06279}
}

@inproceedings{tai-etal-2015-improved,
    title = "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
    author = "Tai, Kai Sheng  and
      Socher, Richard  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1150",
    doi = "10.3115/v1/P15-1150",
    pages = "1556--1566",
}

@inproceedings{wang-etal-2019-tree,
    title = "Tree Transformer: Integrating Tree Structures into Self-Attention",
    author = "Wang, Yaushian  and
      Lee, Hung-Yi  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1098",
    doi = "10.18653/v1/D19-1098",
    pages = "1061--1070",
    abstract = "Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed {``}Constituent Attention{''} module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.",
}


@InProceedings{bosnjak17a,
  title = 	 {Programming with a Differentiable Forth Interpreter},
  author =       {Matko Bo{\v{s}}njak and Tim Rockt{\"a}schel and Jason Naradowsky and Sebastian Riedel},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {547--556},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/bosnjak17a/bosnjak17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/bosnjak17a.html},
  abstract = 	 {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.}
}

@inproceedings{cho-etal-2014-properties,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@inproceedings{sutskever,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}



@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}

@inproceedings{vaswani,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{shiv2019novel,
  title={Novel positional encodings to enable tree-based transformers},
  author={Shiv, Vighnesh and Quirk, Chris},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kleyko2022,
author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
title = {A Survey on Hyperdimensional Computing Aka Vector Symbolic Architectures, Part I: Models and Data Transformations},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3538531},
doi = {10.1145/3538531},
abstract = {This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and distributed vector representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent years, the necessity for a comprehensive survey of the field has become extremely important. Therefore, amongst other aspects of the field, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey [84] is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {130},
numpages = {40},
keywords = {tensor product representations, holographic reduced representations, machine learning, Artificial intelligence, modular composite representations, sparse binary distributed representations, sparse block codes, multiply-add-permute, geometric analogue of holographic reduced representations, data structures, hyperdimensional computing, binary spatter codes, distributed representations, matrix binding of additive terms, vector symbolic architectures}
}


@inproceedings{gayler2003vsa_jackendoff,
  title={Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience},
  author = {Ross W Gayler},
  booktitle = {Proceedings of the ICCS/ASCS Joint International Conference on Cognitive Science (ICCS/ASCS 2003)},
  pages={133--138},
  month = {07},
  year={2003},
  date={2003-07-17},
  publisher = {University of New South Wales},
  address = {Sydney, NSW, AU},
  editor = {Slezak, Peter},
  archivePrefix = {arXiv},
  arxivId = {cs/0412059v1},
  eprint = {0412059v1},
  primaryClass = {cs},
  url = {http://arxiv.org/abs/cs/0412059}
}

@article{kanerva2009hyperdimensional,
  title={Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors},
  author={Kanerva, Pentti},
  journal={Cognitive computation},
  volume={1},
  pages={139--159},
  year={2009},
  publisher={Springer}
}


@article{sartran2022transformer,
  title={Transformer Grammars: Augmenting Transformer language models with syntactic inductive biases at scale},
  author={Sartran, Laurent and Barrett, Samuel and Kuncoro, Adhiguna and Stanojevi{\'c}, Milo{\v{s}} and Blunsom, Phil and Dyer, Chris},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={1423--1439},
  year={2022},
  publisher={MIT Press}
}

@inproceedings{
maddison2017the,
title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1jE5L5gl}
}

@inproceedings{
jang2017categorical,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}
@inproceedings{dong2016language,
  title={Language to Logical Form with Neural Attention},
  author={Dong, Li and Lapata, Mirella},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={33--43},
  year={2016}
}

@InProceedings{chen,
  title = 	 {Mapping natural-language problems to formal-language solutions using structured neural representations},
  author =       {Chen, Kezhen and Huang, Qiuyuan and Palangi, Hamid and Smolensky, Paul and Forbus, Ken and Gao, Jianfeng},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1566--1575},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20g/chen20g.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20g.html},
  abstract = 	 {Generating formal-language programs represented by relational tuples, such as Lisp programs or mathematical operations, to solve problems stated in natural language is a challenging task because it requires explicitly capturing discrete symbolic structural information implicit in the input. However, most general neural sequence models do not explicitly capture such structural information, limiting their performance on these tasks. In this paper, we propose a new encoder-decoder model based on a structured neural representation, Tensor Product Representations (TPRs), for mapping Natural-language problems to Formal-language solutions, called TP-N2F. The encoder of TP-N2F employs TPR ‘binding’ to encode natural-language symbolic structure in vector space and the decoder uses TPR ‘unbinding’ to generate, in symbolic space, a sequential program represented by relational tuples, each consisting of a relation (or operation) and a number of arguments. TP-N2F considerably outperforms LSTM-based seq2seq models on two benchmarks and creates new state-of-the-art results. Ablation studies show that improvements can be attributed to the use of structured TPRs explicitly in both the encoder and decoder. Analysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F.}
}

@article{imanol,
  author    = {Imanol Schlag and
               Paul Smolensky and
               Roland Fernandez and
               Nebojsa Jojic and
               J{\"{u}}rgen Schmidhuber and
               Jianfeng Gao},
  title     = {Enhancing the Transformer with Explicit Relational Encoding for Math
               Problem Solving},
  journal   = {CoRR},
  volume    = {abs/1910.06611},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.06611},
  eprinttype = {arXiv},
  eprint    = {1910.06611},
  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-06611.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{soulos2020discovering,
  title={Discovering the Compositional Structure of Vector Representations with Role Learning Networks},
  author={Soulos, Paul and McCoy, R Thomas and Linzen, Tal and Smolensky, Paul},
  booktitle={Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  pages={238--254},
  year={2020}
}

@inproceedings{
mccoy2018rnns,
title={{RNN}s implicitly implement tensor-product representations},
author={R. Thomas McCoy and Tal Linzen and Ewan Dunbar and Paul Smolensky},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJx0sjC5FX},
}



@article{nram,
title	= {Neural Random Access Machines},
author	= {Karol Kurach and Marcin Andrychowicz and Ilya Sutskever},
year	= {2016},
URL	= {http://arxiv.org/abs/1511.06392},
journal	= {ICLR}
}

@article{Andreas2015NeuralMN,
  title={Neural Module Networks},
  author={Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Dan Klein},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={39-48}
}

@inproceedings{Joulin2015InferringAP,
  title={Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
  author={Armand Joulin and Tomas Mikolov},
  booktitle={NIPS},
  year={2015}
}

@article{Grefenstette2015LearningTT,
  title={Learning to Transduce with Unbounded Memory},
  author={Edward Grefenstette and Karl Moritz Hermann and Mustafa Suleyman and Phil Blunsom},
  journal={ArXiv},
  year={2015},
  volume={abs/1506.02516}
}

@inproceedings{kim-linzen-2020-cogs,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
}

@book{plate,
author = {Plate, Tony A.},
title = {Holographic Reduced Representation: Distributed Representation for Cognitive Structures},
year = {2003},
isbn = {1575864290},
publisher = {CSLI Publications},
address = {USA}
}


@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{
eliasmith,
author = {Chris Eliasmith  and Terrence C. Stewart  and Xuan Choo  and Trevor Bekolay  and Travis DeWolf  and Yichuan Tang  and Daniel Rasmussen },
title = {A Large-Scale Model of the Functioning Brain},
journal = {Science},
volume = {338},
number = {6111},
pages = {1202-1205},
year = {2012},
doi = {10.1126/science.1225266},
URL = {https://www.science.org/doi/abs/10.1126/science.1225266},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1225266},
abstract = {Neurons are pretty complicated cells. They display an endless variety of shapes that sprout highly variable numbers of axons and dendrites; they sport time- and voltage-dependent ion channels along with an impressive array of neurotransmitter receptors; and they connect intimately with near neighbors as well as former neighbors who have since moved away. Simulating a sizeable chunk of brain tissue has recently become achievable, thanks to advances in computer hardware and software. Eliasmith et al. (p. 1202; see the Perspective by Machens) present their million-neuron model of the brain and show that it can recognize numerals, remember lists of digits, and write down those lists—tasks that seem effortless for a human but that encompass the triad of perception, cognition, and behavior. Two-and-a-half million model neurons recognize images, learn via reinforcement, and display fluid intelligence. A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called “Spaun”) that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.}}


@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@inproceedings{patel-etal-2022-revisiting,
    title = "Revisiting the Compositional Generalization Abilities of Neural Sequence Models",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Blunsom, Phil  and
      Goyal, Navin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.46",
    doi = "10.18653/v1/2022.acl-short.46",
    pages = "424--434",
}

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@book{marcus2003algebraic,
  title={The algebraic mind: Integrating connectionism and cognitive science},
  author={Marcus, Gary F},
  year={2003},
  publisher={MIT press}
}

@book{steele1990common,
  title={Common LISP: the language},
  author={Steele, Guy},
  year={1990},
  publisher={Elsevier}
}

@article{newell1982knowledge,
  title={The knowledge level},
  author={Newell, Allen},
  journal={Artificial intelligence},
  volume={18},
  number={1},
  pages={87--127},
  year={1982},
  publisher={Elsevier}
}

@article{schlag2018learning,
  title={Learning to reason with third order tensor products},
  author={Schlag, Imanol and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{
soulos2024compositional,
title={Compositional Generalization Across Distributional Shifts with Sparse Tree Operations},
author={Paul Soulos and Henry Conklin and Mattia Opper and Paul Smolensky and Jianfeng Gao and Roland Fernandez},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=fOQunr2E0T}
}

@article{Smolensky1990TensorPV,
  title={Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
  author={Paul Smolensky},
  journal={Artif. Intell.},
  year={1990},
  volume={46},
  pages={159-216}
}

@misc{li_slog_2023,
	title = {{SLOG}: {A} {Structural} {Generalization} {Benchmark} for {Semantic} {Parsing}},
	shorttitle = {{SLOG}},
	url = {http://arxiv.org/abs/2310.15040},
	abstract = {The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well models can generalize. We introduce SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with 17 structural generalization cases. In our experiments, the generalization accuracy of Transformer models, including pretrained ones, only reaches 40.6\%, while a structure-aware parser only achieves 70.8\%. These results are far from the near-perfect accuracy existing models achieve on COGS, demonstrating the role of SLOG in foregrounding the large discrepancy between models' lexical and structural generalization capacities.},
	urldate = {2024-01-28},
	publisher = {arXiv},
	author = {Li, Bingzhi and Donatelli, Lucia and Koller, Alexander and Linzen, Tal and Yao, Yuekun and Kim, Najoung},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15040 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{Winters_Marra_Manhaeve_Raedt_2022, title={DeepStochLog: Neural Stochastic Logic Programming}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/21248}, DOI={10.1609/aaai.v36i9.21248}, abstractNote={Recent advances in neural-symbolic learning, such as DeepProbLog, extend probabilistic logic programs with neural predicates. Like graphical models, these probabilistic logic programs define a probability distribution over possible worlds, for which inference is computationally hard. We propose DeepStochLog, an alternative neural-symbolic framework based on stochastic definite clause grammars, a kind of stochastic logic program. More specifically, we introduce neural grammar rules into stochastic definite clause grammars to create a framework that can be trained end-to-end. We show that inference and learning in neural stochastic logic programming scale much better than for neural probabilistic logic programs. Furthermore, the experimental evaluation shows that DeepStochLog achieves state-of-the-art results on challenging neural-symbolic learning tasks.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Winters, Thomas and Marra, Giuseppe and Manhaeve, Robin and Raedt, Luc De}, year={2022}, month={Jun.}, pages={10090-10100} }

@inproceedings{rocktaschel-riedel-2016-learning,
    title = "Learning Knowledge Base Inference with Neural Theorem Provers",
    author = {Rockt{\"a}schel, Tim  and
      Riedel, Sebastian},
    editor = "Pujara, Jay  and
      Rocktaschel, Tim  and
      Chen, Danqi  and
      Singh, Sameer",
    booktitle = "Proceedings of the 5th Workshop on Automated Knowledge Base Construction",
    month = jun,
    year = "2016",
    address = "San Diego, CA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-1309",
    doi = "10.18653/v1/W16-1309",
    pages = "45--50",
}

@article{Shindo_Nishino_Yamamoto_2021, title={Differentiable Inductive Logic Programming for Structured Examples}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16637}, DOI={10.1609/aaai.v35i6.16637}, abstractNote={The differentiable implementation of logic yields a seamless combination of symbolic reasoning and deep neural networks. Recent research, which has developed a differentiable framework to learn logic programs from examples, can even acquire reasonable solutions from noisy datasets. However, this framework severely limits expressions for solutions, e.g., no function symbols are allowed, and the shapes of clauses are fixed. As a result, the framework cannot deal with structured examples. Therefore we propose a new framework to learn logic programs from noisy and structured examples, including the following contributions. First, we propose an adaptive clause search method by looking through structured space, which is defined by the generality of the clauses, to yield an efficient search space for differentiable solvers. Second, we propose for ground atoms an enumeration algorithm, which determines a necessary and sufficient set of ground atoms to perform differentiable inference functions. Finally, we propose a new method to compose logic programs softly, enabling the system to deal with complex programs consisting of several clauses. Our experiments show that our new framework can learn logic programs from noisy and structured examples, such as sequences or trees. Our framework can be scaled to deal with complex programs that consist of several clauses with function symbols.}, number={6}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shindo, Hikaru and Nishino, Masaaki and Yamamoto, Akihiro}, year={2021}, month={May}, pages={5034-5041} }

@article{Muggleton_1991_InductiveLogicProgramming,
	title = {Inductive logic programming},
	volume = {8},
	issn = {1882-7055},
	url = {https://doi.org/10.1007/BF03037089},
	doi = {10.1007/BF03037089},
	abstract = {A new research area, Inductive Logic Programming, is presently emerging. While inheriting various positive characteristics of the parent subjects of Logic Programming and Machine Learning, it is hoped that the new area will overcome many of the limitations of its forebears. The background to present developments within this area is discussed and various goals and aspirations for the increasing body of researchers are identified. Inductive Logic Programming needs to be based on sound principles from both Logic and Statistics. On the side of statistical justification of hypotheses we discuss the possible relationship between Algorithmic Complexity theory and Probably-Approximately-Correct (PAC) Learning. In terms of logic we provide a unifying framework for Muggleton and Buntine’s Inverse Resolution (IR) and Plotkin’s Relative Least General Generalisation (RLGG) by rederiving RLGG in terms of IR. This leads to a discussion of the feasibility of extending the RLGG framework to allow for the invention of new predicates, previously discussed only within the context of IR.},
	number = {4},
	journal = {New Generation Computing},
	author = {Muggleton, Stephen},
	month = feb,
	year = {1991},
	pages = {295--318},
}


@inproceedings{NEURIPS2023_bf215fa7,
 author = {Maene, Jaron and De Raedt, Luc},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {60804--60820},
 publisher = {Curran Associates, Inc.},
 title = {Soft-Unification in Deep Probabilistic Logic},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/bf215fa7fe70a38c5e967e59c44a99d0-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@InProceedings{pmlr-v216-de-smet23a,
  title = 	 {Neural probabilistic logic programming in discrete-continuous domains},
  author =       {De Smet, Lennert and Zuidberg Dos Martires, Pedro and Manhaeve, Robin and Marra, Giuseppe and Kimmig, Angelika and De Readt, Luc},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {529--538},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/de-smet23a/de-smet23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/de-smet23a.html},
  abstract = 	 {Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic background knowledge in the form of logic. It has been shown to aid learning in the limited data regime and to facilitate inference on out-of-distribution data. Probabilistic NeSy focuses on integrating neural networks with both logic and probability theory, which additionally allows learning under uncertainty. A major limitation of current probabilistic NeSy systems, such as DeepProbLog, is their restriction to finite probability distributions, i.e., discrete random variables. In contrast, deep probabilistic programming (DPP) excels in modelling and optimising continuous probability distributions. Hence, we introduce DeepSeaProbLog, a neural probabilistic logic programming language that incorporates DPP techniques into NeSy. Doing so results in the support of inference and learning of both discrete and continuous probability distributions under logical constraints. Our main contributions are 1) the semantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a proven asymptotically unbiased learning algorithm, and 3) a series of experiments that illustrate the versatility of our approach.}
}


@inproceedings{ijcai2020p243,
  title     = {NeurASP: Embracing Neural Networks into Answer Set Programming},
  author    = {Yang, Zhun and Ishay, Adam and Lee, Joohyung},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {1755--1762},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/243},
  url       = {https://doi.org/10.24963/ijcai.2020/243},
}


@article{BADREDDINE2022103649,
title = {Logic Tensor Networks},
journal = {Artificial Intelligence},
volume = {303},
pages = {103649},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103649},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221002009},
author = {Samy Badreddine and Artur {d'Avila Garcez} and Luciano Serafini and Michael Spranger},
keywords = {Neurosymbolic AI, Deep learning and reasoning, Many-valued logics},
abstract = {Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2020-06-19},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@article{partee1995lexical,
  title={Lexical semantics and compositionality},
  author={Partee, Barbara and others},
  journal={An invitation to cognitive science: Language},
  volume={1},
  pages={311--360},
  year={1995},
  publisher={MIT Press Cambridge, MA:}
}
@article{steedman1987combinatory,
  title={Combinatory grammars and parasitic gaps},
  author={Steedman, Mark},
  journal={Natural Language \& Linguistic Theory},
  volume={5},
  number={3},
  pages={403--439},
  year={1987},
  publisher={Springer}
}
@inproceedings{klein2002generative,
  title={A generative constituent-context model for improved grammar induction},
  author={Klein, Dan and Manning, Christopher D},
  booktitle={Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  pages={128--135},
  year={2002}
}
@article{furrer2020compositional,
  title={Compositional generalization in semantic parsing: Pre-training vs. specialized architectures},
  author={Furrer, Daniel and van Zee, Marc and Scales, Nathan and Sch{\"a}rli, Nathanael},
  journal={arXiv preprint arXiv:2007.08970},
  year={2020}
}
@article{kim2019compound,
  title={Compound probabilistic context-free grammars for grammar induction},
  author={Kim, Yoon and Dyer, Chris and Rush, Alexander M},
  journal={arXiv preprint arXiv:1906.10225},
  year={2019}
}

@article{devlin_compositional_2019,
	title = {Compositional {Generalization} in {Semantic} {Parsing}: {Pre}-training vs. {Specialized} {Architectures}},
	doi = {10.18653/v1/N19-1423},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186},
}
@article{mccarthy1960recursive,
  title={Recursive functions of symbolic expressions and their computation by machine, part I},
  author={McCarthy, John},
  journal={Communications of the ACM},
  volume={3},
  number={4},
  pages={184--195},
  year={1960},
  publisher={ACM New York, NY, USA}
}

@inproceedings{andreas_good-enough_2020,
	address = {Online},
	title = {Good-{Enough} {Compositional} {Data} {Augmentation}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.676},
	doi = {10.18653/v1/2020.acl-main.676},
	abstract = {We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87\% on diagnostic tasks from the SCAN dataset and 16\% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1\% on small corpora in several languages.},
	language = {en},
	urldate = {2021-01-30},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Andreas, Jacob},
	year = {2020},
	keywords = {\_tablet},
	pages = {7556--7566},
}

@inproceedings{sakai1961syntax,
  title={Syntax in universal translation},
  author={Sakai, Itiroo},
  booktitle={Proceedings of the International Conference on Machine Translation and Applied Language Analysis},
  year={1961}
}

@article{lopez2008statistical,
  title={Statistical machine translation},
  author={Lopez, Adam},
  journal={ACM Computing Surveys (CSUR)},
  volume={40},
  number={3},
  pages={1--49},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@article{guo_sequence-level_2020,
	title = {Sequence-{Level} {Mixed} {Sample} {Data} {Augmentation}},
	url = {http://arxiv.org/abs/2011.09039},
	abstract = {Despite their empirical success, neural networks still have difﬁculty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut (Wang et al., 2018) and word dropout (Sennrich et al., 2016), and show that these techniques are all approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on ﬁve different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.},
	language = {en},
	urldate = {2021-03-04},
	journal = {arXiv:2011.09039 [cs]},
	author = {Guo, Demi and Kim, Yoon and Rush, Alexander M.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.09039},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{
dusell2024stack,
title={Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns},
author={Brian DuSell and David Chiang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=XVhm3X8Fum}
}

@inproceedings{zelle1996learning,
  title={Learning to parse database queries using inductive logic programming},
  author={Zelle, John M and Mooney, Raymond J},
  booktitle={Proceedings of the national conference on artificial intelligence},
  pages={1050--1055},
  year={1996}
}

@article{kim2022uncontrolled,
  title={Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models},
  author={Kim, Najoung and Linzen, Tal and Smolensky, Paul},
  journal={arXiv preprint arXiv:2212.10769},
  year={2022}
}
@book{chomsky_aspects_1965,
	address = {Cambridge, Massachusetts},
	edition = {50th Anniversary Edition},
	series = {Massachusetts {Institute} of {Technology}. {Research} {Laboratory} of {Electronics}. {Special} technical report},
	title = {Aspects of the theory of syntax},
	isbn = {978-0-262-52740-8},
	number = {no. 11},
	publisher = {The MIT Press},
	author = {Chomsky, Noam},
	year = {1965},
	keywords = {Grammar, Comparative and general, Syntax},
}

@book{goldberg_constructions_2006,
	address = {Oxford; New York},
	title = {Constructions at work: the nature of generalization in language},
	shorttitle = {Constructions at work},
	url = {http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3052348},
	abstract = {Includes selected classic and contemporary papers in four areas, this text introduces each field, providing technical background for the non-specialist and explaining the underlying connections across the disciplines.},
	language = {English.},
	urldate = {2020-06-23},
	publisher = {Oxford University Press},
	author = {Goldberg, Adele E},
	year = {2006},
	note = {OCLC: 193697889},
}

@article{Griffiths2020UnderstandingHI,
  title={Understanding Human Intelligence through Human Limitations},
  author={Thomas L. Griffiths},
  journal={Trends in Cognitive Sciences},
  year={2020},
  volume={24},
  pages={873-883},
  url={https://api.semanticscholar.org/CorpusID:221996148}
}

@article{belinkov2019analysis,
  title={Analysis methods in neural language processing: A survey},
  author={Belinkov, Yonatan and Glass, James},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={49--72},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{blevins2018deep,
  title={Deep RNNs encode soft hierarchical syntax},
  author={Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1805.04218},
  year={2018}
}

@article{murty2022characterizing,
  title={Characterizing intrinsic compositionality in transformers with tree projections},
  author={Murty, Shikhar and Sharma, Pratyusha and Andreas, Jacob and Manning, Christopher D},
  journal={arXiv preprint arXiv:2211.01288},
  year={2022}
}

@article{soulos2019discovering,
  title={Discovering the compositional structure of vector representations with role learning networks},
  author={Soulos, Paul and McCoy, Tom and Linzen, Tal and Smolensky, Paul},
  journal={arXiv preprint arXiv:1910.09113},
  year={2019}
}
@article{mccoy2018rnns,
  title={RNNs implicitly implement tensor product representations},
  author={McCoy, R Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
  journal={arXiv preprint arXiv:1812.08718},
  year={2018}
}

@incollection{cuetos2013parsing,
  title={Parsing in different languages},
  author={Cuetos, Fernando and Mitchell, Don C and Corley, Martin MB},
  booktitle={Language processing in Spanish},
  pages={163--208},
  year={2013},
  publisher={Psychology Press}
}

@article{tai2015improved,
  title={Improved semantic representations from tree-structured long short-term memory networks},
  author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
  journal={arXiv preprint arXiv:1503.00075},
  year={2015}
}

@article{dong2016language,
  title={Language to logical form with neural attention},
  author={Dong, Li and Lapata, Mirella},
  journal={arXiv preprint arXiv:1601.01280},
  year={2016}
}

@article{wang2019tree,
  title={Tree transformer: Integrating tree structures into self-attention},
  author={Wang, Yau-Shian and Lee, Hung-Yi and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:1909.06639},
  year={2019}
}

@article{shiv2019novel,
  title={Novel positional encodings to enable tree-based transformers},
  author={Shiv, Vighnesh and Quirk, Chris},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{joulin2015inferring,
  title={Inferring algorithmic patterns with stack-augmented recurrent nets},
  author={Joulin, Armand and Mikolov, Tomas},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{grefenstette2015learning,
  title={Learning to transduce with unbounded memory},
  author={Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{patel2022revisiting,
  title={Revisiting the compositional generalization abilities of neural sequence models},
  author={Patel, Arkil and Bhattamishra, Satwik and Blunsom, Phil and Goyal, Navin},
  journal={arXiv preprint arXiv:2203.07402},
  year={2022}
}

@inproceedings{socher-etal-2012-semantic,
    title = "Semantic Compositionality through Recursive Matrix-Vector Spaces",
    author = "Socher, Richard  and
      Huval, Brody  and
      Manning, Christopher D.  and
      Ng, Andrew Y.",
    editor = "Tsujii, Jun{'}ichi  and
      Henderson, James  and
      Pa{\c{s}}ca, Marius",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D12-1110",
    pages = "1201--1211",
}

@article{conklin2021meta,
  title={Meta-learning to compositionally generalize},
  author={Conklin, Henry and Wang, Bailin and Smith, Kenny and Titov, Ivan},
  journal={arXiv preprint arXiv:2106.04252},
  year={2021}
}

@article{lindemann2023compositional,
  title={Compositional generalization without trees using multiset tagging and latent permutations},
  author={Lindemann, Matthias and Koller, Alexander and Titov, Ivan},
  journal={arXiv preprint arXiv:2305.16954},
  year={2023}
}

@article{russin2019compositional,
  title={Compositional generalization in a deep seq2seq model by separating syntax and semantics},
  author={Russin, Jake and Jo, Jason and O'Reilly, Randall C and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1904.09708},
  year={2019}
}

@article{lake2019compositional,
  title={Compositional generalization through meta sequence-to-sequence learning},
  author={Lake, Brenden M},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{dusell2021learning,
  title={Learning hierarchical structures with differentiable nondeterministic stacks},
  author={DuSell, Brian and Chiang, David},
  journal={arXiv preprint arXiv:2109.01982},
  year={2021}
}

@article{chen2020compositional,
  title={Compositional generalization via neural-symbolic stack machines},
  author={Chen, Xinyun and Liang, Chen and Yu, Adams Wei and Song, Dawn and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1690--1701},
  year={2020}
}

@book{pinker2003language,
  title={The language instinct: How the mind creates language},
  author={Pinker, Steven},
  year={2003},
  publisher={Penguin uK}
}

@inproceedings{huang2022directed,
  title={Directed acyclic transformer for non-autoregressive machine translation},
  author={Huang, Fei and Zhou, Hao and Liu, Yang and Li, Hang and Huang, Minlie},
  booktitle={International Conference on Machine Learning},
  pages={9410--9428},
  year={2022},
  organization={PMLR}
}

@article{Smolensky_1988, title={On the proper treatment of connectionism}, volume={11}, DOI={10.1017/S0140525X00052432}, number={1}, journal={Behavioral and Brain Sciences}, author={Smolensky, Paul}, year={1988}, pages={1–23}}

@book{Marcus2001-MARTAM-10,
	author = {Gary F. Marcus},
	editor = {},
	publisher = {MIT Press},
	title = {The Algebraic Mind: Integrating Connectionism and Cognitive Science},
	year = {2001}
}

@book{garcez2008neural,
  title={Neural-symbolic cognitive reasoning},
  author={Garcez, Artur SD'Avila and Lamb, Luis C and Gabbay, Dov M},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{t5,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {deep learning, attention based models, multi-task learning, natural language processing, transfer learning}
}

@article{nslr,
  author       = {Tarek R. Besold and
                  Artur S. d'Avila Garcez and
                  Sebastian Bader and
                  Howard Bowman and
                  Pedro M. Domingos and
                  Pascal Hitzler and
                  Kai{-}Uwe K{\"{u}}hnberger and
                  Lu{\'{\i}}s C. Lamb and
                  Daniel Lowd and
                  Priscila Machado Vieira Lima and
                  Leo de Penning and
                  Gadi Pinkas and
                  Hoifung Poon and
                  Gerson Zaverucha},
  title        = {Neural-Symbolic Learning and Reasoning: {A} Survey and Interpretation},
  journal      = {CoRR},
  volume       = {abs/1711.03902},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.03902},
  eprinttype    = {arXiv},
  eprint       = {1711.03902},
  timestamp    = {Mon, 13 Aug 2018 16:48:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-03902.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{GARNELO201917,
title = {Reconciling deep learning with symbolic artificial intelligence: representing objects and relations},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {17-23},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301943},
author = {Marta Garnelo and Murray Shanahan},
abstract = {In the history of the quest for human-level artificial intelligence, a number of rival paradigms have vied for supremacy. Symbolic artificial intelligence was dominant for much of the 20th century, but currently a connectionist paradigm is in the ascendant, namely machine learning with deep neural networks. However, both paradigms have strengths and weaknesses, and a significant challenge for the field today is to effect a reconciliation. A central tenet of the symbolic paradigm is that intelligence results from the manipulation of abstract compositional representations whose elements stand for objects and relations. If this is correct, then a key objective for deep learning is to develop architectures capable of discovering objects and relations in raw data, and learning how to represent them in ways that are useful for downstream processing. This short review highlights recent progress in this direction.}
}

@article{kim_cogs_2020,
	title = {{COGS}: {A} {Compositional} {Generalization} {Challenge} {Based} on {Semantic} {Interpretation}},
	shorttitle = {{COGS}},
	url = {http://arxiv.org/abs/2010.05465},
	abstract = {Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99\%), but generalization accuracy was substantially lower (16–35\%) and showed high sensitivity to random seed (±6–8\%). These ﬁndings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.},
	language = {en},
	urldate = {2021-01-25},
	journal = {arXiv:2010.05465 [cs]},
	author = {Kim, Najoung and Linzen, Tal},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.05465},
	keywords = {Computer Science - Computation and Language},
}

@misc{Soulos_2023_DifferentiableTreeOperations,
	title = {Differentiable {Tree} {Operations} {Promote} {Compositional} {Generalization}},
	url = {http://arxiv.org/abs/2306.00751},
	doi = {10.48550/arXiv.2306.00751},
	abstract = {In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine (DTM) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, DTM achieves 100\% while existing baselines such as Transformer, Tree Transformer, LSTM, and Tree2Tree LSTM achieve less than 30\%. DTM remains highly interpretable in addition to its perfect performance.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Soulos, Paul and Hu, Edward and McCurdy, Kate and Chen, Yunmo and Fernandez, Roland and Smolensky, Paul and Gao, Jianfeng},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00751 [cs]},
	annote = {Comment: ICML 2023. Code available at https://github.com/psoulos/dtm},
	file = {Soulos et al. - 2023 - Differentiable Tree Operations Promote Composition.pdf:/Users/psoulos/Zotero/storage/WSWQKEIJ/Soulos et al. - 2023 - Differentiable Tree Operations Promote Composition.pdf:application/pdf},
}


@InProceedings{pmlr-v97-lee19d,
  title = 	 {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks},
  author =       {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3744--3753},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/lee19d/lee19d.pdf},
  url = 	 {https://proceedings.mlr.press/v97/lee19d.html},
  abstract = 	 {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.}
}

@article{
li2023representations,
title={Representations and Computations in Transformers that Support Generalization on Structured Tasks},
author={Yuxuan Li and James McClelland},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=oFC2LAqS6Z},
note={}
}

@inproceedings{ruoss-etal-2023-randomized,
    title = "Randomized Positional Encodings Boost Length Generalization of Transformers",
    author = "Ruoss, Anian  and
      Del{\'e}tang, Gr{\'e}goire  and
      Genewein, Tim  and
      Grau-Moya, Jordi  and
      Csord{\'a}s, R{\'o}bert  and
      Bennani, Mehdi  and
      Legg, Shane  and
      Veness, Joel",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.161",
    doi = "10.18653/v1/2023.acl-short.161",
    pages = "1889--1903",
    abstract = "Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence{'}s length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0{\%} on average).",
}

@inproceedings{NEURIPS2019_6e091746,
 author = {Shiv, Vighnesh and Quirk, Chris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Novel positional encodings to enable tree-based transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{gayler2003vsa_jackendoff,
  title={Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience},
  author = {Ross W Gayler},
  booktitle = {Proceedings of the ICCS/ASCS Joint International Conference on Cognitive Science (ICCS/ASCS 2003)},
  pages={133--138},
  month = {jul},
  year={2003},
  date={2003-07-17},
  publisher = {University of New South Wales},
  address = {Sydney, NSW, AU},
  editor = {Slezak, Peter},
  archivePrefix = {arXiv},
  arxivId = {cs/0412059v1},
  eprint = {0412059v1},
  primaryClass = {cs},
  url = {http://arxiv.org/abs/cs/0412059}
}

@book{plate,
author = {Plate, Tony A.},
title = {Holographic Reduced Representation: Distributed Representation for Cognitive Structures},
year = {2003},
isbn = {1575864290},
publisher = {CSLI Publications},
address = {USA}
}

@article{kanerva2009hyperdimensional,
  title={Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors},
  author={Kanerva, Pentti},
  journal={Cognitive computation},
  volume={1},
  pages={139--159},
  year={2009},
  publisher={Springer}
}

@article{kleyko2022,
author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
title = {A Survey on Hyperdimensional Computing Aka Vector Symbolic Architectures, Part I: Models and Data Transformations},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3538531},
doi = {10.1145/3538531},
abstract = {This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and distributed vector representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent years, the necessity for a comprehensive survey of the field has become extremely important. Therefore, amongst other aspects of the field, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey [84] is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {130},
numpages = {40},
keywords = {tensor product representations, holographic reduced representations, machine learning, Artificial intelligence, modular composite representations, sparse binary distributed representations, sparse block codes, multiply-add-permute, geometric analogue of holographic reduced representations, data structures, hyperdimensional computing, binary spatter codes, distributed representations, matrix binding of additive terms, vector symbolic architectures}
}

@inproceedings{
yogatama2018memory,
title={Memory Architectures in Recurrent Neural Network Language Models},
author={Dani Yogatama and Yishu Miao and Gabor Melis and Wang Ling and Adhiguna Kuncoro and Chris Dyer and Phil Blunsom},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SkFqf0lAZ},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@inproceedings{Sun_2023_ReplicationStudyCompositional,
	title = {A {Replication} {Study} of {Compositional} {Generalization} {Works} on {Semantic} {Parsing}},
	url = {https://openreview.net/forum?id=MF9uv95psps},
	abstract = {Reproducibility Summary Scope of Reproducibility — We examine the reproducibility of compositional generalization results from the task of semantic parsing. We aim to reproduce results from [1], [2], and [3] and seek to verify the claims that 1. A model shouldn't be expected to perform well on non-synthetic datasets just because it performs well on SCAN [1], 2. The approaches from [1] and [2] meet or exceed baseline performance on compositional generalization tests, and 3. NQG-T5 [1] outperforms baselines on both synthetic and natural data. 4. NQG [1] performs well on the instances that it is able to generate a prediction, but it faces the barrier of not being able to generate predictions for all instances. Methodology — We reuse the authors' code along with additional code to run extra experiments, and we re-implement scripts whose support is deprecated. Eight 32GB GPUs were used for experiments, with a detailed description in Section 3.3. Results — Claim 1 is verified: the model with the highest performance on SCAN does not maintain its high performance on other datasets (Section 4.1). Claims 2 and 3 are verified, with a comparison of performance between NQG-T5 and the selected baseline models in [1] and [2]. Claim 4 is also verified by computing the coverage and precision of NQG in Section 4.4. Overall, accuracy for most experiments reaches within 2\% of that reported in the original paper, with a deviation that our T5 achieves higher performance on some splits and slightly lower performance on one split than reported previously. What was easy — All papers provide clearly‐written code and informative documentation, as well as lists of hyperparameters that are used for experiments. The papers also describe their approaches clearly, making the experimental workflow easy to follow. What was difficult — The exact match evaluation metric is formulated somewhat differently across all three papers, leading to a non‐negligible value difference, as discussed in Section 5.2. We also had to re‐implement some training scripts because an original dependency is no longer supported. Moreover, some experiments are computationally expensive: [1] used TPUs for experiments, while our replication with GPUs take several days to train a single T5 model. Communication with original authors — The authors of all three papers provided us with useful instruction to work with their methods and constructive feedback on the draft.},
	language = {en},
	urldate = {2024-03-21},
	author = {Sun, Kaiser and Williams, Adina and Hupkes, Dieuwke},
	month = aug,
	year = {2023},
	file = {Full Text PDF:/Users/psoulos/Zotero/storage/A2K6CK23/Sun et al. - 2023 - A Replication Study of Compositional Generalizatio.pdf:application/pdf},
}


@inproceedings{csordas-etal-2021-devil,
    title = "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
    author = "Csord{\'a}s, R{\'o}bert  and
      Irie, Kazuki  and
      Schmidhuber, Juergen",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.49",
    doi = "10.18653/v1/2021.emnlp-main.49",
    pages = "619--634",
    abstract = "Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50{\%} to 85{\%} on the PCFG productivity split, and from 35{\%} to 81{\%} on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100{\%} accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results.",
}

@inproceedings{NEURIPS2018_d759175d,
 author = {Chen, Xinyun and Liu, Chang and Song, Dawn},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Tree-to-tree Neural Networks for Program Translation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{shaw-etal-2021-compositional,
    title = "Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?",
    author = "Shaw, Peter  and
      Chang, Ming-Wei  and
      Pasupat, Panupong  and
      Toutanova, Kristina",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.75",
    doi = "10.18653/v1/2021.acl-long.75",
    pages = "922--938",
    abstract = "Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization? To better assess this capability, we propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model. It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations. While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.",
}

@inproceedings{smith-eisner-2006-quasi,
    title = "Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies",
    author = "Smith, David  and
      Eisner, Jason",
    editor = "Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings on the Workshop on Statistical Machine Translation",
    month = jun,
    year = "2006",
    address = "New York City",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-3104",
    pages = "23--30",
}

@book{10.7551/mitpress/9780262527347.001.0001,
    author = {Chomsky, Noam},
    title = "{The Minimalist Program}",
    publisher = {The MIT Press},
    year = {2014},
    month = {12},
    abstract = "{A classic work that situates linguistic theory in the broader cognitive sciences, formulating and developing the minimalist program. In his foundational book, The Minimalist Program, published in 1995, Noam Chomsky offered a significant contribution to the generative tradition in linguistics. This twentieth-anniversary edition reissues this classic work with a new preface by the author. In four essays, Chomsky attempts to situate linguistic theory in the broader cognitive sciences, with the essays formulating and progressively developing the minimalist approach to linguistic theory. Building on the theory of principles and parameters and, in particular, on principles of economy of derivation and representation, the minimalist framework takes Universal Grammar as providing a unique computational system, with derivations driven by morphological properties, to which the syntactic variation of languages is also restricted. Within this theoretical framework, linguistic expressions are generated by optimally efficient derivations that must satisfy the conditions that hold on interface levels, the only levels of linguistic representation. The interface levels provide instructions to two types of performance systems, articulatory-perceptual and conceptual-intentional. All syntactic conditions, then, express properties of these interface levels, reflecting the interpretive requirements of language and keeping to very restricted conceptual resources. In the preface to this edition, Chomsky emphasizes that the minimalist approach developed in the book and in subsequent work “is a program, not a theory.” With this book, Chomsky built on pursuits from the earliest days of generative grammar to formulate a new research program that had far-reaching implications for the field.}",
    isbn = {9780262327282},
    doi = {10.7551/mitpress/9780262527347.001.0001},
    url = {https://doi.org/10.7551/mitpress/9780262527347.001.0001},
}


@article{Lake_2018_GeneralizationSystematicityCompositional,
	title = {Generalization without systematicity: {On} the compositional skills of sequence-to-sequence recurrent networks},
	volume = {7},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generaliza-tions when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
	journal = {35th International Conference on Machine Learning, ICML 2018},
	author = {Lake, Brenden and Baroni, Marco},
	year = {2018},
	note = {arXiv: 1711.00350
ISBN: 9781510867963},
	pages = {4487--4499},
	file = {Lake and Baroni - 2018 - Generalization without systematicity On the compo.pdf:/Users/psoulos/Zotero/storage/IWTYSLV4/Lake and Baroni - 2018 - Generalization without systematicity On the compo.pdf:application/pdf},
}

@inbook{gorn+1967+77+115,
url = {https://doi.org/10.3138/9781487592769-008},
title = {Explicit Definitions and Linguistic Dominoes},
booktitle = {Systems and Computer Science},
author = {Saul Gorn},
publisher = {University of Toronto Press},
address = {Toronto},
pages = {77--115},
doi = {doi:10.3138/9781487592769-008},
isbn = {9781487592769},
year = {1967},
lastchecked = {2024-05-07}
}

@ARTICLE{bishop-noise,
  author={Bishop, Chris M.},
  journal={Neural Computation}, 
  title={Training with Noise is Equivalent to Tikhonov Regularization}, 
  year={1995},
  volume={7},
  number={1},
  pages={108-116},
  keywords={},
  doi={10.1162/neco.1995.7.1.108}}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@inproceedings{kim-linzen-2020-cogs,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
    abstract = "Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\%}), but generalization accuracy was substantially lower (16{--}35{\%}) and showed high sensitivity to random seed (+-6{--}8{\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",
}

@inproceedings{
keysers2020measuring,
title={Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
author={Daniel Keysers and Nathanael Sch{\"a}rli and Nathan Scales and Hylke Buisman and Daniel Furrer and Sergii Kashubin and Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and Olivier Bousquet},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SygcCnNKwr}
}

@article{nye2020learning,
  title={Learning compositional rules via neural program synthesis},
  author={Nye, Maxwell and Solar-Lezama, Armando and Tenenbaum, Josh and Lake, Brenden M},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10832--10842},
  year={2020}
}