\chapter{Introduction} \label{chap:chap-0}

Human cognition is distinguished by remarkable compositionality—the capacity to systematically generalize learned knowledge to novel contexts and structures \citep{fodor1988connectionism,lake2018generalization}. Despite impressive successes, contemporary neural networks, particularly Transformers \citep{vaswani2017attention}, frequently struggle to achieve robust compositional generalization. This limitation is especially evident in tasks involving symbolic manipulation, algorithmic reasoning, and low-resource language understanding, where models must not merely recall but creatively combine known elements into new configurations \citep{marcus2003algebraic, lake2018generalization}. These observations echo longstanding critiques in cognitive science: neural networks, lacking explicit structured representations, may fail to capture the productivity of human-like rules. Traditional symbolic approaches, such as formal grammars, build in combinatorial structure and can generalize by design, but often at the cost of efficiency and learnability. Pure neural approaches, by contrast, learn flexibly from data but tend to treat inputs as unanalyzed wholes, making them sample-inefficient and brittle on out-of-distribution cases. This dissertation explores the mechanisms underpinning compositional generalization and proposes novel architectures designed to bridge the gap between neural and symbolic computation.

What is the most suitable framework for understanding human cognition? Some argue that accounting for human language capacity requires mechanisms for variable binding and structured composition, so-called “symbols in the head” \citep{marcus2003algebraic, tenenbaum2011grow}. Others contend that symbolic capabilities can emerge from neural networks with the right architectures and training regimes, without explicit symbol-manipulation mechanisms \citep{elman_finding_1990, rumelhart1986c}. However, traditional deep learning architectures, despite extensive training and vast data exposure, often fail to systematically generalize beyond their immediate training distributions \citep{kim-linzen-2020-cogs, hupkes2020compositionality, kim_uncontrolled_2022}. This failure reflects their limited ability to explicitly represent and operate on symbolic structures \citep{Smolensky1990TensorPV}.

This dissertation addresses this core issue by investigating and developing neurosymbolic approaches that embed symbolic computation within neural architectures in a differentiable manner, thereby enhancing generalization capabilities while preserving scalability and flexibility. Smolensky and others have proposed methods to bridge the symbolic and connectionist camps by designing neural networks that represent and manipulate symbols encoded as distributed vectors \citep{Smolensky1990TensorPV,gayler2003vsa_jackendoff,plate,pollack_recursive_1990}. Notably, Smolensky’s Tensor Product Representations (TPRs) offer a way to embed symbolic structures (such as trees or stacks) into neural network vectors by binding fillers (content) to roles (positions or functions) via tensor algebra. A TPR can encode structured symbolic knowledge (e.g., a parse tree or a list) in a single high-dimensional vector, from which the original symbolic constituents can be recovered.

Fusing neural and symbolic approaches is an attractive idea, and it is essential that various forms of neurosymbolic computation are distinguished. For example, how should we describe a Large Language Model (LLM) that produces a symbolic program? On one hand, the LLM component is clearly a neural network, but the output is a symbolic program. Recent work uses external interpreters to run symbolic programs and generate verifiable feedback for reinforcement (TODO cite). However, even if humans have two distinct systems in the brain for generating and executing programs, the execution system must ultimately be implemented neurally, as the brain has no external interpreter. The distinction between \textit{unified} and \textit{hybrid} neurosymbolic systems is a core motivation behind this dissertation.

Hybrid systems combine a neural front‑end with a discrete symbolic core: the neural network parameterizes rule selection or grammar probabilities, while the symbolic engine performs the actual reasoning. This design delivers rule‑based generalization, but it inherits the brittleness and scalability limits of classical symbolic AI—it must be feasible to express the desired computation in a fully symbolic language, and differentiable learning signals cannot flow through the discrete core. By contrast, a unified neurosymbolic model embeds symbolic structures directly in continuous vector space—symbolic operations become differentiable linear maps, neural and symbolic computation share the same representational substrate, and learning can optimize them jointly. The Differentiable Tree Machine (DTM) developed in this dissertation exemplifies this unified approach: it executes tree‐structured transformations entirely in vector space, thereby retaining the interpretability and systematicity of symbolic computation while matching the flexibility and scalability of deep learning.

%Generalizable symbolic programs have a specified domain and range over which the program will correctly map inputs to outputs. These domains and ranges can be infinite, and there exist fields of Computer Science dedicated to verifying that the programs work correctly over these infinite spaces (TODO). Conversely, specifying domains and ranges over which neural networks correctly map inputs to outputs is very difficult—if the domain or range are infinite, there are bound to be samples drawn from outside the training distribution. Consider the case of multiplying two integers together (TODO cite). No matter how long you let the training samples be, there are always going to be longer numbers that could be processed at test time.

\section{Structure of the Dissertation}
This dissertation is organized into three main parts, each focusing on a different aspect of the research. 

Part 1 (Chapter \ref{chap:chap-1}) introduces the Role Learning Network, which uncovers latent symbolic structures within neural network encodings \citep{soulos2020discovering}. This method reveals that neural representations often implicitly encode symbolic structure in-distribution, providing critical insights into how neural networks succeed on compositional tasks without explicitly symbolic architectures \citep{mccoy2018rnns}. By systematically manipulating discovered symbolic structures within neural networks, this approach demonstrates the causal role of these latent structures in model behavior and in-distribution generalization. 

Part 2 (Chapters \ref{chap:chap-2} and \ref{chap:chap-3}) introduces the Differentiable Tree Machine (DTM), a unified neurosymbolic architecture designed to integrate symbolic tree operations within neural frameworks \citep{pmlr-v202-soulos23a,soulos2024compositional}. Leveraging differentiable symbolic operations allows DTM to achieve near-perfect compositional generalization on structured transformation tasks, significantly outperforming traditional Transformers \citep{vaswani2017attention}, LSTMs \citep{hochreiter_long_1997}, and their tree-based counterparts \citep{tai2015improved,dong2016language,NEURIPS2018_d759175d,shiv_novel_2019}.

Building on DTM, the Sparse Differentiable Tree Machine (sDTM) introduces Sparse Coordinate Trees, a highly efficient representation that maintains symbolic compositional benefits while substantially reducing computational complexity \citep{soulos2024compositional}. Sparse Coordinate Trees are a novel vector-symbolic encoding scheme equivalent to Tensor Product Representations but much more scalable. sDTM expands applicability from specific tree-to-tree transformations to general sequence-to-sequence tasks, showcasing superior compositional generalization across diverse datasets. This development highlights the potential of unified neurosymbolic systems, integrating rather than merely juxtaposing neural and symbolic computations \citep{gayler2003vsa_jackendoff,Smolensky1990TensorPV}.

Part 3 (Chapters \ref{chap:chap-4} and \ref{chap:chap-5}) improves Transformers for modeling Regular and Context-Free Languages. Recurrent Transformers address limitations of standard Transformer architectures in algorithmic and state-tracking tasks typical of regular language modeling \citep{deletang_neural_2022, merrill2024the}. This research identifies the critical trade-off between model parallelism and generalization, emphasizing recurrence as pivotal for effective algorithmic reasoning and length generalization \citep{soulos2024recurrent}. Empirical analyses underscore the delicate balance necessary to design recurrent architectures that enhance algorithmic capabilities without compromising computational efficiency \cite{ju_staircase_2022, hutchins_block-recurrent_2022}.

Next, I experiment with giving Transformers access to a stack data structure to model Context-Free Languages. I find that the encoding mechanism used on the stack can drastically change performance—on nondeterministic context-free languages, the traditional encoding mechanism of reading out the top state performs worse than my proposed enhancement, which encodes the stack using attention over all states.

Finally, I conclude with a discussion of the results and future work in Chapter \ref{chap:chap-6}. 

Collectively, these studies contribute to a broader understanding of how neural networks can emulate the structural richness of symbolic systems while retaining powerful learning capacities. By systematically exploring differentiable symbolic operations, efficient structural representations, recurrence for algorithmic reasoning, and discovering latent symbolic structures, this dissertation offers vital insights and practical methodologies for developing next-generation neural architectures capable of human-like compositional generalization.