\chapter{Introduction} \label{chap:chap-0}

Human cognitive abilities are distinguished by remarkable compositionality—the capacity to generalize learned knowledge systematically to novel contexts and structures \citep{fodor1988connectionism,lake2018generalization}. Despite impressive successes, contemporary neural networks, particularly Transformers \citep{vaswani2017attention}, frequently struggle to achieve such robust compositional generalization. This limitation becomes especially evident in tasks involving symbolic manipulation, algorithmic reasoning, and language understanding, where models must not merely recall but creatively combine known elements into new configurations \citep{marcus2003algebraic, lake2018generalization}. Such observations echo longstanding critiques in cognitive science that neural networks, lacking explicit structured representations, might not capture the productivity of human-like rules. Traditional symbolic approaches (e.g. formal grammars) build in combinatorial structure and can generalize by design, but they trade off efficiency and learnability. Pure neural approaches, by contrast, learn flexibly from data but tend to treat inputs as unanalyzed wholes, making them sample-inefficient and brittle on out-of-distribution cases. This dissertation explores the mechanisms underpinning compositional generalization and proposes novel architectures designed explicitly to bridge the gap between neural and symbolic computation.

What is the most suitable framework for understanding human cognition? On the one hand, some arge that accounting for human language capacity requires mechanisms for variable binding and structured composition (e.g. “symbols in the head” as argued by Marcus, cite some tenenbaum stuff too). On the other hand, connectionist researchers have argued that symbolic capabilities can emerge from neural networks with the right architectures and training regimes, without requiring explicit symbol-manipulation mechanisms \citep{elman_finding_1990, rumelhart1986c}. However, traditional deep learning architectures, despite extensive training and vast data exposure, often fail to systematically generalize beyond their immediate training distributions \citep{kim-linzen-2020-cogs, hupkes2020compositionality}. This failure reflects their inherently limited ability to explicitly represent and operate on symbolic structures \citep{Smolensky1990TensorPV}.

My research addresses this core issue by investigating and developing neurosymbolic approaches that embed symbolic computation within neural architectures in a differentiable manner, thereby enhancing generalization capabilities while preserving scalability and flexibility. Smolensky and others have proposed methods to bridge the symbolic and connectionist camps by designing neural networks that represent and manipulate symbols encoded as distributed vectors \citep{Smolensky1990TensorPV,gayler2003vsa_jackendoff,plate,pollack_recursive_1990}. Notably, Smolensky’s Tensor Product Representations (TPRs) offered a way to embed symbolic structures (like trees or sequences) into neural network vectors by binding fillers (content) to roles (positions or functions) via tensor algebra. A TPR can encode a structured symbolic knowledge (e.g. a parse tree or a list) in a single high-dimensional vector, and decoding that vector can recover the original symbolic constituents.

%TODO: 

Fusing neural and symbolic approaches is an attractive idea, and it is essential that various forms of neurosymbolic computation are distinguished. For example, how should we describe an LLM that produces a symbolic program? On one hand, the LLM component is clearly a neural network, but the output is a symbolic program. Recent work uses external interpreters to run symbolic programs in order to generate verifiable feedback for reinforcement learning (TODO cite). However, even if we take it as a given that humans have two distinct systems, one for generating programs and another for running them, we know that the system for running the program must be implementable in a neural system since all the human brain but not have an external interpreter. The distinction between \textit{unified} and \textit{hybrid} neurosymbolic systems is a core motivation behind this dissertation.

Hybrid systems bolt a neural front‑end onto a discrete symbolic core: the neural network merely parameterizes rule selection or grammar probabilities, while the symbolic engine performs the actual reasoning. This design delivers rule‑based generalization, but it inherits the brittleness and scalability limits of classical symbolic AI—it must be feasible to express the desired computation in a fully symbolic language, and differentiable learning signals cannot flow through the discrete core. By contrast, a unified neurosymbolic model embeds the symbolic structures themselves inside continuous vector space—symbolic operations become differentiable linear maps, neural and symbolic computation live in the same representational substrate, and learning can optimize them jointly. The Differentiable Tree Machine (DTM) developed in this dissertation exemplifies this unified approach: it executes tree‐structured transformations entirely in vector space, thereby retaining the interpretability and systematicity of symbolic computation while matching the flexibility and scalability of deep learning.

%Generalizable symbolic programs have a specified domain and range over which the program will correctly map inputs to outputs. These domains and ranges can be infinite, and there exist fields of Computer Science dedicated to verifying that the programs work correctly over these infinite spaces (TODO). Conversely, specifying domains and ranges over which neural networks correctly map inputs to outputs is very difficult—if the domain or range are infinite, there are bound to be samples drawn from outside the training distribution. Consider the case of multiplying two integers together (TODO cite). No matter how long you let the training samples be, there are always going to be longer numbers that could be processed at test time.

\section{Structure of the Dissertation}
This dissertation is organized into three main parts, each focusing on a different aspect of the research. 

Part 1 (Chapter \ref{chap:chap-1}) explores introduces Role Learning Network framework, which uncovers latent symbolic structures within neural network encodings \citep{soulos2020discovering}. This method reveals that neural representations often implicitly encode symbolic structure in-distribution, providing critical insights into how neural networks succeed on compositional tasks without explicitly symbolic architectures \citep{mccoy2018rnns}. By systematically manipulating discovered symbolic structures within neural networks, this approach demonstrates the causal role of these latent structures in model behavior and generalization capabilities. 

Part 2 (Chapters \ref{chap:chap-2} and \ref{chap:chap-3}) introduces the Differentiable Tree Machine (DTM), a unified neurosymbolic architecture explicitly designed to integrate symbolic tree operations within neural frameworks \citep{pmlr-v202-soulos23a,soulos2024compositional}. Leveraging differentiable symbolic operations allows DTM to achieve near-perfect compositional generalization on structured transformation tasks, significantly outperforming traditional Transformers \citep{vaswani2017attention}, LSTMs \citep{hochreiter_long_1997}, and their tree-based counterparts \citep{tai2015improved,dong2016language,NEURIPS2018_d759175d,shiv_novel_2019}.

Extending the foundational concepts of DTM, the Sparse Differentiable Tree Machine (sDTM) introduces Sparse Coordinate Trees, a highly efficient representation maintaining symbolic compositional benefits while substantially reducing computational complexity \citep{soulos2024compositional}. Sparse Coordinate Trees are a novel vector-symbolic encoding scheme that are equivalent to Tensor Product Representations but much more scalable. sDTM expands applicability from specific tree-to-tree transformations to general sequence-to-sequence tasks, showcasing superior compositional generalization across diverse datasets. This development highlights the potential of unified neurosymbolic systems, integrating rather than merely juxtaposing neural and symbolic computations (Gayler, 2003; Smolensky, 1990).

Part 3 (Chapters \ref{chap:chap-4} and \ref{chap:chap-5}) improves Transformers for modeling Regular and Context Free Languages. Recurrent Transformers addresses limitations of standard Transformer architectures in algorithmic and state-tracking tasks typical of regular language modeling \citep{deletang, merrill2024the}. This research identifies the critical trade-offs between model parallelism and generalization, emphasizing recurrence as pivotal for effective algorithmic reasoning and length generalization \citep{soulos2024recurrent}. Empirical analyses underscore the delicate balance necessary to design recurrent architectures that enhance algorithmic capabilities without compromising computational efficiency \cite{ju_staircase_2022, hutchins_block-recurrent_2022}.



Next, I experiment with giving Transformers access to a stack data structure to model Context Free Languages.

Finally, I conclude with a discussion of the results and future work in Chapter \ref{chap:chap-6}. 

Collectively, these studies contribute to a broader understanding of how neural networks can emulate the structural richness of symbolic systems while retaining powerful learning capacities. By systematically exploring differentiable symbolic operations, efficient structural representations, recurrence for algorithmic reasoning, and discovering latent symbolic structures, this dissertation offers vital insights and practical methodologies for developing next-generation neural architectures capable of human-like compositional generalization.