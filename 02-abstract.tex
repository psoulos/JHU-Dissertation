\chap{Abstract} 
Contemporary neural networks, despite their successes, often struggle with the robust compositional generalization characteristic of human cognition, particularly in tasks requiring symbolic manipulation and algorithmic reasoning. This dissertation investigates the mechanisms underlying compositional generalization in neural networks and proposes novel neurosymbolic architectures designed to bridge the gap between connectionist and symbolic computation, primarily leveraging Tensor Product Representations (TPRs) to embed symbolic structures within vector spaces.

First, I introduce the Role Learning Network (ROLE), a diagnostic model that automatically discovers the latent structure of standard sequence‑to‑sequence RNNs. This analysis reveals how networks can perform compositional tasks by converging to solutions that approximate compositional vector embeddings of symbolic structures. The causal importance of these discovered structures are demonstrated through activation patching for controlling modeling behavior.

Next, I present the the Differentiable Tree Machine (DTM), a unified neurosymbolic architecture that integrates explicit, differentiable tree operations through a differentiable interpreter. An agent learns to leverage the interpreter in order to sequence a neurosybmolic program, while an external memory stores intermediate states. To scale this approach, I develop Sparse Coordinate Trees, a TPR-equivalent encoding scheme that reduces parameters by 70x, memory by 100x, and throughput by 34x. Across a variety of distributional shifts, DTM leveraging Sparse Coordinate Trees achieves the best out-of-distribution performance compared with other neural and neurosymbolic baselines.

Finally, I focus on improving Transformers for modeling formal languages. I investigate the parallelism and generalization trade-off of Recurrent Transformers for modeling Regular Languages, identifying token-layer recurrence as key and analyzing the trade-off between chunk size, parallelizability, and length generalization capabilities. The work also explores augmenting Transformers with stack-like structures for Context-Free Languages.

Collectively, this dissertation contributes novel analysis techniques (ROLE) and unified neurosymbolic architectures (DTM, sDTM) that integrate differentiable symbolic operations and structured representations within neural networks. By exploring latent structures, explicit tree manipulation, efficient sparse representations, and recurrence, this work offers insights and methodologies for developing next-generation neural models capable of more human-like compositional generalization

%%%% your abstract goes here (word limit: 350)

%%%%  committee members (add it right after the abstract w/o page break)
\begin{singlespace}

    %% if you have co-advisor, add here w/ \vspace{0.1in} as shown below
    %% alternatively you can use minipage environment to put side-by-side
    \section*{Primary readers}
    
    Dr. Paul Smolensky \\
    Johns Hopkins University, Baltimore MD 

    \vspace{0.1in}

    Dr. Benjamin Van Durme \\
    Johns Hopkins University, Baltimore MD

    \vspace{0.1in}

    Dr. John T. Hale \\
    Johns Hopkins University, Baltimore MD 

    \vspace{0.1in}

    Dr. Colin Wilson \\
    Johns Hopkins University, Baltimore MD

    \vspace{0.1in}

    Dr. Robert Frank \\
    Yale University, New Haven CT 

    \section*{Alternate readers}
    
    Dr. Jennifer Hu\\
    Johns Hopkins University, Baltimore, MD 
    
    \vspace{0.1in}
    
    Dr. Christopher Honey \\
    Johns Hopkins University, Baltimore, MD 

    %% you can add more readers if you have them on your committee 
    %% use \vspace{0.1in} in between members for clarity
    %% you can also place committee members side-by-side using `minipage`


\end{singlespace}