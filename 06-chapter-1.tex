\chapter{Chapter title goes here} \label{chap:chap-1}

% if you want a short header you can use the following command
% \chapter[short-header-name]{chapter-title} \label{chap:chap-1}


%% add your chapter text here
Traditional models of cognition, and language in particular, have relied heavily on symbol structures and symbol manipulation.
However, in the current era, deep learning research has shown that Neural Networks (NNs) can display remarkable degrees of generalization on tasks traditionally viewed as depending on symbolic structure  \citep{googlenmt, mccoy}, albeit with some important limits to their generalization \citep{lake2018generalization}.
%often surpassing symbolic systems for partially-compositional tasks, including translation \citep{googlenmt}; they can also exhibit good generalization (although generally falling short of symbolic  systems) on fully-compositional tasks \citep{lake2018generalization, mccoy}.
Given that standard NNs have no obvious mechanisms for representing symbolic structures, parsing inputs into such structures, nor applying compositional symbol-manipulating rules to them, this success raises the question that we address in this paper:\textbf{\emph{ How do NNs achieve such strong performance on compositional tasks?}} 
